{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        \"\"\"\n",
    "        데이터셋을 초기화합니다.\n",
    "\n",
    "        Args:\n",
    "            csv (str 또는 pd.DataFrame): 이미지 파일 경로와 레이블이 포함된 CSV 파일 경로 또는 DataFrame.\n",
    "            path (str): 이미지 파일들이 저장된 디렉토리의 경로.\n",
    "            transform (callable, optional): 이미지에 적용할 변환 함수. 기본값은 None.\n",
    "        \"\"\"\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            # CSV가 DataFrame일 경우, 값을 배열로 변환\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            # CSV 파일을 읽어서 값을 배열로 변환\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        데이터셋의 총 샘플 수를 반환합니다.\n",
    "\n",
    "        Returns:\n",
    "            int: 데이터셋의 샘플 수\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        주어진 인덱스에 해당하는 샘플을 반환합니다.\n",
    "\n",
    "        Args:\n",
    "            idx (int): 데이터셋에서 샘플의 인덱스.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (이미지, 레이블)\n",
    "        \"\"\"\n",
    "        name, target = self.df[idx]  # DataFrame에서 이미지 파일명과 타겟 레이블 추출\n",
    "        img_path = os.path.join(self.path, name)  # 이미지 파일 경로 생성\n",
    "        if not os.path.exists(img_path):\n",
    "            # 이미지 파일이 존재하지 않을 경우, 경고 메시지 출력 및 빈 이미지 생성\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((380, 380, 3), dtype=np.uint8)  # 기본 이미지 크기 및 채널 (RGB) 설정\n",
    "        else:\n",
    "            # 이미지 파일을 열고 RGB로 변환\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "\n",
    "        if self.transform:\n",
    "            # 변환 함수가 제공되면 이미지를 변환\n",
    "            img = self.transform(image=img)['image']\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    \"\"\"\n",
    "    한 에포크 동안 모델을 학습합니다.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): 학습 데이터 로더.\n",
    "        model (nn.Module): 학습할 PyTorch 모델.\n",
    "        optimizer (torch.optim.Optimizer): 모델 파라미터를 업데이트할 옵티마이저.\n",
    "        loss_fn (callable): 손실 함수.\n",
    "        device (torch.device): 모델과 데이터를 배치할 디바이스 (CPU 또는 GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: 평균 손실, 정확도, F1 점수\n",
    "    \"\"\"\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    train_loss = 0  # 에포크 동안의 총 손실을 저장할 변수\n",
    "    preds_list = []  # 예측값을 저장할 리스트\n",
    "    targets_list = []  # 실제 레이블을 저장할 리스트\n",
    "\n",
    "    # 데이터 로더를 순회하며 학습\n",
    "    pbar = tqdm(loader, desc=\"Training\")  # 진행 상황을 표시하는 프로그래스 바\n",
    "\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)  # 입력 이미지를 디바이스로 이동\n",
    "        targets = targets.to(device)  # 타겟 레이블을 디바이스로 이동\n",
    "\n",
    "        optimizer.zero_grad()  # 기울기를 초기화\n",
    "\n",
    "        preds = model(image)  # 모델을 사용하여 예측값 계산\n",
    "        loss = loss_fn(preds, targets)  # 손실 함수 계산\n",
    "        loss.backward()  # 역전파를 통해 기울기 계산\n",
    "        optimizer.step()  # 옵티마이저를 사용하여 모델 파라미터 업데이트\n",
    "\n",
    "        train_loss += loss.item()  # 손실 값을 누적\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())  # 예측값을 CPU로 이동하여 리스트에 추가\n",
    "        targets_list.extend(targets.detach().cpu().numpy())  # 실제 레이블을 CPU로 이동하여 리스트에 추가\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")  # 진행 상황에 현재 손실 값을 표시\n",
    "\n",
    "    train_loss /= len(loader)  # 에포크 동안의 평균 손실 계산\n",
    "    train_acc = accuracy_score(targets_list, preds_list)  # 정확도 계산\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')  # F1 점수 계산\n",
    "\n",
    "    return train_loss, train_acc, train_f1  # 평균 손실, 정확도, F1 점수를 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    \"\"\"\n",
    "    모델의 검증을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): 검증 데이터 로더.\n",
    "        model (nn.Module): 검증할 PyTorch 모델.\n",
    "        loss_fn (callable): 손실 함수.\n",
    "        device (torch.device): 모델과 데이터를 배치할 디바이스 (CPU 또는 GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: 평균 손실, 정확도, F1 점수\n",
    "    \"\"\"\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    val_loss = 0  # 검증 동안의 총 손실을 저장할 변수\n",
    "    preds_list = []  # 예측값을 저장할 리스트\n",
    "    targets_list = []  # 실제 레이블을 저장할 리스트\n",
    "\n",
    "    with torch.no_grad():  # 검증 중에는 기울기 계산을 하지 않음\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)  # 입력 이미지를 디바이스로 이동\n",
    "            targets = targets.to(device)  # 타겟 레이블을 디바이스로 이동\n",
    "\n",
    "            preds = model(image)  # 모델을 사용하여 예측값 계산\n",
    "            loss = loss_fn(preds, targets)  # 손실 함수 계산\n",
    "\n",
    "            val_loss += loss.item()  # 손실 값을 누적\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())  # 예측값을 CPU로 이동하여 리스트에 추가\n",
    "            targets_list.extend(targets.detach().cpu().numpy())  # 실제 레이블을 CPU로 이동하여 리스트에 추가\n",
    "\n",
    "    val_loss /= len(loader)  # 검증 동안의 평균 손실 계산\n",
    "    val_acc = accuracy_score(targets_list, preds_list)  # 정확도 계산\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')  # F1 점수 계산\n",
    "\n",
    "    return val_loss, val_acc, val_f1  # 평균 손실, 정확도, F1 점수를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model structure of efficientnet_b4:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 48, 190, 190]           1,296\n",
      "          Identity-2         [-1, 48, 190, 190]               0\n",
      "              SiLU-3         [-1, 48, 190, 190]               0\n",
      "    BatchNormAct2d-4         [-1, 48, 190, 190]              96\n",
      "            Conv2d-5         [-1, 48, 190, 190]             432\n",
      "          Identity-6         [-1, 48, 190, 190]               0\n",
      "              SiLU-7         [-1, 48, 190, 190]               0\n",
      "    BatchNormAct2d-8         [-1, 48, 190, 190]              96\n",
      "            Conv2d-9             [-1, 12, 1, 1]             588\n",
      "             SiLU-10             [-1, 12, 1, 1]               0\n",
      "           Conv2d-11             [-1, 48, 1, 1]             624\n",
      "          Sigmoid-12             [-1, 48, 1, 1]               0\n",
      "    SqueezeExcite-13         [-1, 48, 190, 190]               0\n",
      "           Conv2d-14         [-1, 24, 190, 190]           1,152\n",
      "         Identity-15         [-1, 24, 190, 190]               0\n",
      "         Identity-16         [-1, 24, 190, 190]               0\n",
      "   BatchNormAct2d-17         [-1, 24, 190, 190]              48\n",
      "DepthwiseSeparableConv-18         [-1, 24, 190, 190]               0\n",
      "           Conv2d-19         [-1, 24, 190, 190]             216\n",
      "         Identity-20         [-1, 24, 190, 190]               0\n",
      "             SiLU-21         [-1, 24, 190, 190]               0\n",
      "   BatchNormAct2d-22         [-1, 24, 190, 190]              48\n",
      "           Conv2d-23              [-1, 6, 1, 1]             150\n",
      "             SiLU-24              [-1, 6, 1, 1]               0\n",
      "           Conv2d-25             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-26             [-1, 24, 1, 1]               0\n",
      "    SqueezeExcite-27         [-1, 24, 190, 190]               0\n",
      "           Conv2d-28         [-1, 24, 190, 190]             576\n",
      "         Identity-29         [-1, 24, 190, 190]               0\n",
      "         Identity-30         [-1, 24, 190, 190]               0\n",
      "   BatchNormAct2d-31         [-1, 24, 190, 190]              48\n",
      "         Identity-32         [-1, 24, 190, 190]               0\n",
      "DepthwiseSeparableConv-33         [-1, 24, 190, 190]               0\n",
      "           Conv2d-34        [-1, 144, 190, 190]           3,456\n",
      "         Identity-35        [-1, 144, 190, 190]               0\n",
      "             SiLU-36        [-1, 144, 190, 190]               0\n",
      "   BatchNormAct2d-37        [-1, 144, 190, 190]             288\n",
      "           Conv2d-38          [-1, 144, 95, 95]           1,296\n",
      "         Identity-39          [-1, 144, 95, 95]               0\n",
      "             SiLU-40          [-1, 144, 95, 95]               0\n",
      "   BatchNormAct2d-41          [-1, 144, 95, 95]             288\n",
      "           Conv2d-42              [-1, 6, 1, 1]             870\n",
      "             SiLU-43              [-1, 6, 1, 1]               0\n",
      "           Conv2d-44            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-45            [-1, 144, 1, 1]               0\n",
      "    SqueezeExcite-46          [-1, 144, 95, 95]               0\n",
      "           Conv2d-47           [-1, 32, 95, 95]           4,608\n",
      "         Identity-48           [-1, 32, 95, 95]               0\n",
      "         Identity-49           [-1, 32, 95, 95]               0\n",
      "   BatchNormAct2d-50           [-1, 32, 95, 95]              64\n",
      " InvertedResidual-51           [-1, 32, 95, 95]               0\n",
      "           Conv2d-52          [-1, 192, 95, 95]           6,144\n",
      "         Identity-53          [-1, 192, 95, 95]               0\n",
      "             SiLU-54          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-55          [-1, 192, 95, 95]             384\n",
      "           Conv2d-56          [-1, 192, 95, 95]           1,728\n",
      "         Identity-57          [-1, 192, 95, 95]               0\n",
      "             SiLU-58          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-59          [-1, 192, 95, 95]             384\n",
      "           Conv2d-60              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-61              [-1, 8, 1, 1]               0\n",
      "           Conv2d-62            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-63            [-1, 192, 1, 1]               0\n",
      "    SqueezeExcite-64          [-1, 192, 95, 95]               0\n",
      "           Conv2d-65           [-1, 32, 95, 95]           6,144\n",
      "         Identity-66           [-1, 32, 95, 95]               0\n",
      "         Identity-67           [-1, 32, 95, 95]               0\n",
      "   BatchNormAct2d-68           [-1, 32, 95, 95]              64\n",
      "         Identity-69           [-1, 32, 95, 95]               0\n",
      " InvertedResidual-70           [-1, 32, 95, 95]               0\n",
      "           Conv2d-71          [-1, 192, 95, 95]           6,144\n",
      "         Identity-72          [-1, 192, 95, 95]               0\n",
      "             SiLU-73          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-74          [-1, 192, 95, 95]             384\n",
      "           Conv2d-75          [-1, 192, 95, 95]           1,728\n",
      "         Identity-76          [-1, 192, 95, 95]               0\n",
      "             SiLU-77          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-78          [-1, 192, 95, 95]             384\n",
      "           Conv2d-79              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-80              [-1, 8, 1, 1]               0\n",
      "           Conv2d-81            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-82            [-1, 192, 1, 1]               0\n",
      "    SqueezeExcite-83          [-1, 192, 95, 95]               0\n",
      "           Conv2d-84           [-1, 32, 95, 95]           6,144\n",
      "         Identity-85           [-1, 32, 95, 95]               0\n",
      "         Identity-86           [-1, 32, 95, 95]               0\n",
      "   BatchNormAct2d-87           [-1, 32, 95, 95]              64\n",
      "         Identity-88           [-1, 32, 95, 95]               0\n",
      " InvertedResidual-89           [-1, 32, 95, 95]               0\n",
      "           Conv2d-90          [-1, 192, 95, 95]           6,144\n",
      "         Identity-91          [-1, 192, 95, 95]               0\n",
      "             SiLU-92          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-93          [-1, 192, 95, 95]             384\n",
      "           Conv2d-94          [-1, 192, 95, 95]           1,728\n",
      "         Identity-95          [-1, 192, 95, 95]               0\n",
      "             SiLU-96          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-97          [-1, 192, 95, 95]             384\n",
      "           Conv2d-98              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-99              [-1, 8, 1, 1]               0\n",
      "          Conv2d-100            [-1, 192, 1, 1]           1,728\n",
      "         Sigmoid-101            [-1, 192, 1, 1]               0\n",
      "   SqueezeExcite-102          [-1, 192, 95, 95]               0\n",
      "          Conv2d-103           [-1, 32, 95, 95]           6,144\n",
      "        Identity-104           [-1, 32, 95, 95]               0\n",
      "        Identity-105           [-1, 32, 95, 95]               0\n",
      "  BatchNormAct2d-106           [-1, 32, 95, 95]              64\n",
      "        Identity-107           [-1, 32, 95, 95]               0\n",
      "InvertedResidual-108           [-1, 32, 95, 95]               0\n",
      "          Conv2d-109          [-1, 192, 95, 95]           6,144\n",
      "        Identity-110          [-1, 192, 95, 95]               0\n",
      "            SiLU-111          [-1, 192, 95, 95]               0\n",
      "  BatchNormAct2d-112          [-1, 192, 95, 95]             384\n",
      "          Conv2d-113          [-1, 192, 48, 48]           4,800\n",
      "        Identity-114          [-1, 192, 48, 48]               0\n",
      "            SiLU-115          [-1, 192, 48, 48]               0\n",
      "  BatchNormAct2d-116          [-1, 192, 48, 48]             384\n",
      "          Conv2d-117              [-1, 8, 1, 1]           1,544\n",
      "            SiLU-118              [-1, 8, 1, 1]               0\n",
      "          Conv2d-119            [-1, 192, 1, 1]           1,728\n",
      "         Sigmoid-120            [-1, 192, 1, 1]               0\n",
      "   SqueezeExcite-121          [-1, 192, 48, 48]               0\n",
      "          Conv2d-122           [-1, 56, 48, 48]          10,752\n",
      "        Identity-123           [-1, 56, 48, 48]               0\n",
      "        Identity-124           [-1, 56, 48, 48]               0\n",
      "  BatchNormAct2d-125           [-1, 56, 48, 48]             112\n",
      "InvertedResidual-126           [-1, 56, 48, 48]               0\n",
      "          Conv2d-127          [-1, 336, 48, 48]          18,816\n",
      "        Identity-128          [-1, 336, 48, 48]               0\n",
      "            SiLU-129          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-130          [-1, 336, 48, 48]             672\n",
      "          Conv2d-131          [-1, 336, 48, 48]           8,400\n",
      "        Identity-132          [-1, 336, 48, 48]               0\n",
      "            SiLU-133          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-134          [-1, 336, 48, 48]             672\n",
      "          Conv2d-135             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-136             [-1, 14, 1, 1]               0\n",
      "          Conv2d-137            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-138            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-139          [-1, 336, 48, 48]               0\n",
      "          Conv2d-140           [-1, 56, 48, 48]          18,816\n",
      "        Identity-141           [-1, 56, 48, 48]               0\n",
      "        Identity-142           [-1, 56, 48, 48]               0\n",
      "  BatchNormAct2d-143           [-1, 56, 48, 48]             112\n",
      "        Identity-144           [-1, 56, 48, 48]               0\n",
      "InvertedResidual-145           [-1, 56, 48, 48]               0\n",
      "          Conv2d-146          [-1, 336, 48, 48]          18,816\n",
      "        Identity-147          [-1, 336, 48, 48]               0\n",
      "            SiLU-148          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-149          [-1, 336, 48, 48]             672\n",
      "          Conv2d-150          [-1, 336, 48, 48]           8,400\n",
      "        Identity-151          [-1, 336, 48, 48]               0\n",
      "            SiLU-152          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-153          [-1, 336, 48, 48]             672\n",
      "          Conv2d-154             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-155             [-1, 14, 1, 1]               0\n",
      "          Conv2d-156            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-157            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-158          [-1, 336, 48, 48]               0\n",
      "          Conv2d-159           [-1, 56, 48, 48]          18,816\n",
      "        Identity-160           [-1, 56, 48, 48]               0\n",
      "        Identity-161           [-1, 56, 48, 48]               0\n",
      "  BatchNormAct2d-162           [-1, 56, 48, 48]             112\n",
      "        Identity-163           [-1, 56, 48, 48]               0\n",
      "InvertedResidual-164           [-1, 56, 48, 48]               0\n",
      "          Conv2d-165          [-1, 336, 48, 48]          18,816\n",
      "        Identity-166          [-1, 336, 48, 48]               0\n",
      "            SiLU-167          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-168          [-1, 336, 48, 48]             672\n",
      "          Conv2d-169          [-1, 336, 48, 48]           8,400\n",
      "        Identity-170          [-1, 336, 48, 48]               0\n",
      "            SiLU-171          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-172          [-1, 336, 48, 48]             672\n",
      "          Conv2d-173             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-174             [-1, 14, 1, 1]               0\n",
      "          Conv2d-175            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-176            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-177          [-1, 336, 48, 48]               0\n",
      "          Conv2d-178           [-1, 56, 48, 48]          18,816\n",
      "        Identity-179           [-1, 56, 48, 48]               0\n",
      "        Identity-180           [-1, 56, 48, 48]               0\n",
      "  BatchNormAct2d-181           [-1, 56, 48, 48]             112\n",
      "        Identity-182           [-1, 56, 48, 48]               0\n",
      "InvertedResidual-183           [-1, 56, 48, 48]               0\n",
      "          Conv2d-184          [-1, 336, 48, 48]          18,816\n",
      "        Identity-185          [-1, 336, 48, 48]               0\n",
      "            SiLU-186          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-187          [-1, 336, 48, 48]             672\n",
      "          Conv2d-188          [-1, 336, 24, 24]           3,024\n",
      "        Identity-189          [-1, 336, 24, 24]               0\n",
      "            SiLU-190          [-1, 336, 24, 24]               0\n",
      "  BatchNormAct2d-191          [-1, 336, 24, 24]             672\n",
      "          Conv2d-192             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-193             [-1, 14, 1, 1]               0\n",
      "          Conv2d-194            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-195            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-196          [-1, 336, 24, 24]               0\n",
      "          Conv2d-197          [-1, 112, 24, 24]          37,632\n",
      "        Identity-198          [-1, 112, 24, 24]               0\n",
      "        Identity-199          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-200          [-1, 112, 24, 24]             224\n",
      "InvertedResidual-201          [-1, 112, 24, 24]               0\n",
      "          Conv2d-202          [-1, 672, 24, 24]          75,264\n",
      "        Identity-203          [-1, 672, 24, 24]               0\n",
      "            SiLU-204          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-205          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-206          [-1, 672, 24, 24]           6,048\n",
      "        Identity-207          [-1, 672, 24, 24]               0\n",
      "            SiLU-208          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-209          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-210             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-211             [-1, 28, 1, 1]               0\n",
      "          Conv2d-212            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-213            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-214          [-1, 672, 24, 24]               0\n",
      "          Conv2d-215          [-1, 112, 24, 24]          75,264\n",
      "        Identity-216          [-1, 112, 24, 24]               0\n",
      "        Identity-217          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-218          [-1, 112, 24, 24]             224\n",
      "        Identity-219          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-220          [-1, 112, 24, 24]               0\n",
      "          Conv2d-221          [-1, 672, 24, 24]          75,264\n",
      "        Identity-222          [-1, 672, 24, 24]               0\n",
      "            SiLU-223          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-224          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-225          [-1, 672, 24, 24]           6,048\n",
      "        Identity-226          [-1, 672, 24, 24]               0\n",
      "            SiLU-227          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-228          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-229             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-230             [-1, 28, 1, 1]               0\n",
      "          Conv2d-231            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-232            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-233          [-1, 672, 24, 24]               0\n",
      "          Conv2d-234          [-1, 112, 24, 24]          75,264\n",
      "        Identity-235          [-1, 112, 24, 24]               0\n",
      "        Identity-236          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-237          [-1, 112, 24, 24]             224\n",
      "        Identity-238          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-239          [-1, 112, 24, 24]               0\n",
      "          Conv2d-240          [-1, 672, 24, 24]          75,264\n",
      "        Identity-241          [-1, 672, 24, 24]               0\n",
      "            SiLU-242          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-243          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-244          [-1, 672, 24, 24]           6,048\n",
      "        Identity-245          [-1, 672, 24, 24]               0\n",
      "            SiLU-246          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-247          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-248             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-249             [-1, 28, 1, 1]               0\n",
      "          Conv2d-250            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-251            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-252          [-1, 672, 24, 24]               0\n",
      "          Conv2d-253          [-1, 112, 24, 24]          75,264\n",
      "        Identity-254          [-1, 112, 24, 24]               0\n",
      "        Identity-255          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-256          [-1, 112, 24, 24]             224\n",
      "        Identity-257          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-258          [-1, 112, 24, 24]               0\n",
      "          Conv2d-259          [-1, 672, 24, 24]          75,264\n",
      "        Identity-260          [-1, 672, 24, 24]               0\n",
      "            SiLU-261          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-262          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-263          [-1, 672, 24, 24]           6,048\n",
      "        Identity-264          [-1, 672, 24, 24]               0\n",
      "            SiLU-265          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-266          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-267             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-268             [-1, 28, 1, 1]               0\n",
      "          Conv2d-269            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-270            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-271          [-1, 672, 24, 24]               0\n",
      "          Conv2d-272          [-1, 112, 24, 24]          75,264\n",
      "        Identity-273          [-1, 112, 24, 24]               0\n",
      "        Identity-274          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-275          [-1, 112, 24, 24]             224\n",
      "        Identity-276          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-277          [-1, 112, 24, 24]               0\n",
      "          Conv2d-278          [-1, 672, 24, 24]          75,264\n",
      "        Identity-279          [-1, 672, 24, 24]               0\n",
      "            SiLU-280          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-281          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-282          [-1, 672, 24, 24]           6,048\n",
      "        Identity-283          [-1, 672, 24, 24]               0\n",
      "            SiLU-284          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-285          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-286             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-287             [-1, 28, 1, 1]               0\n",
      "          Conv2d-288            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-289            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-290          [-1, 672, 24, 24]               0\n",
      "          Conv2d-291          [-1, 112, 24, 24]          75,264\n",
      "        Identity-292          [-1, 112, 24, 24]               0\n",
      "        Identity-293          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-294          [-1, 112, 24, 24]             224\n",
      "        Identity-295          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-296          [-1, 112, 24, 24]               0\n",
      "          Conv2d-297          [-1, 672, 24, 24]          75,264\n",
      "        Identity-298          [-1, 672, 24, 24]               0\n",
      "            SiLU-299          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-300          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-301          [-1, 672, 24, 24]          16,800\n",
      "        Identity-302          [-1, 672, 24, 24]               0\n",
      "            SiLU-303          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-304          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-305             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-306             [-1, 28, 1, 1]               0\n",
      "          Conv2d-307            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-308            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-309          [-1, 672, 24, 24]               0\n",
      "          Conv2d-310          [-1, 160, 24, 24]         107,520\n",
      "        Identity-311          [-1, 160, 24, 24]               0\n",
      "        Identity-312          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-313          [-1, 160, 24, 24]             320\n",
      "InvertedResidual-314          [-1, 160, 24, 24]               0\n",
      "          Conv2d-315          [-1, 960, 24, 24]         153,600\n",
      "        Identity-316          [-1, 960, 24, 24]               0\n",
      "            SiLU-317          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-318          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-319          [-1, 960, 24, 24]          24,000\n",
      "        Identity-320          [-1, 960, 24, 24]               0\n",
      "            SiLU-321          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-322          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-323             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-324             [-1, 40, 1, 1]               0\n",
      "          Conv2d-325            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-326            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-327          [-1, 960, 24, 24]               0\n",
      "          Conv2d-328          [-1, 160, 24, 24]         153,600\n",
      "        Identity-329          [-1, 160, 24, 24]               0\n",
      "        Identity-330          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-331          [-1, 160, 24, 24]             320\n",
      "        Identity-332          [-1, 160, 24, 24]               0\n",
      "InvertedResidual-333          [-1, 160, 24, 24]               0\n",
      "          Conv2d-334          [-1, 960, 24, 24]         153,600\n",
      "        Identity-335          [-1, 960, 24, 24]               0\n",
      "            SiLU-336          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-337          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-338          [-1, 960, 24, 24]          24,000\n",
      "        Identity-339          [-1, 960, 24, 24]               0\n",
      "            SiLU-340          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-341          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-342             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-343             [-1, 40, 1, 1]               0\n",
      "          Conv2d-344            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-345            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-346          [-1, 960, 24, 24]               0\n",
      "          Conv2d-347          [-1, 160, 24, 24]         153,600\n",
      "        Identity-348          [-1, 160, 24, 24]               0\n",
      "        Identity-349          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-350          [-1, 160, 24, 24]             320\n",
      "        Identity-351          [-1, 160, 24, 24]               0\n",
      "InvertedResidual-352          [-1, 160, 24, 24]               0\n",
      "          Conv2d-353          [-1, 960, 24, 24]         153,600\n",
      "        Identity-354          [-1, 960, 24, 24]               0\n",
      "            SiLU-355          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-356          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-357          [-1, 960, 24, 24]          24,000\n",
      "        Identity-358          [-1, 960, 24, 24]               0\n",
      "            SiLU-359          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-360          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-361             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-362             [-1, 40, 1, 1]               0\n",
      "          Conv2d-363            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-364            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-365          [-1, 960, 24, 24]               0\n",
      "          Conv2d-366          [-1, 160, 24, 24]         153,600\n",
      "        Identity-367          [-1, 160, 24, 24]               0\n",
      "        Identity-368          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-369          [-1, 160, 24, 24]             320\n",
      "        Identity-370          [-1, 160, 24, 24]               0\n",
      "InvertedResidual-371          [-1, 160, 24, 24]               0\n",
      "          Conv2d-372          [-1, 960, 24, 24]         153,600\n",
      "        Identity-373          [-1, 960, 24, 24]               0\n",
      "            SiLU-374          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-375          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-376          [-1, 960, 24, 24]          24,000\n",
      "        Identity-377          [-1, 960, 24, 24]               0\n",
      "            SiLU-378          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-379          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-380             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-381             [-1, 40, 1, 1]               0\n",
      "          Conv2d-382            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-383            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-384          [-1, 960, 24, 24]               0\n",
      "          Conv2d-385          [-1, 160, 24, 24]         153,600\n",
      "        Identity-386          [-1, 160, 24, 24]               0\n",
      "        Identity-387          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-388          [-1, 160, 24, 24]             320\n",
      "        Identity-389          [-1, 160, 24, 24]               0\n",
      "InvertedResidual-390          [-1, 160, 24, 24]               0\n",
      "          Conv2d-391          [-1, 960, 24, 24]         153,600\n",
      "        Identity-392          [-1, 960, 24, 24]               0\n",
      "            SiLU-393          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-394          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-395          [-1, 960, 24, 24]          24,000\n",
      "        Identity-396          [-1, 960, 24, 24]               0\n",
      "            SiLU-397          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-398          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-399             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-400             [-1, 40, 1, 1]               0\n",
      "          Conv2d-401            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-402            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-403          [-1, 960, 24, 24]               0\n",
      "          Conv2d-404          [-1, 160, 24, 24]         153,600\n",
      "        Identity-405          [-1, 160, 24, 24]               0\n",
      "        Identity-406          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-407          [-1, 160, 24, 24]             320\n",
      "        Identity-408          [-1, 160, 24, 24]               0\n",
      "InvertedResidual-409          [-1, 160, 24, 24]               0\n",
      "          Conv2d-410          [-1, 960, 24, 24]         153,600\n",
      "        Identity-411          [-1, 960, 24, 24]               0\n",
      "            SiLU-412          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-413          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-414          [-1, 960, 12, 12]          24,000\n",
      "        Identity-415          [-1, 960, 12, 12]               0\n",
      "            SiLU-416          [-1, 960, 12, 12]               0\n",
      "  BatchNormAct2d-417          [-1, 960, 12, 12]           1,920\n",
      "          Conv2d-418             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-419             [-1, 40, 1, 1]               0\n",
      "          Conv2d-420            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-421            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-422          [-1, 960, 12, 12]               0\n",
      "          Conv2d-423          [-1, 272, 12, 12]         261,120\n",
      "        Identity-424          [-1, 272, 12, 12]               0\n",
      "        Identity-425          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-426          [-1, 272, 12, 12]             544\n",
      "InvertedResidual-427          [-1, 272, 12, 12]               0\n",
      "          Conv2d-428         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-429         [-1, 1632, 12, 12]               0\n",
      "            SiLU-430         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-431         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-432         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-433         [-1, 1632, 12, 12]               0\n",
      "            SiLU-434         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-435         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-436             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-437             [-1, 68, 1, 1]               0\n",
      "          Conv2d-438           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-439           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-440         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-441          [-1, 272, 12, 12]         443,904\n",
      "        Identity-442          [-1, 272, 12, 12]               0\n",
      "        Identity-443          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-444          [-1, 272, 12, 12]             544\n",
      "        Identity-445          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-446          [-1, 272, 12, 12]               0\n",
      "          Conv2d-447         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-448         [-1, 1632, 12, 12]               0\n",
      "            SiLU-449         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-450         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-451         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-452         [-1, 1632, 12, 12]               0\n",
      "            SiLU-453         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-454         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-455             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-456             [-1, 68, 1, 1]               0\n",
      "          Conv2d-457           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-458           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-459         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-460          [-1, 272, 12, 12]         443,904\n",
      "        Identity-461          [-1, 272, 12, 12]               0\n",
      "        Identity-462          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-463          [-1, 272, 12, 12]             544\n",
      "        Identity-464          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-465          [-1, 272, 12, 12]               0\n",
      "          Conv2d-466         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-467         [-1, 1632, 12, 12]               0\n",
      "            SiLU-468         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-469         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-470         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-471         [-1, 1632, 12, 12]               0\n",
      "            SiLU-472         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-473         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-474             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-475             [-1, 68, 1, 1]               0\n",
      "          Conv2d-476           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-477           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-478         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-479          [-1, 272, 12, 12]         443,904\n",
      "        Identity-480          [-1, 272, 12, 12]               0\n",
      "        Identity-481          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-482          [-1, 272, 12, 12]             544\n",
      "        Identity-483          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-484          [-1, 272, 12, 12]               0\n",
      "          Conv2d-485         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-486         [-1, 1632, 12, 12]               0\n",
      "            SiLU-487         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-488         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-489         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-490         [-1, 1632, 12, 12]               0\n",
      "            SiLU-491         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-492         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-493             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-494             [-1, 68, 1, 1]               0\n",
      "          Conv2d-495           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-496           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-497         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-498          [-1, 272, 12, 12]         443,904\n",
      "        Identity-499          [-1, 272, 12, 12]               0\n",
      "        Identity-500          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-501          [-1, 272, 12, 12]             544\n",
      "        Identity-502          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-503          [-1, 272, 12, 12]               0\n",
      "          Conv2d-504         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-505         [-1, 1632, 12, 12]               0\n",
      "            SiLU-506         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-507         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-508         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-509         [-1, 1632, 12, 12]               0\n",
      "            SiLU-510         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-511         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-512             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-513             [-1, 68, 1, 1]               0\n",
      "          Conv2d-514           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-515           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-516         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-517          [-1, 272, 12, 12]         443,904\n",
      "        Identity-518          [-1, 272, 12, 12]               0\n",
      "        Identity-519          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-520          [-1, 272, 12, 12]             544\n",
      "        Identity-521          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-522          [-1, 272, 12, 12]               0\n",
      "          Conv2d-523         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-524         [-1, 1632, 12, 12]               0\n",
      "            SiLU-525         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-526         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-527         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-528         [-1, 1632, 12, 12]               0\n",
      "            SiLU-529         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-530         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-531             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-532             [-1, 68, 1, 1]               0\n",
      "          Conv2d-533           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-534           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-535         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-536          [-1, 272, 12, 12]         443,904\n",
      "        Identity-537          [-1, 272, 12, 12]               0\n",
      "        Identity-538          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-539          [-1, 272, 12, 12]             544\n",
      "        Identity-540          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-541          [-1, 272, 12, 12]               0\n",
      "          Conv2d-542         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-543         [-1, 1632, 12, 12]               0\n",
      "            SiLU-544         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-545         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-546         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-547         [-1, 1632, 12, 12]               0\n",
      "            SiLU-548         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-549         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-550             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-551             [-1, 68, 1, 1]               0\n",
      "          Conv2d-552           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-553           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-554         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-555          [-1, 272, 12, 12]         443,904\n",
      "        Identity-556          [-1, 272, 12, 12]               0\n",
      "        Identity-557          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-558          [-1, 272, 12, 12]             544\n",
      "        Identity-559          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-560          [-1, 272, 12, 12]               0\n",
      "          Conv2d-561         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-562         [-1, 1632, 12, 12]               0\n",
      "            SiLU-563         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-564         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-565         [-1, 1632, 12, 12]          14,688\n",
      "        Identity-566         [-1, 1632, 12, 12]               0\n",
      "            SiLU-567         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-568         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-569             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-570             [-1, 68, 1, 1]               0\n",
      "          Conv2d-571           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-572           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-573         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-574          [-1, 448, 12, 12]         731,136\n",
      "        Identity-575          [-1, 448, 12, 12]               0\n",
      "        Identity-576          [-1, 448, 12, 12]               0\n",
      "  BatchNormAct2d-577          [-1, 448, 12, 12]             896\n",
      "InvertedResidual-578          [-1, 448, 12, 12]               0\n",
      "          Conv2d-579         [-1, 2688, 12, 12]       1,204,224\n",
      "        Identity-580         [-1, 2688, 12, 12]               0\n",
      "            SiLU-581         [-1, 2688, 12, 12]               0\n",
      "  BatchNormAct2d-582         [-1, 2688, 12, 12]           5,376\n",
      "          Conv2d-583         [-1, 2688, 12, 12]          24,192\n",
      "        Identity-584         [-1, 2688, 12, 12]               0\n",
      "            SiLU-585         [-1, 2688, 12, 12]               0\n",
      "  BatchNormAct2d-586         [-1, 2688, 12, 12]           5,376\n",
      "          Conv2d-587            [-1, 112, 1, 1]         301,168\n",
      "            SiLU-588            [-1, 112, 1, 1]               0\n",
      "          Conv2d-589           [-1, 2688, 1, 1]         303,744\n",
      "         Sigmoid-590           [-1, 2688, 1, 1]               0\n",
      "   SqueezeExcite-591         [-1, 2688, 12, 12]               0\n",
      "          Conv2d-592          [-1, 448, 12, 12]       1,204,224\n",
      "        Identity-593          [-1, 448, 12, 12]               0\n",
      "        Identity-594          [-1, 448, 12, 12]               0\n",
      "  BatchNormAct2d-595          [-1, 448, 12, 12]             896\n",
      "        Identity-596          [-1, 448, 12, 12]               0\n",
      "InvertedResidual-597          [-1, 448, 12, 12]               0\n",
      "          Conv2d-598         [-1, 1792, 12, 12]         802,816\n",
      "        Identity-599         [-1, 1792, 12, 12]               0\n",
      "            SiLU-600         [-1, 1792, 12, 12]               0\n",
      "  BatchNormAct2d-601         [-1, 1792, 12, 12]           3,584\n",
      "AdaptiveAvgPool2d-602           [-1, 1792, 1, 1]               0\n",
      "         Flatten-603                 [-1, 1792]               0\n",
      "SelectAdaptivePool2d-604                 [-1, 1792]               0\n",
      "          Linear-605                   [-1, 17]          30,481\n",
      "================================================================\n",
      "Total params: 17,579,097\n",
      "Trainable params: 17,579,097\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.65\n",
      "Forward/backward pass size (MB): 1720.25\n",
      "Params size (MB): 67.06\n",
      "Estimated Total Size (MB): 1788.96\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5300: 100%|██████████| 44/44 [00:13<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.3605, Train Acc: 0.5970, Train F1: 0.5941\n",
      "Val Loss: 0.3334, Val Acc: 0.8963, Val F1: 0.8970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1011: 100%|██████████| 44/44 [00:13<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.3284, Train Acc: 0.8861, Train F1: 0.8868\n",
      "Val Loss: 0.3426, Val Acc: 0.8703, Val F1: 0.8629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4260: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.2198, Train Acc: 0.9221, Train F1: 0.9212\n",
      "Val Loss: 0.2252, Val Acc: 0.9135, Val F1: 0.9147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3319: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1640, Train Acc: 0.9438, Train F1: 0.9431\n",
      "Val Loss: 0.2022, Val Acc: 0.9366, Val F1: 0.9376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2850: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.1202, Train Acc: 0.9546, Train F1: 0.9542\n",
      "Val Loss: 0.2313, Val Acc: 0.9280, Val F1: 0.9273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0205: 100%|██████████| 44/44 [00:13<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.1014, Train Acc: 0.9603, Train F1: 0.9602\n",
      "Val Loss: 0.2352, Val Acc: 0.9135, Val F1: 0.9157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0464: 100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0694, Train Acc: 0.9784, Train F1: 0.9783\n",
      "Val Loss: 0.2227, Val Acc: 0.9222, Val F1: 0.9249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0093: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0664, Train Acc: 0.9798, Train F1: 0.9798\n",
      "Val Loss: 0.1877, Val Acc: 0.9280, Val F1: 0.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0245: 100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0463, Train Acc: 0.9805, Train F1: 0.9805\n",
      "Val Loss: 0.2347, Val Acc: 0.9222, Val F1: 0.9244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0059: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0370, Train Acc: 0.9906, Train F1: 0.9906\n",
      "Val Loss: 0.2820, Val Acc: 0.9222, Val F1: 0.9236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0384: 100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "Train Loss: 0.0461, Train Acc: 0.9877, Train F1: 0.9877\n",
      "Val Loss: 0.2628, Val Acc: 0.9308, Val F1: 0.9314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0237: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "Train Loss: 0.0320, Train Acc: 0.9877, Train F1: 0.9877\n",
      "Val Loss: 0.2001, Val Acc: 0.9452, Val F1: 0.9456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1611: 100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "Train Loss: 0.0231, Train Acc: 0.9935, Train F1: 0.9935\n",
      "Val Loss: 0.2207, Val Acc: 0.9395, Val F1: 0.9401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0149: 100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "Train Loss: 0.0390, Train Acc: 0.9877, Train F1: 0.9877\n",
      "Val Loss: 0.2678, Val Acc: 0.9251, Val F1: 0.9266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0036: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "Train Loss: 0.0160, Train Acc: 0.9942, Train F1: 0.9942\n",
      "Val Loss: 0.2327, Val Acc: 0.9395, Val F1: 0.9416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0036: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "Train Loss: 0.0092, Train Acc: 0.9978, Train F1: 0.9978\n",
      "Val Loss: 0.2572, Val Acc: 0.9395, Val F1: 0.9403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0343: 100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "Train Loss: 0.0150, Train Acc: 0.9942, Train F1: 0.9942\n",
      "Val Loss: 0.2537, Val Acc: 0.9424, Val F1: 0.9435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0647: 100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "Train Loss: 0.0194, Train Acc: 0.9957, Train F1: 0.9957\n",
      "Val Loss: 0.2620, Val Acc: 0.9308, Val F1: 0.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0008: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "Train Loss: 0.0064, Train Acc: 0.9986, Train F1: 0.9985\n",
      "Val Loss: 0.2418, Val Acc: 0.9366, Val F1: 0.9384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0399: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "Train Loss: 0.0101, Train Acc: 0.9957, Train F1: 0.9957\n",
      "Val Loss: 0.2378, Val Acc: 0.9395, Val F1: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0038: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "Train Loss: 0.0064, Train Acc: 0.9986, Train F1: 0.9985\n",
      "Val Loss: 0.2674, Val Acc: 0.9424, Val F1: 0.9427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0330: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "Train Loss: 0.0098, Train Acc: 0.9978, Train F1: 0.9978\n",
      "Val Loss: 0.2376, Val Acc: 0.9510, Val F1: 0.9523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "Train Loss: 0.0028, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2287, Val Acc: 0.9481, Val F1: 0.9497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0018: 100%|██████████| 44/44 [00:13<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "Train Loss: 0.0079, Train Acc: 0.9964, Train F1: 0.9964\n",
      "Val Loss: 0.2308, Val Acc: 0.9452, Val F1: 0.9464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0022: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "Train Loss: 0.0054, Train Acc: 0.9978, Train F1: 0.9978\n",
      "Val Loss: 0.2240, Val Acc: 0.9424, Val F1: 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0375: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "Train Loss: 0.0029, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2264, Val Acc: 0.9424, Val F1: 0.9435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0039: 100%|██████████| 44/44 [00:13<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "Train Loss: 0.0021, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2343, Val Acc: 0.9452, Val F1: 0.9464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "Train Loss: 0.0070, Train Acc: 0.9978, Train F1: 0.9978\n",
      "Val Loss: 0.2348, Val Acc: 0.9424, Val F1: 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0020: 100%|██████████| 44/44 [00:13<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "Train Loss: 0.0027, Train Acc: 0.9993, Train F1: 0.9993\n",
      "Val Loss: 0.2287, Val Acc: 0.9452, Val F1: 0.9465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0017: 100%|██████████| 44/44 [00:13<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "Train Loss: 0.0033, Train Acc: 0.9993, Train F1: 0.9993\n",
      "Val Loss: 0.2339, Val Acc: 0.9452, Val F1: 0.9462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:08<00:00, 11.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '/data/ephemeral/home/data/'\n",
    "    model_name = 'efficientnet_b4'\n",
    "    img_size = 380  # EfficientNet-B4에 적합한 이미지 크기\n",
    "    LR = 5e-4  # 학습률 조정\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32  # 배치 크기 감소\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),  # 이미지 크기 조정\n",
    "        A.HorizontalFlip(p=0.5),  # 확률적으로 수평 플립\n",
    "        A.VerticalFlip(p=0.5),  # 확률적으로 수직 플립\n",
    "        A.RandomRotate90(p=0.5),  # 확률적으로 90도 회전\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),  # 이동, 크기 조정 및 회전\n",
    "        A.RandomBrightnessContrast(p=0.5),  # 확률적으로 밝기 및 대비 조정\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 이미지 정규화\n",
    "        ToTensorV2(),  # 이미지 텐서로 변환\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),  # 이미지 크기 조정\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 이미지 정규화\n",
    "        ToTensorV2(),  # 이미지 텐서로 변환\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_add.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    # 데이터셋 객체를 생성\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_add\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_add\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test\"), transform=val_transform)\n",
    "\n",
    "    # 데이터 로더 객체를 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred_efficientNetB4.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = '/data/ephemeral/home/data/'\n",
    "df = pd.read_csv(os.path.join(data_path, \"train_add.csv\"))\n",
    "\n",
    "# K-Fold 설정\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(df, data_path, fold, kfold, batch_size, num_workers):\n",
    "    train_idx, val_idx = list(kfold.split(df))[fold]\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "    \n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_add\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_add\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return epoch_loss / len(train_loader), correct / total\n",
    "\n",
    "def validate(val_loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return epoch_loss / len(val_loader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/30\n",
      "Train Loss: 1.4633, Train Acc: 0.5775\n",
      "Val Loss: 0.4290, Val Acc: 0.8588\n",
      "Epoch 2/30\n",
      "Train Loss: 0.3913, Train Acc: 0.8536\n",
      "Val Loss: 0.3210, Val Acc: 0.8790\n",
      "Epoch 3/30\n",
      "Train Loss: 0.2544, Train Acc: 0.9120\n",
      "Val Loss: 0.2606, Val Acc: 0.9049\n",
      "Epoch 4/30\n",
      "Train Loss: 0.1764, Train Acc: 0.9344\n",
      "Val Loss: 0.2659, Val Acc: 0.9107\n",
      "Epoch 5/30\n",
      "Train Loss: 0.1160, Train Acc: 0.9560\n",
      "Val Loss: 0.2936, Val Acc: 0.8963\n",
      "Epoch 6/30\n",
      "Train Loss: 0.1107, Train Acc: 0.9625\n",
      "Val Loss: 0.2073, Val Acc: 0.9337\n",
      "Epoch 7/30\n",
      "Train Loss: 0.0887, Train Acc: 0.9719\n",
      "Val Loss: 0.2133, Val Acc: 0.9222\n",
      "Epoch 8/30\n",
      "Train Loss: 0.0745, Train Acc: 0.9748\n",
      "Val Loss: 0.1766, Val Acc: 0.9280\n",
      "Epoch 9/30\n",
      "Train Loss: 0.0452, Train Acc: 0.9877\n",
      "Val Loss: 0.2504, Val Acc: 0.9193\n",
      "Epoch 10/30\n",
      "Train Loss: 0.0477, Train Acc: 0.9834\n",
      "Val Loss: 0.2085, Val Acc: 0.9280\n",
      "Epoch 11/30\n",
      "Train Loss: 0.0369, Train Acc: 0.9885\n",
      "Val Loss: 0.2213, Val Acc: 0.9280\n",
      "Epoch 12/30\n",
      "Train Loss: 0.0405, Train Acc: 0.9870\n",
      "Val Loss: 0.2473, Val Acc: 0.9280\n",
      "Epoch 13/30\n",
      "Train Loss: 0.0329, Train Acc: 0.9885\n",
      "Val Loss: 0.2518, Val Acc: 0.9251\n",
      "Epoch 14/30\n",
      "Train Loss: 0.0364, Train Acc: 0.9892\n",
      "Val Loss: 0.2151, Val Acc: 0.9280\n",
      "Epoch 15/30\n",
      "Train Loss: 0.0164, Train Acc: 0.9942\n",
      "Val Loss: 0.2134, Val Acc: 0.9395\n",
      "Epoch 16/30\n",
      "Train Loss: 0.0113, Train Acc: 0.9978\n",
      "Val Loss: 0.2201, Val Acc: 0.9337\n",
      "Epoch 17/30\n",
      "Train Loss: 0.0153, Train Acc: 0.9950\n",
      "Val Loss: 0.2092, Val Acc: 0.9452\n",
      "Epoch 18/30\n",
      "Train Loss: 0.0080, Train Acc: 0.9986\n",
      "Val Loss: 0.2288, Val Acc: 0.9337\n",
      "Epoch 19/30\n",
      "Train Loss: 0.0119, Train Acc: 0.9978\n",
      "Val Loss: 0.2390, Val Acc: 0.9395\n",
      "Epoch 20/30\n",
      "Train Loss: 0.0044, Train Acc: 0.9993\n",
      "Val Loss: 0.2722, Val Acc: 0.9366\n",
      "Epoch 21/30\n",
      "Train Loss: 0.0037, Train Acc: 0.9993\n",
      "Val Loss: 0.2650, Val Acc: 0.9337\n",
      "Epoch 22/30\n",
      "Train Loss: 0.0066, Train Acc: 0.9978\n",
      "Val Loss: 0.2786, Val Acc: 0.9337\n",
      "Epoch 23/30\n",
      "Train Loss: 0.0071, Train Acc: 0.9957\n",
      "Val Loss: 0.2770, Val Acc: 0.9308\n",
      "Epoch 24/30\n",
      "Train Loss: 0.0104, Train Acc: 0.9964\n",
      "Val Loss: 0.2673, Val Acc: 0.9308\n",
      "Epoch 25/30\n",
      "Train Loss: 0.0077, Train Acc: 0.9971\n",
      "Val Loss: 0.2595, Val Acc: 0.9280\n",
      "Epoch 26/30\n",
      "Train Loss: 0.0109, Train Acc: 0.9986\n",
      "Val Loss: 0.2508, Val Acc: 0.9366\n",
      "Epoch 27/30\n",
      "Train Loss: 0.0086, Train Acc: 0.9971\n",
      "Val Loss: 0.2539, Val Acc: 0.9395\n",
      "Epoch 28/30\n",
      "Train Loss: 0.0047, Train Acc: 0.9986\n",
      "Val Loss: 0.2448, Val Acc: 0.9395\n",
      "Epoch 29/30\n",
      "Train Loss: 0.0050, Train Acc: 0.9978\n",
      "Val Loss: 0.2598, Val Acc: 0.9366\n",
      "Epoch 30/30\n",
      "Train Loss: 0.0044, Train Acc: 0.9993\n",
      "Val Loss: 0.2439, Val Acc: 0.9337\n",
      "Fold 2/5\n",
      "Epoch 1/30\n",
      "Train Loss: 1.3463, Train Acc: 0.5905\n",
      "Val Loss: 0.3786, Val Acc: 0.8732\n",
      "Epoch 2/30\n",
      "Train Loss: 0.3224, Train Acc: 0.8782\n",
      "Val Loss: 0.2772, Val Acc: 0.8905\n",
      "Epoch 3/30\n",
      "Train Loss: 0.2144, Train Acc: 0.9185\n",
      "Val Loss: 0.2825, Val Acc: 0.8876\n",
      "Epoch 4/30\n",
      "Train Loss: 0.1413, Train Acc: 0.9503\n",
      "Val Loss: 0.2840, Val Acc: 0.8991\n",
      "Epoch 5/30\n",
      "Train Loss: 0.1018, Train Acc: 0.9654\n",
      "Val Loss: 0.3733, Val Acc: 0.8703\n",
      "Epoch 6/30\n",
      "Train Loss: 0.1127, Train Acc: 0.9582\n",
      "Val Loss: 0.3164, Val Acc: 0.9020\n",
      "Epoch 7/30\n",
      "Train Loss: 0.0875, Train Acc: 0.9697\n",
      "Val Loss: 0.3017, Val Acc: 0.8934\n",
      "Epoch 8/30\n",
      "Train Loss: 0.0629, Train Acc: 0.9791\n",
      "Val Loss: 0.3743, Val Acc: 0.9049\n",
      "Epoch 9/30\n",
      "Train Loss: 0.0556, Train Acc: 0.9784\n",
      "Val Loss: 0.3443, Val Acc: 0.8991\n",
      "Epoch 10/30\n",
      "Train Loss: 0.0420, Train Acc: 0.9863\n",
      "Val Loss: 0.3309, Val Acc: 0.8905\n",
      "Epoch 11/30\n",
      "Train Loss: 0.0460, Train Acc: 0.9820\n",
      "Val Loss: 0.3573, Val Acc: 0.9049\n",
      "Epoch 12/30\n",
      "Train Loss: 0.0358, Train Acc: 0.9863\n",
      "Val Loss: 0.4002, Val Acc: 0.9078\n",
      "Epoch 13/30\n",
      "Train Loss: 0.0281, Train Acc: 0.9892\n",
      "Val Loss: 0.4203, Val Acc: 0.8991\n",
      "Epoch 14/30\n",
      "Train Loss: 0.0286, Train Acc: 0.9928\n",
      "Val Loss: 0.4083, Val Acc: 0.9164\n",
      "Epoch 15/30\n",
      "Train Loss: 0.0367, Train Acc: 0.9863\n",
      "Val Loss: 0.3107, Val Acc: 0.8991\n",
      "Epoch 16/30\n",
      "Train Loss: 0.0169, Train Acc: 0.9964\n",
      "Val Loss: 0.2800, Val Acc: 0.9078\n",
      "Epoch 17/30\n",
      "Train Loss: 0.0103, Train Acc: 0.9978\n",
      "Val Loss: 0.3834, Val Acc: 0.9135\n",
      "Epoch 18/30\n",
      "Train Loss: 0.0102, Train Acc: 0.9964\n",
      "Val Loss: 0.3163, Val Acc: 0.9164\n",
      "Epoch 19/30\n",
      "Train Loss: 0.0093, Train Acc: 0.9978\n",
      "Val Loss: 0.3915, Val Acc: 0.9222\n",
      "Epoch 20/30\n",
      "Train Loss: 0.0155, Train Acc: 0.9957\n",
      "Val Loss: 0.3706, Val Acc: 0.9078\n",
      "Epoch 21/30\n",
      "Train Loss: 0.0210, Train Acc: 0.9957\n",
      "Val Loss: 0.3321, Val Acc: 0.9078\n",
      "Epoch 22/30\n",
      "Train Loss: 0.0107, Train Acc: 0.9964\n",
      "Val Loss: 0.3420, Val Acc: 0.9193\n",
      "Epoch 23/30\n",
      "Train Loss: 0.0118, Train Acc: 0.9978\n",
      "Val Loss: 0.3642, Val Acc: 0.9135\n",
      "Epoch 24/30\n",
      "Train Loss: 0.0076, Train Acc: 0.9978\n",
      "Val Loss: 0.3498, Val Acc: 0.9164\n",
      "Epoch 25/30\n",
      "Train Loss: 0.0059, Train Acc: 0.9978\n",
      "Val Loss: 0.3483, Val Acc: 0.9107\n",
      "Epoch 26/30\n",
      "Train Loss: 0.0052, Train Acc: 0.9993\n",
      "Val Loss: 0.3525, Val Acc: 0.9164\n",
      "Epoch 27/30\n",
      "Train Loss: 0.0097, Train Acc: 0.9971\n",
      "Val Loss: 0.3431, Val Acc: 0.9164\n",
      "Epoch 28/30\n",
      "Train Loss: 0.0057, Train Acc: 0.9986\n",
      "Val Loss: 0.3457, Val Acc: 0.9049\n",
      "Epoch 29/30\n",
      "Train Loss: 0.0037, Train Acc: 1.0000\n",
      "Val Loss: 0.3525, Val Acc: 0.9164\n",
      "Epoch 30/30\n",
      "Train Loss: 0.0119, Train Acc: 0.9978\n",
      "Val Loss: 0.3741, Val Acc: 0.9107\n",
      "Fold 3/5\n",
      "Epoch 1/30\n",
      "Train Loss: 1.3675, Train Acc: 0.5890\n",
      "Val Loss: 0.4824, Val Acc: 0.8501\n",
      "Epoch 2/30\n",
      "Train Loss: 0.3544, Train Acc: 0.8753\n",
      "Val Loss: 0.4263, Val Acc: 0.8444\n",
      "Epoch 3/30\n",
      "Train Loss: 0.2357, Train Acc: 0.9200\n",
      "Val Loss: 0.3465, Val Acc: 0.8732\n",
      "Epoch 4/30\n",
      "Train Loss: 0.1590, Train Acc: 0.9495\n",
      "Val Loss: 0.3713, Val Acc: 0.8703\n",
      "Epoch 5/30\n",
      "Train Loss: 0.1288, Train Acc: 0.9553\n",
      "Val Loss: 0.3568, Val Acc: 0.8847\n",
      "Epoch 6/30\n",
      "Train Loss: 0.1068, Train Acc: 0.9618\n",
      "Val Loss: 0.3761, Val Acc: 0.8847\n",
      "Epoch 7/30\n",
      "Train Loss: 0.0929, Train Acc: 0.9654\n",
      "Val Loss: 0.3498, Val Acc: 0.8790\n",
      "Epoch 8/30\n",
      "Train Loss: 0.0516, Train Acc: 0.9870\n",
      "Val Loss: 0.3875, Val Acc: 0.8761\n",
      "Epoch 9/30\n",
      "Train Loss: 0.0568, Train Acc: 0.9834\n",
      "Val Loss: 0.3605, Val Acc: 0.8991\n",
      "Epoch 10/30\n",
      "Train Loss: 0.0494, Train Acc: 0.9820\n",
      "Val Loss: 0.3777, Val Acc: 0.8790\n",
      "Epoch 11/30\n",
      "Train Loss: 0.0480, Train Acc: 0.9849\n",
      "Val Loss: 0.3454, Val Acc: 0.8876\n",
      "Epoch 12/30\n",
      "Train Loss: 0.0291, Train Acc: 0.9899\n",
      "Val Loss: 0.4751, Val Acc: 0.8790\n",
      "Epoch 13/30\n",
      "Train Loss: 0.0402, Train Acc: 0.9906\n",
      "Val Loss: 0.3840, Val Acc: 0.8934\n",
      "Epoch 14/30\n",
      "Train Loss: 0.0256, Train Acc: 0.9906\n",
      "Val Loss: 0.3924, Val Acc: 0.8905\n",
      "Epoch 15/30\n",
      "Train Loss: 0.0139, Train Acc: 0.9957\n",
      "Val Loss: 0.4228, Val Acc: 0.8732\n",
      "Epoch 16/30\n",
      "Train Loss: 0.0150, Train Acc: 0.9942\n",
      "Val Loss: 0.4338, Val Acc: 0.8905\n",
      "Epoch 17/30\n",
      "Train Loss: 0.0130, Train Acc: 0.9942\n",
      "Val Loss: 0.4052, Val Acc: 0.8963\n",
      "Epoch 18/30\n",
      "Train Loss: 0.0087, Train Acc: 0.9978\n",
      "Val Loss: 0.4811, Val Acc: 0.8876\n",
      "Epoch 19/30\n",
      "Train Loss: 0.0060, Train Acc: 0.9986\n",
      "Val Loss: 0.4623, Val Acc: 0.8905\n",
      "Epoch 20/30\n",
      "Train Loss: 0.0083, Train Acc: 0.9978\n",
      "Val Loss: 0.4789, Val Acc: 0.8934\n",
      "Epoch 21/30\n",
      "Train Loss: 0.0088, Train Acc: 0.9964\n",
      "Val Loss: 0.5193, Val Acc: 0.8847\n",
      "Epoch 22/30\n",
      "Train Loss: 0.0114, Train Acc: 0.9964\n",
      "Val Loss: 0.4866, Val Acc: 0.8934\n",
      "Epoch 23/30\n",
      "Train Loss: 0.0053, Train Acc: 0.9993\n",
      "Val Loss: 0.4879, Val Acc: 0.8963\n",
      "Epoch 24/30\n",
      "Train Loss: 0.0104, Train Acc: 0.9957\n",
      "Val Loss: 0.5225, Val Acc: 0.8963\n",
      "Epoch 25/30\n",
      "Train Loss: 0.0118, Train Acc: 0.9971\n",
      "Val Loss: 0.4954, Val Acc: 0.8905\n",
      "Epoch 26/30\n",
      "Train Loss: 0.0043, Train Acc: 0.9993\n",
      "Val Loss: 0.5098, Val Acc: 0.8876\n",
      "Epoch 27/30\n",
      "Train Loss: 0.0039, Train Acc: 0.9986\n",
      "Val Loss: 0.5017, Val Acc: 0.8905\n",
      "Epoch 28/30\n",
      "Train Loss: 0.0063, Train Acc: 0.9986\n",
      "Val Loss: 0.5013, Val Acc: 0.8905\n",
      "Epoch 29/30\n",
      "Train Loss: 0.0047, Train Acc: 0.9986\n",
      "Val Loss: 0.4822, Val Acc: 0.8876\n",
      "Epoch 30/30\n",
      "Train Loss: 0.0065, Train Acc: 0.9978\n",
      "Val Loss: 0.4790, Val Acc: 0.8934\n",
      "Fold 4/5\n",
      "Epoch 1/30\n",
      "Train Loss: 1.4115, Train Acc: 0.5869\n",
      "Val Loss: 0.4589, Val Acc: 0.8357\n",
      "Epoch 2/30\n",
      "Train Loss: 0.3654, Train Acc: 0.8818\n",
      "Val Loss: 0.3338, Val Acc: 0.8646\n",
      "Epoch 3/30\n",
      "Train Loss: 0.2079, Train Acc: 0.9272\n",
      "Val Loss: 0.2732, Val Acc: 0.8876\n",
      "Epoch 4/30\n",
      "Train Loss: 0.1520, Train Acc: 0.9531\n",
      "Val Loss: 0.2737, Val Acc: 0.9078\n",
      "Epoch 5/30\n",
      "Train Loss: 0.1322, Train Acc: 0.9510\n",
      "Val Loss: 0.2827, Val Acc: 0.9020\n",
      "Epoch 6/30\n",
      "Train Loss: 0.1030, Train Acc: 0.9632\n",
      "Val Loss: 0.2229, Val Acc: 0.9164\n",
      "Epoch 7/30\n",
      "Train Loss: 0.0677, Train Acc: 0.9726\n",
      "Val Loss: 0.2521, Val Acc: 0.9193\n",
      "Epoch 8/30\n",
      "Train Loss: 0.0588, Train Acc: 0.9813\n",
      "Val Loss: 0.2572, Val Acc: 0.9020\n",
      "Epoch 9/30\n",
      "Train Loss: 0.0835, Train Acc: 0.9776\n",
      "Val Loss: 0.2827, Val Acc: 0.9078\n",
      "Epoch 10/30\n",
      "Train Loss: 0.0433, Train Acc: 0.9863\n",
      "Val Loss: 0.2798, Val Acc: 0.9135\n",
      "Epoch 11/30\n",
      "Train Loss: 0.0505, Train Acc: 0.9849\n",
      "Val Loss: 0.2172, Val Acc: 0.9280\n",
      "Epoch 12/30\n",
      "Train Loss: 0.0316, Train Acc: 0.9877\n",
      "Val Loss: 0.2186, Val Acc: 0.9280\n",
      "Epoch 13/30\n",
      "Train Loss: 0.0399, Train Acc: 0.9870\n",
      "Val Loss: 0.2031, Val Acc: 0.9366\n",
      "Epoch 14/30\n",
      "Train Loss: 0.0307, Train Acc: 0.9913\n",
      "Val Loss: 0.2962, Val Acc: 0.9049\n",
      "Epoch 15/30\n",
      "Train Loss: 0.0241, Train Acc: 0.9906\n",
      "Val Loss: 0.2425, Val Acc: 0.9251\n",
      "Epoch 16/30\n",
      "Train Loss: 0.0164, Train Acc: 0.9964\n",
      "Val Loss: 0.2860, Val Acc: 0.9164\n",
      "Epoch 17/30\n",
      "Train Loss: 0.0078, Train Acc: 0.9978\n",
      "Val Loss: 0.2526, Val Acc: 0.9222\n",
      "Epoch 18/30\n",
      "Train Loss: 0.0225, Train Acc: 0.9928\n",
      "Val Loss: 0.2276, Val Acc: 0.9366\n",
      "Epoch 19/30\n",
      "Train Loss: 0.0260, Train Acc: 0.9899\n",
      "Val Loss: 0.2487, Val Acc: 0.9222\n",
      "Epoch 20/30\n",
      "Train Loss: 0.0139, Train Acc: 0.9957\n",
      "Val Loss: 0.2744, Val Acc: 0.9193\n",
      "Epoch 21/30\n",
      "Train Loss: 0.0045, Train Acc: 0.9993\n",
      "Val Loss: 0.2545, Val Acc: 0.9135\n",
      "Epoch 22/30\n",
      "Train Loss: 0.0033, Train Acc: 1.0000\n",
      "Val Loss: 0.2654, Val Acc: 0.9222\n",
      "Epoch 23/30\n",
      "Train Loss: 0.0078, Train Acc: 0.9978\n",
      "Val Loss: 0.2814, Val Acc: 0.9193\n",
      "Epoch 24/30\n",
      "Train Loss: 0.0049, Train Acc: 0.9986\n",
      "Val Loss: 0.2655, Val Acc: 0.9280\n",
      "Epoch 25/30\n",
      "Train Loss: 0.0034, Train Acc: 1.0000\n",
      "Val Loss: 0.2839, Val Acc: 0.9193\n",
      "Epoch 26/30\n",
      "Train Loss: 0.0094, Train Acc: 0.9978\n",
      "Val Loss: 0.2966, Val Acc: 0.9222\n",
      "Epoch 27/30\n",
      "Train Loss: 0.0040, Train Acc: 0.9986\n",
      "Val Loss: 0.2819, Val Acc: 0.9251\n",
      "Epoch 28/30\n",
      "Train Loss: 0.0047, Train Acc: 0.9986\n",
      "Val Loss: 0.2788, Val Acc: 0.9251\n",
      "Epoch 29/30\n",
      "Train Loss: 0.0044, Train Acc: 0.9993\n",
      "Val Loss: 0.2760, Val Acc: 0.9280\n",
      "Epoch 30/30\n",
      "Train Loss: 0.0042, Train Acc: 1.0000\n",
      "Val Loss: 0.2852, Val Acc: 0.9222\n",
      "Fold 5/5\n",
      "Epoch 1/30\n",
      "Train Loss: 1.3777, Train Acc: 0.5944\n",
      "Val Loss: 0.4040, Val Acc: 0.8728\n",
      "Epoch 2/30\n",
      "Train Loss: 0.3616, Train Acc: 0.8754\n",
      "Val Loss: 0.2836, Val Acc: 0.8988\n",
      "Epoch 3/30\n",
      "Train Loss: 0.2469, Train Acc: 0.9215\n",
      "Val Loss: 0.2635, Val Acc: 0.9046\n",
      "Epoch 4/30\n",
      "Train Loss: 0.1714, Train Acc: 0.9366\n",
      "Val Loss: 0.2419, Val Acc: 0.8988\n",
      "Epoch 5/30\n",
      "Train Loss: 0.1419, Train Acc: 0.9424\n",
      "Val Loss: 0.2180, Val Acc: 0.9191\n",
      "Epoch 6/30\n",
      "Train Loss: 0.0974, Train Acc: 0.9654\n",
      "Val Loss: 0.2586, Val Acc: 0.9191\n",
      "Epoch 7/30\n",
      "Train Loss: 0.0909, Train Acc: 0.9705\n",
      "Val Loss: 0.2662, Val Acc: 0.9191\n",
      "Epoch 8/30\n",
      "Train Loss: 0.0658, Train Acc: 0.9733\n",
      "Val Loss: 0.2484, Val Acc: 0.9104\n",
      "Epoch 9/30\n",
      "Train Loss: 0.0375, Train Acc: 0.9892\n",
      "Val Loss: 0.2608, Val Acc: 0.9191\n",
      "Epoch 10/30\n",
      "Train Loss: 0.0518, Train Acc: 0.9827\n",
      "Val Loss: 0.3387, Val Acc: 0.9133\n",
      "Epoch 11/30\n",
      "Train Loss: 0.0465, Train Acc: 0.9827\n",
      "Val Loss: 0.2759, Val Acc: 0.9249\n",
      "Epoch 12/30\n",
      "Train Loss: 0.0357, Train Acc: 0.9863\n",
      "Val Loss: 0.2917, Val Acc: 0.9191\n",
      "Epoch 13/30\n",
      "Train Loss: 0.0218, Train Acc: 0.9914\n",
      "Val Loss: 0.3311, Val Acc: 0.9075\n",
      "Epoch 14/30\n",
      "Train Loss: 0.0261, Train Acc: 0.9906\n",
      "Val Loss: 0.3271, Val Acc: 0.9162\n",
      "Epoch 15/30\n",
      "Train Loss: 0.0212, Train Acc: 0.9935\n",
      "Val Loss: 0.3362, Val Acc: 0.9133\n",
      "Epoch 16/30\n",
      "Train Loss: 0.0159, Train Acc: 0.9950\n",
      "Val Loss: 0.3311, Val Acc: 0.9249\n",
      "Epoch 17/30\n",
      "Train Loss: 0.0414, Train Acc: 0.9878\n",
      "Val Loss: 0.2932, Val Acc: 0.9191\n",
      "Epoch 18/30\n",
      "Train Loss: 0.0171, Train Acc: 0.9950\n",
      "Val Loss: 0.3064, Val Acc: 0.9191\n",
      "Epoch 19/30\n",
      "Train Loss: 0.0219, Train Acc: 0.9921\n",
      "Val Loss: 0.2776, Val Acc: 0.9277\n",
      "Epoch 20/30\n",
      "Train Loss: 0.0154, Train Acc: 0.9950\n",
      "Val Loss: 0.2874, Val Acc: 0.9277\n",
      "Epoch 21/30\n",
      "Train Loss: 0.0066, Train Acc: 0.9971\n",
      "Val Loss: 0.2979, Val Acc: 0.9277\n",
      "Epoch 22/30\n",
      "Train Loss: 0.0081, Train Acc: 0.9986\n",
      "Val Loss: 0.3085, Val Acc: 0.9393\n",
      "Epoch 23/30\n",
      "Train Loss: 0.0086, Train Acc: 0.9971\n",
      "Val Loss: 0.3343, Val Acc: 0.9306\n",
      "Epoch 24/30\n",
      "Train Loss: 0.0105, Train Acc: 0.9957\n",
      "Val Loss: 0.3245, Val Acc: 0.9249\n",
      "Epoch 25/30\n",
      "Train Loss: 0.0168, Train Acc: 0.9950\n",
      "Val Loss: 0.3264, Val Acc: 0.9249\n",
      "Epoch 26/30\n",
      "Train Loss: 0.0070, Train Acc: 0.9978\n",
      "Val Loss: 0.3204, Val Acc: 0.9306\n",
      "Epoch 27/30\n",
      "Train Loss: 0.0084, Train Acc: 0.9978\n",
      "Val Loss: 0.3227, Val Acc: 0.9249\n",
      "Epoch 28/30\n",
      "Train Loss: 0.0055, Train Acc: 0.9986\n",
      "Val Loss: 0.3258, Val Acc: 0.9277\n",
      "Epoch 29/30\n",
      "Train Loss: 0.0051, Train Acc: 0.9986\n",
      "Val Loss: 0.3176, Val Acc: 0.9277\n",
      "Epoch 30/30\n",
      "Train Loss: 0.0062, Train Acc: 0.9978\n",
      "Val Loss: 0.3225, Val Acc: 0.9277\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 하이퍼파라미터\n",
    "LR = 5e-4\n",
    "model_name = 'efficientnet_b4'\n",
    "img_size = 380\n",
    "\n",
    "# K-Fold 교차 검증 수행\n",
    "for fold in range(5):\n",
    "    print(f\"Fold {fold+1}/{5}\")\n",
    "    \n",
    "    train_loader, val_loader = get_data_loaders(df, data_path, fold, kfold, BATCH_SIZE, NUM_WORKERS)\n",
    "    \n",
    "    # 모델, 손실 함수, 최적화기 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    # 학습 루프\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # 모델 저장 (필요한 경우)\n",
    "    torch.save(model.state_dict(), f\"model_fold_{fold+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:08<00:00, 11.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to pred_efficientNetB4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '/data/ephemeral/home/data/'\n",
    "    model_name = 'efficientnet_b4'\n",
    "    img_size = 380  # EfficientNet-B4에 적합한 이미지 크기\n",
    "    BATCH_SIZE = 32  # 배치 크기\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),  # 이미지 크기 조정\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 이미지 정규화\n",
    "        ToTensorV2(),  # 이미지 텐서로 변환\n",
    "    ])\n",
    "\n",
    "    # 테스트 데이터 로드\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test\"), transform=val_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=False, num_classes=17).to(device)\n",
    "    model.load_state_dict(torch.load('/data/ephemeral/home/code/BeomCheol_code/best_model.pth'))  # 업로드된 모델 파일 로드\n",
    "    model.eval()\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    preds_list = []\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred_efficientNetB4_kFold.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred_efficientNetB4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

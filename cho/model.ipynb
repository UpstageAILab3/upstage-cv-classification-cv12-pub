{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 기반 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple EfficientNet-B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '../data/'\n",
    "model_name = 'efficientnet_b0'\n",
    "img_size = 384\n",
    "LR = 1e-3\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 4\n",
    "\n",
    "# 데이터 증강 설정\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 분할\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "train_dataset = ImageDataset(train_df, \"../data/train_preprocessed/\", transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, \"../data/train_preprocessed/\", transform=val_transform)\n",
    "test_dataset = ImageDataset(\"../data/sample_submission.csv\", \"../data/test_preprocessed/\", transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "    \n",
    "# 모델 구조 출력\n",
    "print(f\"\\nModel structure of {model_name}:\")\n",
    "print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "# 모델 아키텍처 출력\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "best_val_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 추론\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet-B4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((380, 380, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'efficientnet_b4'\n",
    "    img_size = 380  # EfficientNet-B4에 적합한 이미지 크기\n",
    "    LR = 5e-4  # 학습률 조정\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 16  # 배치 크기 감소\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNext V2 Large 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'convnextv2_large'\n",
    "    img_size = 224  # ConvNeXt V2 Large에 적합한 이미지 크기\n",
    "    LR = 1e-4  # 학습률 조정\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32  # 배치 크기 조정\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"convNext_model.pth\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"convNext_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convNext v2 + fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model structure of convnextv2_large:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 192, 56, 56]           9,408\n",
      "       LayerNorm2d-2          [-1, 192, 56, 56]             384\n",
      "          Identity-3          [-1, 192, 56, 56]               0\n",
      "            Conv2d-4          [-1, 192, 56, 56]           9,600\n",
      "         LayerNorm-5          [-1, 56, 56, 192]             384\n",
      "            Linear-6          [-1, 56, 56, 768]         148,224\n",
      "              GELU-7          [-1, 56, 56, 768]               0\n",
      "           Dropout-8          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-9          [-1, 56, 56, 768]           1,536\n",
      "           Linear-10          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-11          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-12          [-1, 56, 56, 192]               0\n",
      "         Identity-13          [-1, 192, 56, 56]               0\n",
      "         Identity-14          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-15          [-1, 192, 56, 56]               0\n",
      "           Conv2d-16          [-1, 192, 56, 56]           9,600\n",
      "        LayerNorm-17          [-1, 56, 56, 192]             384\n",
      "           Linear-18          [-1, 56, 56, 768]         148,224\n",
      "             GELU-19          [-1, 56, 56, 768]               0\n",
      "          Dropout-20          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-21          [-1, 56, 56, 768]           1,536\n",
      "           Linear-22          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-23          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-24          [-1, 56, 56, 192]               0\n",
      "         Identity-25          [-1, 192, 56, 56]               0\n",
      "         Identity-26          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-27          [-1, 192, 56, 56]               0\n",
      "           Conv2d-28          [-1, 192, 56, 56]           9,600\n",
      "        LayerNorm-29          [-1, 56, 56, 192]             384\n",
      "           Linear-30          [-1, 56, 56, 768]         148,224\n",
      "             GELU-31          [-1, 56, 56, 768]               0\n",
      "          Dropout-32          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-33          [-1, 56, 56, 768]           1,536\n",
      "           Linear-34          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-35          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-36          [-1, 56, 56, 192]               0\n",
      "         Identity-37          [-1, 192, 56, 56]               0\n",
      "         Identity-38          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-39          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtStage-40          [-1, 192, 56, 56]               0\n",
      "      LayerNorm2d-41          [-1, 192, 56, 56]             384\n",
      "           Conv2d-42          [-1, 384, 28, 28]         295,296\n",
      "           Conv2d-43          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-44          [-1, 28, 28, 384]             768\n",
      "           Linear-45         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-46         [-1, 28, 28, 1536]               0\n",
      "          Dropout-47         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-48         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-49          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-50          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-51          [-1, 28, 28, 384]               0\n",
      "         Identity-52          [-1, 384, 28, 28]               0\n",
      "         Identity-53          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-54          [-1, 384, 28, 28]               0\n",
      "           Conv2d-55          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-56          [-1, 28, 28, 384]             768\n",
      "           Linear-57         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-58         [-1, 28, 28, 1536]               0\n",
      "          Dropout-59         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-60         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-61          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-62          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-63          [-1, 28, 28, 384]               0\n",
      "         Identity-64          [-1, 384, 28, 28]               0\n",
      "         Identity-65          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-66          [-1, 384, 28, 28]               0\n",
      "           Conv2d-67          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-68          [-1, 28, 28, 384]             768\n",
      "           Linear-69         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-70         [-1, 28, 28, 1536]               0\n",
      "          Dropout-71         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-72         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-73          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-74          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-75          [-1, 28, 28, 384]               0\n",
      "         Identity-76          [-1, 384, 28, 28]               0\n",
      "         Identity-77          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-78          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtStage-79          [-1, 384, 28, 28]               0\n",
      "      LayerNorm2d-80          [-1, 384, 28, 28]             768\n",
      "           Conv2d-81          [-1, 768, 14, 14]       1,180,416\n",
      "           Conv2d-82          [-1, 768, 14, 14]          38,400\n",
      "        LayerNorm-83          [-1, 14, 14, 768]           1,536\n",
      "           Linear-84         [-1, 14, 14, 3072]       2,362,368\n",
      "             GELU-85         [-1, 14, 14, 3072]               0\n",
      "          Dropout-86         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-87         [-1, 14, 14, 3072]           6,144\n",
      "           Linear-88          [-1, 14, 14, 768]       2,360,064\n",
      "          Dropout-89          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-90          [-1, 14, 14, 768]               0\n",
      "         Identity-91          [-1, 768, 14, 14]               0\n",
      "         Identity-92          [-1, 768, 14, 14]               0\n",
      "    ConvNeXtBlock-93          [-1, 768, 14, 14]               0\n",
      "           Conv2d-94          [-1, 768, 14, 14]          38,400\n",
      "        LayerNorm-95          [-1, 14, 14, 768]           1,536\n",
      "           Linear-96         [-1, 14, 14, 3072]       2,362,368\n",
      "             GELU-97         [-1, 14, 14, 3072]               0\n",
      "          Dropout-98         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-99         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-100          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-101          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-102          [-1, 14, 14, 768]               0\n",
      "        Identity-103          [-1, 768, 14, 14]               0\n",
      "        Identity-104          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-105          [-1, 768, 14, 14]               0\n",
      "          Conv2d-106          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-107          [-1, 14, 14, 768]           1,536\n",
      "          Linear-108         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-109         [-1, 14, 14, 3072]               0\n",
      "         Dropout-110         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-111         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-112          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-113          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-114          [-1, 14, 14, 768]               0\n",
      "        Identity-115          [-1, 768, 14, 14]               0\n",
      "        Identity-116          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-117          [-1, 768, 14, 14]               0\n",
      "          Conv2d-118          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-119          [-1, 14, 14, 768]           1,536\n",
      "          Linear-120         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-121         [-1, 14, 14, 3072]               0\n",
      "         Dropout-122         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-123         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-124          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-125          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-126          [-1, 14, 14, 768]               0\n",
      "        Identity-127          [-1, 768, 14, 14]               0\n",
      "        Identity-128          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-129          [-1, 768, 14, 14]               0\n",
      "          Conv2d-130          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-131          [-1, 14, 14, 768]           1,536\n",
      "          Linear-132         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-133         [-1, 14, 14, 3072]               0\n",
      "         Dropout-134         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-135         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-136          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-137          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-138          [-1, 14, 14, 768]               0\n",
      "        Identity-139          [-1, 768, 14, 14]               0\n",
      "        Identity-140          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-141          [-1, 768, 14, 14]               0\n",
      "          Conv2d-142          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-143          [-1, 14, 14, 768]           1,536\n",
      "          Linear-144         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-145         [-1, 14, 14, 3072]               0\n",
      "         Dropout-146         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-147         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-148          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-149          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-150          [-1, 14, 14, 768]               0\n",
      "        Identity-151          [-1, 768, 14, 14]               0\n",
      "        Identity-152          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-153          [-1, 768, 14, 14]               0\n",
      "          Conv2d-154          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-155          [-1, 14, 14, 768]           1,536\n",
      "          Linear-156         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-157         [-1, 14, 14, 3072]               0\n",
      "         Dropout-158         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-159         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-160          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-161          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-162          [-1, 14, 14, 768]               0\n",
      "        Identity-163          [-1, 768, 14, 14]               0\n",
      "        Identity-164          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-165          [-1, 768, 14, 14]               0\n",
      "          Conv2d-166          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-167          [-1, 14, 14, 768]           1,536\n",
      "          Linear-168         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-169         [-1, 14, 14, 3072]               0\n",
      "         Dropout-170         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-171         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-172          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-173          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-174          [-1, 14, 14, 768]               0\n",
      "        Identity-175          [-1, 768, 14, 14]               0\n",
      "        Identity-176          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-177          [-1, 768, 14, 14]               0\n",
      "          Conv2d-178          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-179          [-1, 14, 14, 768]           1,536\n",
      "          Linear-180         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-181         [-1, 14, 14, 3072]               0\n",
      "         Dropout-182         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-183         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-184          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-185          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-186          [-1, 14, 14, 768]               0\n",
      "        Identity-187          [-1, 768, 14, 14]               0\n",
      "        Identity-188          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-189          [-1, 768, 14, 14]               0\n",
      "          Conv2d-190          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-191          [-1, 14, 14, 768]           1,536\n",
      "          Linear-192         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-193         [-1, 14, 14, 3072]               0\n",
      "         Dropout-194         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-195         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-196          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-197          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-198          [-1, 14, 14, 768]               0\n",
      "        Identity-199          [-1, 768, 14, 14]               0\n",
      "        Identity-200          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-201          [-1, 768, 14, 14]               0\n",
      "          Conv2d-202          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-203          [-1, 14, 14, 768]           1,536\n",
      "          Linear-204         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-205         [-1, 14, 14, 3072]               0\n",
      "         Dropout-206         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-207         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-208          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-209          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-210          [-1, 14, 14, 768]               0\n",
      "        Identity-211          [-1, 768, 14, 14]               0\n",
      "        Identity-212          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-213          [-1, 768, 14, 14]               0\n",
      "          Conv2d-214          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-215          [-1, 14, 14, 768]           1,536\n",
      "          Linear-216         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-217         [-1, 14, 14, 3072]               0\n",
      "         Dropout-218         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-219         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-220          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-221          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-222          [-1, 14, 14, 768]               0\n",
      "        Identity-223          [-1, 768, 14, 14]               0\n",
      "        Identity-224          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-225          [-1, 768, 14, 14]               0\n",
      "          Conv2d-226          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-227          [-1, 14, 14, 768]           1,536\n",
      "          Linear-228         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-229         [-1, 14, 14, 3072]               0\n",
      "         Dropout-230         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-231         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-232          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-233          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-234          [-1, 14, 14, 768]               0\n",
      "        Identity-235          [-1, 768, 14, 14]               0\n",
      "        Identity-236          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-237          [-1, 768, 14, 14]               0\n",
      "          Conv2d-238          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-239          [-1, 14, 14, 768]           1,536\n",
      "          Linear-240         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-241         [-1, 14, 14, 3072]               0\n",
      "         Dropout-242         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-243         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-244          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-245          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-246          [-1, 14, 14, 768]               0\n",
      "        Identity-247          [-1, 768, 14, 14]               0\n",
      "        Identity-248          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-249          [-1, 768, 14, 14]               0\n",
      "          Conv2d-250          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-251          [-1, 14, 14, 768]           1,536\n",
      "          Linear-252         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-253         [-1, 14, 14, 3072]               0\n",
      "         Dropout-254         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-255         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-256          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-257          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-258          [-1, 14, 14, 768]               0\n",
      "        Identity-259          [-1, 768, 14, 14]               0\n",
      "        Identity-260          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-261          [-1, 768, 14, 14]               0\n",
      "          Conv2d-262          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-263          [-1, 14, 14, 768]           1,536\n",
      "          Linear-264         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-265         [-1, 14, 14, 3072]               0\n",
      "         Dropout-266         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-267         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-268          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-269          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-270          [-1, 14, 14, 768]               0\n",
      "        Identity-271          [-1, 768, 14, 14]               0\n",
      "        Identity-272          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-273          [-1, 768, 14, 14]               0\n",
      "          Conv2d-274          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-275          [-1, 14, 14, 768]           1,536\n",
      "          Linear-276         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-277         [-1, 14, 14, 3072]               0\n",
      "         Dropout-278         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-279         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-280          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-281          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-282          [-1, 14, 14, 768]               0\n",
      "        Identity-283          [-1, 768, 14, 14]               0\n",
      "        Identity-284          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-285          [-1, 768, 14, 14]               0\n",
      "          Conv2d-286          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-287          [-1, 14, 14, 768]           1,536\n",
      "          Linear-288         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-289         [-1, 14, 14, 3072]               0\n",
      "         Dropout-290         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-291         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-292          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-293          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-294          [-1, 14, 14, 768]               0\n",
      "        Identity-295          [-1, 768, 14, 14]               0\n",
      "        Identity-296          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-297          [-1, 768, 14, 14]               0\n",
      "          Conv2d-298          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-299          [-1, 14, 14, 768]           1,536\n",
      "          Linear-300         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-301         [-1, 14, 14, 3072]               0\n",
      "         Dropout-302         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-303         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-304          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-305          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-306          [-1, 14, 14, 768]               0\n",
      "        Identity-307          [-1, 768, 14, 14]               0\n",
      "        Identity-308          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-309          [-1, 768, 14, 14]               0\n",
      "          Conv2d-310          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-311          [-1, 14, 14, 768]           1,536\n",
      "          Linear-312         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-313         [-1, 14, 14, 3072]               0\n",
      "         Dropout-314         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-315         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-316          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-317          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-318          [-1, 14, 14, 768]               0\n",
      "        Identity-319          [-1, 768, 14, 14]               0\n",
      "        Identity-320          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-321          [-1, 768, 14, 14]               0\n",
      "          Conv2d-322          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-323          [-1, 14, 14, 768]           1,536\n",
      "          Linear-324         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-325         [-1, 14, 14, 3072]               0\n",
      "         Dropout-326         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-327         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-328          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-329          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-330          [-1, 14, 14, 768]               0\n",
      "        Identity-331          [-1, 768, 14, 14]               0\n",
      "        Identity-332          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-333          [-1, 768, 14, 14]               0\n",
      "          Conv2d-334          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-335          [-1, 14, 14, 768]           1,536\n",
      "          Linear-336         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-337         [-1, 14, 14, 3072]               0\n",
      "         Dropout-338         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-339         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-340          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-341          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-342          [-1, 14, 14, 768]               0\n",
      "        Identity-343          [-1, 768, 14, 14]               0\n",
      "        Identity-344          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-345          [-1, 768, 14, 14]               0\n",
      "          Conv2d-346          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-347          [-1, 14, 14, 768]           1,536\n",
      "          Linear-348         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-349         [-1, 14, 14, 3072]               0\n",
      "         Dropout-350         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-351         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-352          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-353          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-354          [-1, 14, 14, 768]               0\n",
      "        Identity-355          [-1, 768, 14, 14]               0\n",
      "        Identity-356          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-357          [-1, 768, 14, 14]               0\n",
      "          Conv2d-358          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-359          [-1, 14, 14, 768]           1,536\n",
      "          Linear-360         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-361         [-1, 14, 14, 3072]               0\n",
      "         Dropout-362         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-363         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-364          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-365          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-366          [-1, 14, 14, 768]               0\n",
      "        Identity-367          [-1, 768, 14, 14]               0\n",
      "        Identity-368          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-369          [-1, 768, 14, 14]               0\n",
      "          Conv2d-370          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-371          [-1, 14, 14, 768]           1,536\n",
      "          Linear-372         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-373         [-1, 14, 14, 3072]               0\n",
      "         Dropout-374         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-375         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-376          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-377          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-378          [-1, 14, 14, 768]               0\n",
      "        Identity-379          [-1, 768, 14, 14]               0\n",
      "        Identity-380          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-381          [-1, 768, 14, 14]               0\n",
      "          Conv2d-382          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-383          [-1, 14, 14, 768]           1,536\n",
      "          Linear-384         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-385         [-1, 14, 14, 3072]               0\n",
      "         Dropout-386         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-387         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-388          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-389          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-390          [-1, 14, 14, 768]               0\n",
      "        Identity-391          [-1, 768, 14, 14]               0\n",
      "        Identity-392          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-393          [-1, 768, 14, 14]               0\n",
      "          Conv2d-394          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-395          [-1, 14, 14, 768]           1,536\n",
      "          Linear-396         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-397         [-1, 14, 14, 3072]               0\n",
      "         Dropout-398         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-399         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-400          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-401          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-402          [-1, 14, 14, 768]               0\n",
      "        Identity-403          [-1, 768, 14, 14]               0\n",
      "        Identity-404          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-405          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtStage-406          [-1, 768, 14, 14]               0\n",
      "     LayerNorm2d-407          [-1, 768, 14, 14]           1,536\n",
      "          Conv2d-408           [-1, 1536, 7, 7]       4,720,128\n",
      "          Conv2d-409           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-410           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-411           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-412           [-1, 7, 7, 6144]               0\n",
      "         Dropout-413           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-414           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-415           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-416           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-417           [-1, 7, 7, 1536]               0\n",
      "        Identity-418           [-1, 1536, 7, 7]               0\n",
      "        Identity-419           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-420           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-421           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-422           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-423           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-424           [-1, 7, 7, 6144]               0\n",
      "         Dropout-425           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-426           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-427           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-428           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-429           [-1, 7, 7, 1536]               0\n",
      "        Identity-430           [-1, 1536, 7, 7]               0\n",
      "        Identity-431           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-432           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-433           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-434           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-435           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-436           [-1, 7, 7, 6144]               0\n",
      "         Dropout-437           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-438           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-439           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-440           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-441           [-1, 7, 7, 1536]               0\n",
      "        Identity-442           [-1, 1536, 7, 7]               0\n",
      "        Identity-443           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-444           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtStage-445           [-1, 1536, 7, 7]               0\n",
      "        Identity-446           [-1, 1536, 7, 7]               0\n",
      "AdaptiveAvgPool2d-447           [-1, 1536, 1, 1]               0\n",
      "        Identity-448           [-1, 1536, 1, 1]               0\n",
      "SelectAdaptivePool2d-449           [-1, 1536, 1, 1]               0\n",
      "     LayerNorm2d-450           [-1, 1536, 1, 1]           3,072\n",
      "         Flatten-451                 [-1, 1536]               0\n",
      "        Identity-452                 [-1, 1536]               0\n",
      "         Dropout-453                 [-1, 1536]               0\n",
      "          Linear-454                   [-1, 17]          26,129\n",
      "NormMlpClassifierHead-455                   [-1, 17]               0\n",
      "================================================================\n",
      "Total params: 196,445,969\n",
      "Trainable params: 196,445,969\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 1316.77\n",
      "Params size (MB): 749.38\n",
      "Estimated Total Size (MB): 2066.72\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2666: 100%|██████████| 40/40 [00:29<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.9142, Train Acc: 0.7142, Train F1: 0.6951\n",
      "Val Loss: 0.3429, Val Acc: 0.8758, Val F1: 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0162: 100%|██████████| 40/40 [00:29<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.2789, Train Acc: 0.8941, Train F1: 0.8816\n",
      "Val Loss: 0.3312, Val Acc: 0.8662, Val F1: 0.8275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0052: 100%|██████████| 40/40 [00:29<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.2074, Train Acc: 0.9188, Train F1: 0.9083\n",
      "Val Loss: 0.2308, Val Acc: 0.9204, Val F1: 0.9089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5555: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1710, Train Acc: 0.9347, Train F1: 0.9280\n",
      "Val Loss: 0.2320, Val Acc: 0.9140, Val F1: 0.9007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2525: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.1384, Train Acc: 0.9443, Train F1: 0.9392\n",
      "Val Loss: 0.1470, Val Acc: 0.9459, Val F1: 0.9418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0816: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0985, Train Acc: 0.9642, Train F1: 0.9626\n",
      "Val Loss: 0.2139, Val Acc: 0.9268, Val F1: 0.9174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0811: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0926, Train Acc: 0.9697, Train F1: 0.9683\n",
      "Val Loss: 0.2329, Val Acc: 0.8981, Val F1: 0.8919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2371: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0625, Train Acc: 0.9809, Train F1: 0.9812\n",
      "Val Loss: 0.1600, Val Acc: 0.9459, Val F1: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0015: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0628, Train Acc: 0.9793, Train F1: 0.9774\n",
      "Val Loss: 0.2908, Val Acc: 0.9045, Val F1: 0.8956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0145: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0458, Train Acc: 0.9785, Train F1: 0.9777\n",
      "Val Loss: 0.2150, Val Acc: 0.9395, Val F1: 0.9314\n",
      "Early stopping triggered\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.2694: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 1/5\n",
      "Train Loss: 2.2694, Train Acc: 0.2632, Train F1: 0.1238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.2706: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 2/5\n",
      "Train Loss: 2.2706, Train Acc: 0.4211, Train F1: 0.3792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8660: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 3/5\n",
      "Train Loss: 0.8660, Train Acc: 0.7368, Train F1: 0.5988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3658: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 4/5\n",
      "Train Loss: 0.3658, Train Acc: 0.8947, Train F1: 0.8084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3274: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 5/5\n",
      "Train Loss: 0.3274, Train Acc: 0.8947, Train F1: 0.8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3086: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 6/5\n",
      "Train Loss: 0.3086, Train Acc: 0.8947, Train F1: 0.8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1315: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 7/5\n",
      "Train Loss: 0.1315, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2255: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 8/5\n",
      "Train Loss: 0.2255, Train Acc: 0.9474, Train F1: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1177: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 9/5\n",
      "Train Loss: 0.1177, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1283: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 10/5\n",
      "Train Loss: 0.1283, Train Acc: 0.9474, Train F1: 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160351/3141216663.py:227: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"convNext_model_final.pth\"))\n",
      "100%|██████████| 99/99 [00:18<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    misclassified = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (image, targets) in enumerate(loader):\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_np = preds.argmax(dim=1).detach().cpu().numpy()\n",
    "            targets_np = targets.detach().cpu().numpy()\n",
    "            preds_list.extend(preds_np)\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            # 오분류된 데이터의 인덱스 저장\n",
    "            misclassified.extend(np.where(preds_np != targets_np)[0] + i * loader.batch_size)\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1, misclassified\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# EarlyStopping 클래스 정의\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'convnextv2_large'\n",
    "    img_size = 224\n",
    "    LR = 1e-4\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # EarlyStopping 초기화\n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1, misclassified = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"convNext_model.pth\")\n",
    "\n",
    "        # Early Stopping 체크\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # 오분류된 데이터로 fine-tuning\n",
    "    misclassified_df = val_df.iloc[misclassified]\n",
    "    misclassified_dataset = ImageDataset(misclassified_df, os.path.join(data_path, \"train/\"), transform=train_transform)\n",
    "    misclassified_loader = DataLoader(misclassified_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    print(\"\\nFine-tuning with misclassified data\")\n",
    "    for epoch in range(10):  # 10 에폭 동안 fine-tuning\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(misclassified_loader, model, optimizer, loss_fn, device)\n",
    "        print(f\"Fine-tuning Epoch {epoch+1}/5\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "    # 최종 모델 저장\n",
    "    torch.save(model.state_dict(), \"convNext_model_final.pth\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"convNext_model_final.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"conv_fine_pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convNext v2 + new augmentaion + K-Fold finetunning + ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/blur/transforms.py:184: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "\n",
      "Model structure of convnextv2_large:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 192, 56, 56]           9,408\n",
      "       LayerNorm2d-2          [-1, 192, 56, 56]             384\n",
      "          Identity-3          [-1, 192, 56, 56]               0\n",
      "            Conv2d-4          [-1, 192, 56, 56]           9,600\n",
      "         LayerNorm-5          [-1, 56, 56, 192]             384\n",
      "            Linear-6          [-1, 56, 56, 768]         148,224\n",
      "              GELU-7          [-1, 56, 56, 768]               0\n",
      "           Dropout-8          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-9          [-1, 56, 56, 768]           1,536\n",
      "           Linear-10          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-11          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-12          [-1, 56, 56, 192]               0\n",
      "         Identity-13          [-1, 192, 56, 56]               0\n",
      "         Identity-14          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-15          [-1, 192, 56, 56]               0\n",
      "           Conv2d-16          [-1, 192, 56, 56]           9,600\n",
      "        LayerNorm-17          [-1, 56, 56, 192]             384\n",
      "           Linear-18          [-1, 56, 56, 768]         148,224\n",
      "             GELU-19          [-1, 56, 56, 768]               0\n",
      "          Dropout-20          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-21          [-1, 56, 56, 768]           1,536\n",
      "           Linear-22          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-23          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-24          [-1, 56, 56, 192]               0\n",
      "         Identity-25          [-1, 192, 56, 56]               0\n",
      "         Identity-26          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-27          [-1, 192, 56, 56]               0\n",
      "           Conv2d-28          [-1, 192, 56, 56]           9,600\n",
      "        LayerNorm-29          [-1, 56, 56, 192]             384\n",
      "           Linear-30          [-1, 56, 56, 768]         148,224\n",
      "             GELU-31          [-1, 56, 56, 768]               0\n",
      "          Dropout-32          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-33          [-1, 56, 56, 768]           1,536\n",
      "           Linear-34          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-35          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-36          [-1, 56, 56, 192]               0\n",
      "         Identity-37          [-1, 192, 56, 56]               0\n",
      "         Identity-38          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-39          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtStage-40          [-1, 192, 56, 56]               0\n",
      "      LayerNorm2d-41          [-1, 192, 56, 56]             384\n",
      "           Conv2d-42          [-1, 384, 28, 28]         295,296\n",
      "           Conv2d-43          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-44          [-1, 28, 28, 384]             768\n",
      "           Linear-45         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-46         [-1, 28, 28, 1536]               0\n",
      "          Dropout-47         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-48         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-49          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-50          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-51          [-1, 28, 28, 384]               0\n",
      "         Identity-52          [-1, 384, 28, 28]               0\n",
      "         Identity-53          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-54          [-1, 384, 28, 28]               0\n",
      "           Conv2d-55          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-56          [-1, 28, 28, 384]             768\n",
      "           Linear-57         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-58         [-1, 28, 28, 1536]               0\n",
      "          Dropout-59         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-60         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-61          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-62          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-63          [-1, 28, 28, 384]               0\n",
      "         Identity-64          [-1, 384, 28, 28]               0\n",
      "         Identity-65          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-66          [-1, 384, 28, 28]               0\n",
      "           Conv2d-67          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-68          [-1, 28, 28, 384]             768\n",
      "           Linear-69         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-70         [-1, 28, 28, 1536]               0\n",
      "          Dropout-71         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-72         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-73          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-74          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-75          [-1, 28, 28, 384]               0\n",
      "         Identity-76          [-1, 384, 28, 28]               0\n",
      "         Identity-77          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-78          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtStage-79          [-1, 384, 28, 28]               0\n",
      "      LayerNorm2d-80          [-1, 384, 28, 28]             768\n",
      "           Conv2d-81          [-1, 768, 14, 14]       1,180,416\n",
      "           Conv2d-82          [-1, 768, 14, 14]          38,400\n",
      "        LayerNorm-83          [-1, 14, 14, 768]           1,536\n",
      "           Linear-84         [-1, 14, 14, 3072]       2,362,368\n",
      "             GELU-85         [-1, 14, 14, 3072]               0\n",
      "          Dropout-86         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-87         [-1, 14, 14, 3072]           6,144\n",
      "           Linear-88          [-1, 14, 14, 768]       2,360,064\n",
      "          Dropout-89          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-90          [-1, 14, 14, 768]               0\n",
      "         Identity-91          [-1, 768, 14, 14]               0\n",
      "         Identity-92          [-1, 768, 14, 14]               0\n",
      "    ConvNeXtBlock-93          [-1, 768, 14, 14]               0\n",
      "           Conv2d-94          [-1, 768, 14, 14]          38,400\n",
      "        LayerNorm-95          [-1, 14, 14, 768]           1,536\n",
      "           Linear-96         [-1, 14, 14, 3072]       2,362,368\n",
      "             GELU-97         [-1, 14, 14, 3072]               0\n",
      "          Dropout-98         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-99         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-100          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-101          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-102          [-1, 14, 14, 768]               0\n",
      "        Identity-103          [-1, 768, 14, 14]               0\n",
      "        Identity-104          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-105          [-1, 768, 14, 14]               0\n",
      "          Conv2d-106          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-107          [-1, 14, 14, 768]           1,536\n",
      "          Linear-108         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-109         [-1, 14, 14, 3072]               0\n",
      "         Dropout-110         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-111         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-112          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-113          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-114          [-1, 14, 14, 768]               0\n",
      "        Identity-115          [-1, 768, 14, 14]               0\n",
      "        Identity-116          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-117          [-1, 768, 14, 14]               0\n",
      "          Conv2d-118          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-119          [-1, 14, 14, 768]           1,536\n",
      "          Linear-120         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-121         [-1, 14, 14, 3072]               0\n",
      "         Dropout-122         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-123         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-124          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-125          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-126          [-1, 14, 14, 768]               0\n",
      "        Identity-127          [-1, 768, 14, 14]               0\n",
      "        Identity-128          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-129          [-1, 768, 14, 14]               0\n",
      "          Conv2d-130          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-131          [-1, 14, 14, 768]           1,536\n",
      "          Linear-132         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-133         [-1, 14, 14, 3072]               0\n",
      "         Dropout-134         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-135         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-136          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-137          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-138          [-1, 14, 14, 768]               0\n",
      "        Identity-139          [-1, 768, 14, 14]               0\n",
      "        Identity-140          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-141          [-1, 768, 14, 14]               0\n",
      "          Conv2d-142          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-143          [-1, 14, 14, 768]           1,536\n",
      "          Linear-144         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-145         [-1, 14, 14, 3072]               0\n",
      "         Dropout-146         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-147         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-148          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-149          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-150          [-1, 14, 14, 768]               0\n",
      "        Identity-151          [-1, 768, 14, 14]               0\n",
      "        Identity-152          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-153          [-1, 768, 14, 14]               0\n",
      "          Conv2d-154          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-155          [-1, 14, 14, 768]           1,536\n",
      "          Linear-156         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-157         [-1, 14, 14, 3072]               0\n",
      "         Dropout-158         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-159         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-160          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-161          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-162          [-1, 14, 14, 768]               0\n",
      "        Identity-163          [-1, 768, 14, 14]               0\n",
      "        Identity-164          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-165          [-1, 768, 14, 14]               0\n",
      "          Conv2d-166          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-167          [-1, 14, 14, 768]           1,536\n",
      "          Linear-168         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-169         [-1, 14, 14, 3072]               0\n",
      "         Dropout-170         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-171         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-172          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-173          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-174          [-1, 14, 14, 768]               0\n",
      "        Identity-175          [-1, 768, 14, 14]               0\n",
      "        Identity-176          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-177          [-1, 768, 14, 14]               0\n",
      "          Conv2d-178          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-179          [-1, 14, 14, 768]           1,536\n",
      "          Linear-180         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-181         [-1, 14, 14, 3072]               0\n",
      "         Dropout-182         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-183         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-184          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-185          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-186          [-1, 14, 14, 768]               0\n",
      "        Identity-187          [-1, 768, 14, 14]               0\n",
      "        Identity-188          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-189          [-1, 768, 14, 14]               0\n",
      "          Conv2d-190          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-191          [-1, 14, 14, 768]           1,536\n",
      "          Linear-192         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-193         [-1, 14, 14, 3072]               0\n",
      "         Dropout-194         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-195         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-196          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-197          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-198          [-1, 14, 14, 768]               0\n",
      "        Identity-199          [-1, 768, 14, 14]               0\n",
      "        Identity-200          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-201          [-1, 768, 14, 14]               0\n",
      "          Conv2d-202          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-203          [-1, 14, 14, 768]           1,536\n",
      "          Linear-204         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-205         [-1, 14, 14, 3072]               0\n",
      "         Dropout-206         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-207         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-208          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-209          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-210          [-1, 14, 14, 768]               0\n",
      "        Identity-211          [-1, 768, 14, 14]               0\n",
      "        Identity-212          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-213          [-1, 768, 14, 14]               0\n",
      "          Conv2d-214          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-215          [-1, 14, 14, 768]           1,536\n",
      "          Linear-216         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-217         [-1, 14, 14, 3072]               0\n",
      "         Dropout-218         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-219         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-220          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-221          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-222          [-1, 14, 14, 768]               0\n",
      "        Identity-223          [-1, 768, 14, 14]               0\n",
      "        Identity-224          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-225          [-1, 768, 14, 14]               0\n",
      "          Conv2d-226          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-227          [-1, 14, 14, 768]           1,536\n",
      "          Linear-228         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-229         [-1, 14, 14, 3072]               0\n",
      "         Dropout-230         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-231         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-232          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-233          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-234          [-1, 14, 14, 768]               0\n",
      "        Identity-235          [-1, 768, 14, 14]               0\n",
      "        Identity-236          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-237          [-1, 768, 14, 14]               0\n",
      "          Conv2d-238          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-239          [-1, 14, 14, 768]           1,536\n",
      "          Linear-240         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-241         [-1, 14, 14, 3072]               0\n",
      "         Dropout-242         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-243         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-244          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-245          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-246          [-1, 14, 14, 768]               0\n",
      "        Identity-247          [-1, 768, 14, 14]               0\n",
      "        Identity-248          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-249          [-1, 768, 14, 14]               0\n",
      "          Conv2d-250          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-251          [-1, 14, 14, 768]           1,536\n",
      "          Linear-252         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-253         [-1, 14, 14, 3072]               0\n",
      "         Dropout-254         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-255         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-256          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-257          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-258          [-1, 14, 14, 768]               0\n",
      "        Identity-259          [-1, 768, 14, 14]               0\n",
      "        Identity-260          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-261          [-1, 768, 14, 14]               0\n",
      "          Conv2d-262          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-263          [-1, 14, 14, 768]           1,536\n",
      "          Linear-264         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-265         [-1, 14, 14, 3072]               0\n",
      "         Dropout-266         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-267         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-268          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-269          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-270          [-1, 14, 14, 768]               0\n",
      "        Identity-271          [-1, 768, 14, 14]               0\n",
      "        Identity-272          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-273          [-1, 768, 14, 14]               0\n",
      "          Conv2d-274          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-275          [-1, 14, 14, 768]           1,536\n",
      "          Linear-276         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-277         [-1, 14, 14, 3072]               0\n",
      "         Dropout-278         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-279         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-280          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-281          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-282          [-1, 14, 14, 768]               0\n",
      "        Identity-283          [-1, 768, 14, 14]               0\n",
      "        Identity-284          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-285          [-1, 768, 14, 14]               0\n",
      "          Conv2d-286          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-287          [-1, 14, 14, 768]           1,536\n",
      "          Linear-288         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-289         [-1, 14, 14, 3072]               0\n",
      "         Dropout-290         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-291         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-292          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-293          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-294          [-1, 14, 14, 768]               0\n",
      "        Identity-295          [-1, 768, 14, 14]               0\n",
      "        Identity-296          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-297          [-1, 768, 14, 14]               0\n",
      "          Conv2d-298          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-299          [-1, 14, 14, 768]           1,536\n",
      "          Linear-300         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-301         [-1, 14, 14, 3072]               0\n",
      "         Dropout-302         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-303         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-304          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-305          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-306          [-1, 14, 14, 768]               0\n",
      "        Identity-307          [-1, 768, 14, 14]               0\n",
      "        Identity-308          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-309          [-1, 768, 14, 14]               0\n",
      "          Conv2d-310          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-311          [-1, 14, 14, 768]           1,536\n",
      "          Linear-312         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-313         [-1, 14, 14, 3072]               0\n",
      "         Dropout-314         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-315         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-316          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-317          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-318          [-1, 14, 14, 768]               0\n",
      "        Identity-319          [-1, 768, 14, 14]               0\n",
      "        Identity-320          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-321          [-1, 768, 14, 14]               0\n",
      "          Conv2d-322          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-323          [-1, 14, 14, 768]           1,536\n",
      "          Linear-324         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-325         [-1, 14, 14, 3072]               0\n",
      "         Dropout-326         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-327         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-328          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-329          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-330          [-1, 14, 14, 768]               0\n",
      "        Identity-331          [-1, 768, 14, 14]               0\n",
      "        Identity-332          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-333          [-1, 768, 14, 14]               0\n",
      "          Conv2d-334          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-335          [-1, 14, 14, 768]           1,536\n",
      "          Linear-336         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-337         [-1, 14, 14, 3072]               0\n",
      "         Dropout-338         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-339         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-340          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-341          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-342          [-1, 14, 14, 768]               0\n",
      "        Identity-343          [-1, 768, 14, 14]               0\n",
      "        Identity-344          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-345          [-1, 768, 14, 14]               0\n",
      "          Conv2d-346          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-347          [-1, 14, 14, 768]           1,536\n",
      "          Linear-348         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-349         [-1, 14, 14, 3072]               0\n",
      "         Dropout-350         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-351         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-352          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-353          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-354          [-1, 14, 14, 768]               0\n",
      "        Identity-355          [-1, 768, 14, 14]               0\n",
      "        Identity-356          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-357          [-1, 768, 14, 14]               0\n",
      "          Conv2d-358          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-359          [-1, 14, 14, 768]           1,536\n",
      "          Linear-360         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-361         [-1, 14, 14, 3072]               0\n",
      "         Dropout-362         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-363         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-364          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-365          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-366          [-1, 14, 14, 768]               0\n",
      "        Identity-367          [-1, 768, 14, 14]               0\n",
      "        Identity-368          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-369          [-1, 768, 14, 14]               0\n",
      "          Conv2d-370          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-371          [-1, 14, 14, 768]           1,536\n",
      "          Linear-372         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-373         [-1, 14, 14, 3072]               0\n",
      "         Dropout-374         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-375         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-376          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-377          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-378          [-1, 14, 14, 768]               0\n",
      "        Identity-379          [-1, 768, 14, 14]               0\n",
      "        Identity-380          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-381          [-1, 768, 14, 14]               0\n",
      "          Conv2d-382          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-383          [-1, 14, 14, 768]           1,536\n",
      "          Linear-384         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-385         [-1, 14, 14, 3072]               0\n",
      "         Dropout-386         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-387         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-388          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-389          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-390          [-1, 14, 14, 768]               0\n",
      "        Identity-391          [-1, 768, 14, 14]               0\n",
      "        Identity-392          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-393          [-1, 768, 14, 14]               0\n",
      "          Conv2d-394          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-395          [-1, 14, 14, 768]           1,536\n",
      "          Linear-396         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-397         [-1, 14, 14, 3072]               0\n",
      "         Dropout-398         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-399         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-400          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-401          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-402          [-1, 14, 14, 768]               0\n",
      "        Identity-403          [-1, 768, 14, 14]               0\n",
      "        Identity-404          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-405          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtStage-406          [-1, 768, 14, 14]               0\n",
      "     LayerNorm2d-407          [-1, 768, 14, 14]           1,536\n",
      "          Conv2d-408           [-1, 1536, 7, 7]       4,720,128\n",
      "          Conv2d-409           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-410           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-411           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-412           [-1, 7, 7, 6144]               0\n",
      "         Dropout-413           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-414           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-415           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-416           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-417           [-1, 7, 7, 1536]               0\n",
      "        Identity-418           [-1, 1536, 7, 7]               0\n",
      "        Identity-419           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-420           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-421           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-422           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-423           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-424           [-1, 7, 7, 6144]               0\n",
      "         Dropout-425           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-426           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-427           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-428           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-429           [-1, 7, 7, 1536]               0\n",
      "        Identity-430           [-1, 1536, 7, 7]               0\n",
      "        Identity-431           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-432           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-433           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-434           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-435           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-436           [-1, 7, 7, 6144]               0\n",
      "         Dropout-437           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-438           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-439           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-440           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-441           [-1, 7, 7, 1536]               0\n",
      "        Identity-442           [-1, 1536, 7, 7]               0\n",
      "        Identity-443           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-444           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtStage-445           [-1, 1536, 7, 7]               0\n",
      "        Identity-446           [-1, 1536, 7, 7]               0\n",
      "AdaptiveAvgPool2d-447           [-1, 1536, 1, 1]               0\n",
      "        Identity-448           [-1, 1536, 1, 1]               0\n",
      "SelectAdaptivePool2d-449           [-1, 1536, 1, 1]               0\n",
      "     LayerNorm2d-450           [-1, 1536, 1, 1]           3,072\n",
      "         Flatten-451                 [-1, 1536]               0\n",
      "        Identity-452                 [-1, 1536]               0\n",
      "         Dropout-453                 [-1, 1536]               0\n",
      "          Linear-454                   [-1, 17]          26,129\n",
      "NormMlpClassifierHead-455                   [-1, 17]               0\n",
      "================================================================\n",
      "Total params: 196,445,969\n",
      "Trainable params: 196,445,969\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 1316.77\n",
      "Params size (MB): 749.38\n",
      "Estimated Total Size (MB): 2066.72\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3000: 100%|██████████| 40/40 [00:29<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.8191, Train Acc: 0.7524, Train F1: 0.7394\n",
      "Val Loss: 0.3038, Val Acc: 0.8790, Val F1: 0.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0397: 100%|██████████| 40/40 [00:29<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.2373, Train Acc: 0.9053, Train F1: 0.8968\n",
      "Val Loss: 0.2177, Val Acc: 0.9045, Val F1: 0.8850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0291: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.1529, Train Acc: 0.9395, Train F1: 0.9360\n",
      "Val Loss: 0.1973, Val Acc: 0.9108, Val F1: 0.9005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0148: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.0879, Train Acc: 0.9674, Train F1: 0.9661\n",
      "Val Loss: 0.1966, Val Acc: 0.9331, Val F1: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.0640, Train Acc: 0.9777, Train F1: 0.9784\n",
      "Val Loss: 0.1732, Val Acc: 0.9490, Val F1: 0.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0454, Train Acc: 0.9841, Train F1: 0.9843\n",
      "Val Loss: 0.1889, Val Acc: 0.9522, Val F1: 0.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0022: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0230, Train Acc: 0.9928, Train F1: 0.9934\n",
      "Val Loss: 0.2811, Val Acc: 0.9363, Val F1: 0.9257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0028: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0569, Train Acc: 0.9801, Train F1: 0.9795\n",
      "Val Loss: 0.3390, Val Acc: 0.9045, Val F1: 0.8940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0018: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0415, Train Acc: 0.9833, Train F1: 0.9827\n",
      "Val Loss: 0.2686, Val Acc: 0.9299, Val F1: 0.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0239, Train Acc: 0.9904, Train F1: 0.9909\n",
      "Val Loss: 0.2953, Val Acc: 0.9140, Val F1: 0.9039\n",
      "Early stopping\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0044, Train Acc: 0.9960, Train F1: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.0010, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.0009, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.0003, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.0004, Train Acc: 1.0000, Train F1: 1.0000\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2489: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.9736, Train Acc: 0.6990, Train F1: 0.6796\n",
      "Val Loss: 0.4136, Val Acc: 0.8503, Val F1: 0.8294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0315: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.2272, Train Acc: 0.9156, Train F1: 0.9069\n",
      "Val Loss: 0.1553, Val Acc: 0.9268, Val F1: 0.9180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0192: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.1267, Train Acc: 0.9498, Train F1: 0.9486\n",
      "Val Loss: 0.2288, Val Acc: 0.9140, Val F1: 0.9112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0719: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1010, Train Acc: 0.9602, Train F1: 0.9585\n",
      "Val Loss: 0.1379, Val Acc: 0.9459, Val F1: 0.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0226: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.0870, Train Acc: 0.9650, Train F1: 0.9625\n",
      "Val Loss: 0.1803, Val Acc: 0.9395, Val F1: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1696: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0623, Train Acc: 0.9745, Train F1: 0.9749\n",
      "Val Loss: 0.1711, Val Acc: 0.9268, Val F1: 0.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2599: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0503, Train Acc: 0.9785, Train F1: 0.9782\n",
      "Val Loss: 0.1470, Val Acc: 0.9363, Val F1: 0.9358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0261: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0366, Train Acc: 0.9881, Train F1: 0.9881\n",
      "Val Loss: 0.1707, Val Acc: 0.9459, Val F1: 0.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0233, Train Acc: 0.9896, Train F1: 0.9893\n",
      "Val Loss: 0.2952, Val Acc: 0.9140, Val F1: 0.9135\n",
      "Early stopping\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0355: 100%|██████████| 7/7 [00:05<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0431, Train Acc: 0.9773, Train F1: 0.8854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0015: 100%|██████████| 7/7 [00:05<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.0030, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0031: 100%|██████████| 7/7 [00:05<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.0017, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.0009, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 7/7 [00:05<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.0004, Train Acc: 1.0000, Train F1: 1.0000\n",
      "\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3387: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.8628, Train Acc: 0.7357, Train F1: 0.7151\n",
      "Val Loss: 0.2835, Val Acc: 0.9013, Val F1: 0.8785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2889: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.2508, Train Acc: 0.8981, Train F1: 0.8829\n",
      "Val Loss: 0.2340, Val Acc: 0.9045, Val F1: 0.8899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0812: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.1691, Train Acc: 0.9363, Train F1: 0.9296\n",
      "Val Loss: 0.1590, Val Acc: 0.9459, Val F1: 0.9470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0016: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1134, Train Acc: 0.9498, Train F1: 0.9447\n",
      "Val Loss: 0.2332, Val Acc: 0.9076, Val F1: 0.9008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0057: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.0588, Train Acc: 0.9817, Train F1: 0.9810\n",
      "Val Loss: 0.1414, Val Acc: 0.9459, Val F1: 0.9432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0285, Train Acc: 0.9873, Train F1: 0.9871\n",
      "Val Loss: 0.1545, Val Acc: 0.9522, Val F1: 0.9537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0322, Train Acc: 0.9881, Train F1: 0.9874\n",
      "Val Loss: 0.2753, Val Acc: 0.9236, Val F1: 0.9242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0030: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0268, Train Acc: 0.9920, Train F1: 0.9919\n",
      "Val Loss: 0.1844, Val Acc: 0.9490, Val F1: 0.9537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0182: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0242, Train Acc: 0.9928, Train F1: 0.9930\n",
      "Val Loss: 0.2028, Val Acc: 0.9490, Val F1: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0158, Train Acc: 0.9920, Train F1: 0.9915\n",
      "Val Loss: 0.2649, Val Acc: 0.9427, Val F1: 0.9465\n",
      "Early stopping\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0233: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0192, Train Acc: 0.9953, Train F1: 0.9937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0121: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.0242, Train Acc: 0.9953, Train F1: 0.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.0079, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 7/7 [00:05<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.0006, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0017: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.0009, Train Acc: 1.0000, Train F1: 1.0000\n",
      "\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5893: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.1355, Train Acc: 0.6425, Train F1: 0.6094\n",
      "Val Loss: 0.3313, Val Acc: 0.8822, Val F1: 0.8322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2319: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.3021, Train Acc: 0.8822, Train F1: 0.8674\n",
      "Val Loss: 0.2439, Val Acc: 0.9108, Val F1: 0.8810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0319: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.1494, Train Acc: 0.9403, Train F1: 0.9340\n",
      "Val Loss: 0.2276, Val Acc: 0.9236, Val F1: 0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0891: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1041, Train Acc: 0.9554, Train F1: 0.9537\n",
      "Val Loss: 0.1598, Val Acc: 0.9331, Val F1: 0.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1453: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.0634, Train Acc: 0.9737, Train F1: 0.9744\n",
      "Val Loss: 0.1860, Val Acc: 0.9427, Val F1: 0.9272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0079: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0479, Train Acc: 0.9817, Train F1: 0.9810\n",
      "Val Loss: 0.2045, Val Acc: 0.9299, Val F1: 0.9256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0026: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0210, Train Acc: 0.9920, Train F1: 0.9916\n",
      "Val Loss: 0.5239, Val Acc: 0.9108, Val F1: 0.9026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0023: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0996, Train Acc: 0.9674, Train F1: 0.9665\n",
      "Val Loss: 0.2299, Val Acc: 0.9204, Val F1: 0.9104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0442: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0272, Train Acc: 0.9912, Train F1: 0.9918\n",
      "Val Loss: 0.2198, Val Acc: 0.9363, Val F1: 0.9295\n",
      "Early stopping\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0025: 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0456, Train Acc: 0.9910, Train F1: 0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0018: 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.0036, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0010: 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.0012, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.0027, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 7/7 [00:05<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.0003, Train Acc: 1.0000, Train F1: 1.0000\n",
      "\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4951: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.8277, Train Acc: 0.7532, Train F1: 0.7310\n",
      "Val Loss: 0.3096, Val Acc: 0.8949, Val F1: 0.8761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0037: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.2414, Train Acc: 0.9053, Train F1: 0.8950\n",
      "Val Loss: 0.3961, Val Acc: 0.8854, Val F1: 0.8565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0087: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.1802, Train Acc: 0.9275, Train F1: 0.9228\n",
      "Val Loss: 0.2693, Val Acc: 0.9013, Val F1: 0.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0332: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1195, Train Acc: 0.9459, Train F1: 0.9434\n",
      "Val Loss: 0.2757, Val Acc: 0.9013, Val F1: 0.8908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0636: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.0597, Train Acc: 0.9761, Train F1: 0.9766\n",
      "Val Loss: 0.1685, Val Acc: 0.9363, Val F1: 0.9290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0498, Train Acc: 0.9777, Train F1: 0.9785\n",
      "Val Loss: 0.2059, Val Acc: 0.9299, Val F1: 0.9291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0080: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0233, Train Acc: 0.9928, Train F1: 0.9927\n",
      "Val Loss: 0.2588, Val Acc: 0.9331, Val F1: 0.9229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0178, Train Acc: 0.9944, Train F1: 0.9949\n",
      "Val Loss: 0.3170, Val Acc: 0.9076, Val F1: 0.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0477: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0291, Train Acc: 0.9920, Train F1: 0.9920\n",
      "Val Loss: 0.1705, Val Acc: 0.9459, Val F1: 0.9411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0031: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0265, Train Acc: 0.9881, Train F1: 0.9880\n",
      "Val Loss: 0.2058, Val Acc: 0.9331, Val F1: 0.9272\n",
      "Early stopping\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0030: 100%|██████████| 9/9 [00:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0170, Train Acc: 0.9962, Train F1: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 9/9 [00:06<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.0013, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 9/9 [00:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.0008, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 9/9 [00:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.0005, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 9/9 [00:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.0006, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160351/481695052.py:234: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"convNext_model_fold{fold}_final.pth\"))\n",
      "Predicting Fold 1: 100%|██████████| 99/99 [00:18<00:00,  5.29it/s]\n",
      "Predicting Fold 2: 100%|██████████| 99/99 [00:18<00:00,  5.28it/s]\n",
      "Predicting Fold 3: 100%|██████████| 99/99 [00:18<00:00,  5.28it/s]\n",
      "Predicting Fold 4: 100%|██████████| 99/99 [00:18<00:00,  5.28it/s]\n",
      "Predicting Fold 5: 100%|██████████| 99/99 [00:18<00:00,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble prediction completed and saved to pred_ensemble.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    misclassified = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_np = preds.argmax(dim=1).detach().cpu().numpy()\n",
    "            targets_np = targets.detach().cpu().numpy()\n",
    "            preds_list.extend(preds_np)\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            misclassified.extend(np.where(preds_np != targets_np)[0])\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1, misclassified\n",
    "\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'convnextv2_large'\n",
    "    img_size = 224\n",
    "    LR = 1e-4\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32\n",
    "    num_workers = 4\n",
    "    n_splits = 5\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "        ], p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "            A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.5),\n",
    "        ], p=0.5),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(blur_limit=3, p=0.5),\n",
    "            A.MedianBlur(blur_limit=3, p=0.5),\n",
    "            A.GaussianBlur(blur_limit=3, p=0.5),\n",
    "        ], p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=5, border_mode=0, p=0.5),\n",
    "        A.CoarseDropout(max_holes=8, max_height=img_size//20, max_width=img_size//20, min_holes=5, fill_value=255, p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['target']), 1):\n",
    "        print(f\"\\nFold {fold}\")\n",
    "\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train/\"), transform=train_transform)\n",
    "        val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train/\"), transform=val_transform)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "        if fold == 1:\n",
    "            print(f\"\\nModel structure of {model_name}:\")\n",
    "            print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "        best_val_f1 = 0\n",
    "        misclassified_data = []\n",
    "        early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "            val_loss, val_acc, val_f1, misclassified = validate(val_loader, model, loss_fn, device)\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f\"convNext_model_fold{fold}.pth\")\n",
    "\n",
    "            misclassified_data.extend(val_df.iloc[misclassified].index)\n",
    "\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        misclassified_df = df.loc[misclassified_data]\n",
    "        misclassified_dataset = ImageDataset(misclassified_df, os.path.join(data_path, \"train/\"), transform=train_transform)\n",
    "        misclassified_loader = DataLoader(misclassified_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        print(\"\\nFine-tuning with misclassified data\")\n",
    "        for epoch in range(5):\n",
    "            train_loss, train_acc, train_f1 = train_one_epoch(misclassified_loader, model, optimizer, loss_fn, device)\n",
    "            print(f\"Epoch {epoch+1}/5\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"convNext_model_fold{fold}_final.pth\")\n",
    "\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test/\"), transform=val_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    ensemble_preds = []\n",
    "    for fold in range(1, n_splits + 1):\n",
    "        model.load_state_dict(torch.load(f\"convNext_model_fold{fold}_final.pth\"))\n",
    "        model.eval()\n",
    "        fold_preds = []\n",
    "\n",
    "        for image, _ in tqdm(test_loader, desc=f\"Predicting Fold {fold}\"):\n",
    "            image = image.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(image)\n",
    "            fold_preds.extend(preds.softmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        ensemble_preds.append(fold_preds)\n",
    "\n",
    "    final_preds = np.mean(ensemble_preds, axis=0).argmax(axis=1)\n",
    "\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = final_preds\n",
    "    pred_df.to_csv(\"pred_ensemble.csv\", index=False)\n",
    "    print(\"Ensemble prediction completed and saved to pred_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters Tunning With CNN Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNeXt V2 Large 모델 + Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import optuna\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# Optuna를 이용한 하이퍼파라미터 최적화 함수\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 탐색 공간 정의\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "\n",
    "    # 데이터 로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 학습 및 검증\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "\n",
    "    return best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '../data/'\n",
    "model_name = 'convnextv2_large'\n",
    "img_size = 224  # ConvNeXt V2에 적합한 이미지 크기\n",
    "EPOCHS = 30\n",
    "num_workers = 4\n",
    "\n",
    "# 데이터 증강 설정\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터 로드 및 분할\n",
    "df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "# Optuna를 이용한 하이퍼파라미터 최적화\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 최종 모델 학습\n",
    "best_lr = best_params['lr']\n",
    "best_batch_size = best_params['batch_size']\n",
    "best_weight_decay = best_params['weight_decay']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# 모델 구조 출력\n",
    "print(f\"\\nModel structure of {model_name}:\")\n",
    "print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "# 학습 루프\n",
    "best_val_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNext V2 Large + WanDB Sweep\n",
    "- pip install wandb\n",
    "- wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "#class ImageDataset(Dataset):\n",
    "\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "#def validate(loader, model, loss_fn, device):\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "#def print_model_summary(model, input_size):\n",
    "\n",
    "\n",
    "# wandb sweep을 위한 학습 함수\n",
    "def train():\n",
    "    # wandb 초기화\n",
    "    run = wandb.init(entity=\"cho\") #사용자에 따라 자신의 도메인 네임 설정!!!\n",
    "    config = wandb.config\n",
    "\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = config.model_name\n",
    "    img_size = config.img_size\n",
    "    LR = config.learning_rate\n",
    "    EPOCHS = config.epochs\n",
    "    BATCH_SIZE = config.batch_size\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=config.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # wandb에 로그 기록\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            wandb.run.summary[\"best_val_f1\"] = best_val_f1\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# wandb sweep 설정\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'val_f1',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'model_name': {\n",
    "            'values': ['convnextv2_large', 'efficientnet_b4']\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-3\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'img_size': {\n",
    "            'values': [224, 256, 288]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [1e-5, 1e-4, 1e-3]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 30\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# wandb sweep 실행 및 최고 성능 모델 찾기\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"cvmodel\",entity=\"cho\")\n",
    "wandb.agent(sweep_id, train, count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 최고 성능 모델의 설정 가져오기\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"dl-12/cvmodel/{sweep_id}\")\n",
    "best_run = sweep.best_run()\n",
    "best_config = best_run.config\n",
    "\n",
    "  \n",
    "# 최고 성능 모델의 설정 사용\n",
    "model_name = best_config['model_name']\n",
    "img_size = best_config['img_size']\n",
    "BATCH_SIZE = best_config['batch_size']\n",
    "num_workers = 4\n",
    "\n",
    "# 테스트 데이터 변환\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 테스트 데이터셋 및 데이터로더 생성\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 최고 성능 모델 생성\n",
    "model = timm.create_model(model_name, pretrained=False, num_classes=17).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to pred.csv\")\n",
    "\n",
    "# wandb에 결과 업로드\n",
    "wandb.init(project=\"cvmodel\", name=\"best_model_prediction\", entity=\"cho\")\n",
    "wandb.config.update(best_config)\n",
    "wandb.save(\"pred.csv\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 기반 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Loss: 2.8486:   2%|▎         | 1/40 [00:01<01:09,  1.78s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 9.31 MiB is free. Process 4106016 has 5.39 GiB memory in use. Process 231175 has 786.00 MiB memory in use. Process 233911 has 786.00 MiB memory in use. Process 238049 has 876.00 MiB memory in use. Process 243247 has 876.00 MiB memory in use. Process 248199 has 876.00 MiB memory in use. Process 253853 has 786.00 MiB memory in use. Process 256125 has 786.00 MiB memory in use. Process 262202 has 12.62 GiB memory in use. Of the allocated memory 11.57 GiB is allocated by PyTorch, and 764.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 187\u001b[0m\n\u001b[1;32m    185\u001b[0m best_val_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m--> 187\u001b[0m     train_loss, train_acc, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     val_loss, val_acc, val_f1 \u001b[38;5;241m=\u001b[39m validate(val_loader, model, loss_fn, device)\n\u001b[1;32m    189\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader, model, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 60\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, targets)\n\u001b[1;32m     62\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/swin_transformer.py:619\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 619\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/swin_transformer.py:611\u001b[0m, in \u001b[0;36mSwinTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    610\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x)\n\u001b[0;32m--> 611\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    612\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/swin_transformer.py:443\u001b[0m, in \u001b[0;36mSwinTransformerStage.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    441\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/swin_transformer.py:323\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    322\u001b[0m     B, H, W, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 323\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    324\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C)\n\u001b[1;32m    325\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/swin_transformer.py:307\u001b[0m, in \u001b[0;36mSwinTransformerBlock._attn\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    304\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m x_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_area, C)  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# W-MSA/SW-MSA\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# merge windows\u001b[39;00m\n\u001b[1;32m    310\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m], C)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/swin_transformer.py:171\u001b[0m, in \u001b[0;36mWindowAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m--> 171\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     attn \u001b[38;5;241m=\u001b[39m attn \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rel_pos_bias()\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 9.31 MiB is free. Process 4106016 has 5.39 GiB memory in use. Process 231175 has 786.00 MiB memory in use. Process 233911 has 786.00 MiB memory in use. Process 238049 has 876.00 MiB memory in use. Process 243247 has 876.00 MiB memory in use. Process 248199 has 876.00 MiB memory in use. Process 253853 has 786.00 MiB memory in use. Process 256125 has 786.00 MiB memory in use. Process 262202 has 12.62 GiB memory in use. Of the allocated memory 11.57 GiB is allocated by PyTorch, and 764.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0, min_f1_score=0.9):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.min_f1_score = min_f1_score\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, val_f1):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience and val_f1 >= self.min_f1_score:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            \n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'swin_large_patch4_window7_224'  # Swin Transformer Large 모델\n",
    "    img_size = 224  # Swin Transformer에 적합한 이미지 크기\n",
    "    LR = 2e-5  # 1e-4 에서 학습률 조정 : 기존이 더 좋음\n",
    "    EPOCHS = 30 # 30에서 조정 : 기존이 더 좋음\n",
    "    BATCH_SIZE = 32  # 배치 크기 조정\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    # Early stopping 설정\n",
    "    # early_stopping = EarlyStopping(patience=5, min_delta=0.001, min_f1_score=0.93)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    # print(f\"\\nModel structure of {model_name}:\")\n",
    "    # print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"swin_t_model.pth\")\n",
    "        \n",
    "        # Early stopping 체크\n",
    "        # early_stopping(val_loss, val_f1)\n",
    "        # if early_stopping.early_stop:\n",
    "        #     print(f\"Early stopping triggered at epoch {epoch+1} with F1 score: {val_f1:.4f}\")\n",
    "        #     break\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"swin_t_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred_swin.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred_swin.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified k-fold cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1379: 100%|██████████| 40/40 [00:20<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 1.9977, Train Acc: 0.4865, Train F1: 0.4503\n",
      "Val Loss: 0.8827, Val Acc: 0.7739, Val F1: 0.6936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4577: 100%|██████████| 40/40 [00:20<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.6732, Train Acc: 0.8049, Train F1: 0.7707\n",
      "Val Loss: 0.4043, Val Acc: 0.8631, Val F1: 0.8179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3563: 100%|██████████| 40/40 [00:20<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.3740, Train Acc: 0.8814, Train F1: 0.8610\n",
      "Val Loss: 0.2678, Val Acc: 0.8822, Val F1: 0.8490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3622: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.2812, Train Acc: 0.8989, Train F1: 0.8843\n",
      "Val Loss: 0.2281, Val Acc: 0.8949, Val F1: 0.8587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1351: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.2306, Train Acc: 0.9092, Train F1: 0.8988\n",
      "Val Loss: 0.3207, Val Acc: 0.8885, Val F1: 0.8724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4066: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.1993, Train Acc: 0.9307, Train F1: 0.9236\n",
      "Val Loss: 0.2139, Val Acc: 0.9108, Val F1: 0.8802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0564: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.1616, Train Acc: 0.9411, Train F1: 0.9367\n",
      "Val Loss: 0.2266, Val Acc: 0.9236, Val F1: 0.9109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0156: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.1497, Train Acc: 0.9427, Train F1: 0.9381\n",
      "Val Loss: 0.1785, Val Acc: 0.9076, Val F1: 0.8937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1676: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.1273, Train Acc: 0.9570, Train F1: 0.9535\n",
      "Val Loss: 0.2308, Val Acc: 0.9236, Val F1: 0.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2569: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.1152, Train Acc: 0.9562, Train F1: 0.9555\n",
      "Val Loss: 0.2009, Val Acc: 0.9299, Val F1: 0.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0047: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0833, Train Acc: 0.9737, Train F1: 0.9735\n",
      "Val Loss: 0.2476, Val Acc: 0.9236, Val F1: 0.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0017: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "Train Loss: 0.1018, Train Acc: 0.9586, Train F1: 0.9557\n",
      "Val Loss: 0.1968, Val Acc: 0.9331, Val F1: 0.9255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2389: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "Train Loss: 0.1085, Train Acc: 0.9578, Train F1: 0.9558\n",
      "Val Loss: 0.2119, Val Acc: 0.9076, Val F1: 0.8933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0059: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.0823, Train Acc: 0.9705, Train F1: 0.9685\n",
      "Val Loss: 0.1973, Val Acc: 0.9268, Val F1: 0.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "Train Loss: 0.0565, Train Acc: 0.9825, Train F1: 0.9822\n",
      "Val Loss: 0.1846, Val Acc: 0.9299, Val F1: 0.9248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0499: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "Train Loss: 0.0534, Train Acc: 0.9817, Train F1: 0.9814\n",
      "Val Loss: 0.2300, Val Acc: 0.9268, Val F1: 0.9204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0983: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "Train Loss: 0.0583, Train Acc: 0.9833, Train F1: 0.9821\n",
      "Val Loss: 0.1958, Val Acc: 0.9363, Val F1: 0.9308\n",
      "Early stopping triggered at epoch 17 with F1 score: 0.9308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:18<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7443: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 2.0241, Train Acc: 0.4682, Train F1: 0.4419\n",
      "Val Loss: 0.9525, Val Acc: 0.7516, Val F1: 0.7064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5209: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.7057, Train Acc: 0.7986, Train F1: 0.7692\n",
      "Val Loss: 0.4094, Val Acc: 0.8408, Val F1: 0.8119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3512: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.3797, Train Acc: 0.8838, Train F1: 0.8663\n",
      "Val Loss: 0.3192, Val Acc: 0.8631, Val F1: 0.8498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1091: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.2739, Train Acc: 0.9037, Train F1: 0.8893\n",
      "Val Loss: 0.3723, Val Acc: 0.8694, Val F1: 0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6342: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.2704, Train Acc: 0.9005, Train F1: 0.8886\n",
      "Val Loss: 0.2797, Val Acc: 0.8854, Val F1: 0.8749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1422: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.2116, Train Acc: 0.9172, Train F1: 0.9051\n",
      "Val Loss: 0.2317, Val Acc: 0.8949, Val F1: 0.8831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0532: 100%|██████████| 40/40 [00:20<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.1665, Train Acc: 0.9482, Train F1: 0.9401\n",
      "Val Loss: 0.2309, Val Acc: 0.8949, Val F1: 0.8818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0940: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.1499, Train Acc: 0.9530, Train F1: 0.9486\n",
      "Val Loss: 0.2127, Val Acc: 0.9108, Val F1: 0.9076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5626: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.1320, Train Acc: 0.9562, Train F1: 0.9528\n",
      "Val Loss: 0.2164, Val Acc: 0.8949, Val F1: 0.8864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0135: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.1205, Train Acc: 0.9546, Train F1: 0.9499\n",
      "Val Loss: 0.1943, Val Acc: 0.9045, Val F1: 0.8985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1353: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0913, Train Acc: 0.9745, Train F1: 0.9730\n",
      "Val Loss: 0.2765, Val Acc: 0.9013, Val F1: 0.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0312:  78%|███████▊  | 31/40 [00:16<00:04,  1.91it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "        self.df = df.values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0, min_f1_score=0.9):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.min_f1_score = min_f1_score\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, val_f1):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience and val_f1 >= self.min_f1_score:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            \n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'swin_large_patch4_window7_224'  # Swin Transformer Large 모델\n",
    "    img_size = 224  # Swin Transformer에 적합한 이미지 크기\n",
    "    LR = 2e-5  # 1e-4 에서 학습률 조정 : 기존이 더 좋음\n",
    "    EPOCHS = 100 # 30에서 조정 : 기존이 더 좋음\n",
    "    BATCH_SIZE = 32  # 배치 크기 조정\n",
    "    num_workers = 4\n",
    "    n_splits = 5  # Number of K-fold splits\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "\n",
    "    # Stratified K-Fold 설정\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # 전체 결과 저장\n",
    "    final_preds_list = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['target'])):\n",
    "        print(f\"Fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        train_df = df.iloc[train_idx]\n",
    "        val_df = df.iloc[val_idx]\n",
    "\n",
    "        train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "        val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "        test_dataset = ImageDataset(pd.read_csv(os.path.join(data_path, \"sample_submission.csv\")), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        # 모델 설정\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "        # Early stopping 설정\n",
    "        early_stopping = EarlyStopping(patience=5, min_delta=0.001, min_f1_score=0.93)\n",
    "\n",
    "        # 모델 구조 출력\n",
    "        # print(f\"\\nModel structure of {model_name}:\")\n",
    "        # print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "        # 학습 루프\n",
    "        best_val_f1 = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "            val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f\"swin_t_model_fold_{fold}.pth\")\n",
    "            \n",
    "            # Early stopping 체크\n",
    "            early_stopping(val_loss, val_f1)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1} with F1 score: {val_f1:.4f}\")\n",
    "                break\n",
    "\n",
    "        # 테스트 데이터 추론\n",
    "        model.load_state_dict(torch.load(f\"swin_t_model_fold_{fold}.pth\"))\n",
    "        model.eval()\n",
    "        fold_preds_list = []\n",
    "\n",
    "        for image, _ in tqdm(test_loader):\n",
    "            image = image.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(image)\n",
    "            fold_preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        # 결과 저장\n",
    "        final_preds_list.append(fold_preds_list)\n",
    "\n",
    "    # 결과 앙상블\n",
    "    final_preds = np.mean(np.array(final_preds_list), axis=0).astype(int)\n",
    "\n",
    "    pred_df = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n",
    "    pred_df['target'] = final_preds\n",
    "    pred_df.to_csv(\"pred_swin_kfold.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred_swin.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin-T clustering & classification\n",
    "- 이미지를 유사한 이미지로 5개로 그룹핑 하고 분석하는 모델\n",
    "- early stoping 코드에서 강제적으로 f1 score boundary 를 줄수 있게 변경(분류하면 과소적합이 되는 구간이 있음.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Extracting features: 100%|██████████| 1570/1570 [08:21<00:00,  3.13it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for cluster 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.9624: 100%|██████████| 23/23 [00:07<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 2.3894, Train Acc: 0.2272, Train F1: 0.1160\n",
      "Val Loss: 1.7668, Val Acc: 0.4709, Val F1: 0.3021\n",
      "New best F1 score: 0.3021\n",
      "Validation loss decreased (inf --> 1.766814). F1 score: 0.302139. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1065: 100%|██████████| 23/23 [00:06<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 1.3509, Train Acc: 0.6027, Train F1: 0.4836\n",
      "Val Loss: 0.8510, Val Acc: 0.7302, Val F1: 0.6464\n",
      "New best F1 score: 0.6464\n",
      "Validation loss decreased (1.766814 --> 0.851001). F1 score: 0.646374. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6082: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.7066, Train Acc: 0.7891, Train F1: 0.7322\n",
      "Val Loss: 0.6014, Val Acc: 0.7778, Val F1: 0.6829\n",
      "New best F1 score: 0.6829\n",
      "Validation loss decreased (0.851001 --> 0.601394). F1 score: 0.682854. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3245: 100%|██████████| 23/23 [00:06<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.4193, Train Acc: 0.8721, Train F1: 0.8133\n",
      "Val Loss: 0.4649, Val Acc: 0.8095, Val F1: 0.7426\n",
      "New best F1 score: 0.7426\n",
      "Validation loss decreased (0.601394 --> 0.464942). F1 score: 0.742565. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2456: 100%|██████████| 23/23 [00:06<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.3084, Train Acc: 0.9034, Train F1: 0.8855\n",
      "Val Loss: 0.4378, Val Acc: 0.7989, Val F1: 0.8113\n",
      "New best F1 score: 0.8113\n",
      "Validation loss decreased (0.464942 --> 0.437803). F1 score: 0.811311. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2351: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.2114, Train Acc: 0.9374, Train F1: 0.9182\n",
      "Val Loss: 0.4343, Val Acc: 0.8148, Val F1: 0.8266\n",
      "New best F1 score: 0.8266\n",
      "Validation loss decreased (0.437803 --> 0.434306). F1 score: 0.826585. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1550: 100%|██████████| 23/23 [00:06<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.1245, Train Acc: 0.9782, Train F1: 0.9810\n",
      "Val Loss: 0.4091, Val Acc: 0.8307, Val F1: 0.8408\n",
      "New best F1 score: 0.8408\n",
      "Validation loss decreased (0.434306 --> 0.409088). F1 score: 0.840833. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0210: 100%|██████████| 23/23 [00:06<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.0843, Train Acc: 0.9823, Train F1: 0.9584\n",
      "Val Loss: 0.4157, Val Acc: 0.8254, Val F1: 0.8502\n",
      "New best F1 score: 0.8502\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1330: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.0708, Train Acc: 0.9932, Train F1: 0.9947\n",
      "Val Loss: 0.4342, Val Acc: 0.8360, Val F1: 0.8642\n",
      "New best F1 score: 0.8642\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0200: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.0580, Train Acc: 0.9891, Train F1: 0.9914\n",
      "Val Loss: 0.5063, Val Acc: 0.8148, Val F1: 0.8343\n",
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1000: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0391, Train Acc: 0.9959, Train F1: 0.9965\n",
      "Val Loss: 0.5177, Val Acc: 0.8148, Val F1: 0.8329\n",
      "EarlyStopping counter: 4 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0433: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "Train Loss: 0.0328, Train Acc: 0.9973, Train F1: 0.9981\n",
      "Val Loss: 0.5408, Val Acc: 0.8201, Val F1: 0.8485\n",
      "EarlyStopping counter: 5 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0346: 100%|██████████| 23/23 [00:06<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "Train Loss: 0.0216, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.5339, Val Acc: 0.8307, Val F1: 0.8589\n",
      "EarlyStopping counter: 6 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0366: 100%|██████████| 23/23 [00:06<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.0146, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.5482, Val Acc: 0.8571, Val F1: 0.8697\n",
      "New best F1 score: 0.8697\n",
      "EarlyStopping counter: 7 out of 7\n",
      "Early stopping. Best validation loss: 0.409088, Best F1 score: 0.840833\n",
      "\n",
      "Training model for cluster 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5359: 100%|██████████| 17/17 [00:04<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 1.7745, Train Acc: 0.5969, Train F1: 0.2815\n",
      "Val Loss: 0.5390, Val Acc: 0.9600, Val F1: 0.8432\n",
      "New best F1 score: 0.8432\n",
      "Validation loss decreased (inf --> 0.539037). F1 score: 0.843164. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5383: 100%|██████████| 17/17 [00:04<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.3359, Train Acc: 0.9731, Train F1: 0.7593\n",
      "Val Loss: 0.0748, Val Acc: 0.9920, Val F1: 0.8684\n",
      "New best F1 score: 0.8684\n",
      "Validation loss decreased (0.539037 --> 0.074824). F1 score: 0.868421. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0186: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.0658, Train Acc: 0.9942, Train F1: 0.8629\n",
      "Val Loss: 0.0176, Val Acc: 1.0000, Val F1: 1.0000\n",
      "New best F1 score: 1.0000\n",
      "Validation loss decreased (0.074824 --> 0.017606). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0084: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.0263, Train Acc: 0.9981, Train F1: 0.8877\n",
      "Val Loss: 0.0055, Val Acc: 1.0000, Val F1: 1.0000\n",
      "Validation loss decreased (0.017606 --> 0.005532). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.0147, Train Acc: 0.9981, Train F1: 0.8877\n",
      "Val Loss: 0.0039, Val Acc: 1.0000, Val F1: 1.0000\n",
      "Validation loss decreased (0.005532 --> 0.003931). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0066: 100%|██████████| 17/17 [00:04<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.0122, Train Acc: 0.9981, Train F1: 0.8877\n",
      "Val Loss: 0.0028, Val Acc: 1.0000, Val F1: 1.0000\n",
      "Validation loss decreased (0.003931 --> 0.002810). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0056: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.0107, Train Acc: 0.9981, Train F1: 0.8881\n",
      "Val Loss: 0.0017, Val Acc: 1.0000, Val F1: 1.0000\n",
      "Validation loss decreased (0.002810 --> 0.001662). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0091: 100%|██████████| 17/17 [00:04<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.0080, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0016, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0008: 100%|██████████| 17/17 [00:04<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.0027, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0008, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0065: 100%|██████████| 17/17 [00:04<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.0042, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0007, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0014: 100%|██████████| 17/17 [00:04<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0027, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0006, Val Acc: 1.0000, Val F1: 1.0000\n",
      "Validation loss decreased (0.001662 --> 0.000624). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "Train Loss: 0.0024, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0005, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 17/17 [00:04<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "Train Loss: 0.0020, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0004, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.0026, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0005, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 17/17 [00:04<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "Train Loss: 0.0012, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0004, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 4 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0018: 100%|██████████| 17/17 [00:04<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "Train Loss: 0.0048, Train Acc: 0.9981, Train F1: 0.8881\n",
      "Val Loss: 0.0004, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 5 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0019: 100%|██████████| 17/17 [00:04<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "Train Loss: 0.0014, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0004, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 6 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 17/17 [00:04<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "Train Loss: 0.0008, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0004, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 7 out of 7\n",
      "Early stopping. Best validation loss: 0.000624, Best F1 score: 1.000000\n",
      "\n",
      "Performing inference on test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1549283/822344420.py:299: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"swin_b_model_cluster_{cluster}.pth\"))\n",
      "Predicting cluster 0: 100%|██████████| 99/99 [00:10<00:00,  9.82it/s]\n",
      "Predicting cluster 1: 100%|██████████| 99/99 [00:10<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Swin-B 모델 로드\n",
    "def load_swin_b_model(num_classes=None):\n",
    "    model = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "# 특성 추출 함수\n",
    "def extract_features(img_path, model):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = np.array(img)\n",
    "    img = transform(image=img)['image']\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = model.forward_features(img)\n",
    "    return features.squeeze().numpy()\n",
    "\n",
    "# 이미지 클러스터링 함수\n",
    "def cluster_images(data_path, n_clusters=5):\n",
    "    feature_extractor = load_swin_b_model(num_classes=None)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    image_files = [f for f in os.listdir(data_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    features = []\n",
    "    for img_file in tqdm(image_files, desc=\"Extracting features\"):\n",
    "        img_path = os.path.join(data_path, img_file)\n",
    "        feature = extract_features(img_path, feature_extractor)\n",
    "        features.append(feature.reshape(-1))  # Flatten the feature array\n",
    "    \n",
    "    features = np.array(features)\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "    return dict(zip(image_files, clusters))\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None, cluster_dict=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.cluster_dict = cluster_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        \n",
    "        cluster = self.cluster_dict.get(name, -1) if self.cluster_dict else -1\n",
    "        return img, target, cluster\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        self.best_f1 = -np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, f1_score, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, f1_score, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). '\n",
    "                            f'F1 score: {f1_score:.6f}. Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        self.best_f1 = f1_score\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets, _ in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets, _ in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    img_size = 224\n",
    "    LR = 2e-5\n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 32\n",
    "    num_workers = 4\n",
    "    n_clusters = 2\n",
    "\n",
    "    # 클러스터링 수행\n",
    "    print(\"Clustering images...\")\n",
    "    cluster_dict = cluster_images(os.path.join(data_path, \"train_preprocessed/\"), n_clusters)\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform, cluster_dict=cluster_dict)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform, cluster_dict=cluster_dict)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 각 클러스터에 대한 모델 학습\n",
    "    for cluster in range(n_clusters):\n",
    "        print(f\"\\nTraining model for cluster {cluster}\")\n",
    "        \n",
    "        # 클러스터에 해당하는 데이터만 선택\n",
    "        train_cluster = [data for data in train_dataset if data[2] == cluster]\n",
    "        val_cluster = [data for data in val_dataset if data[2] == cluster]\n",
    "        \n",
    "        if len(train_cluster) == 0 or len(val_cluster) == 0:\n",
    "            print(f\"Skipping cluster {cluster} due to insufficient data\")\n",
    "            continue\n",
    "        \n",
    "        train_cluster_loader = DataLoader(train_cluster, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        val_cluster_loader = DataLoader(val_cluster, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        # Swin-B 모델 설정\n",
    "        model = load_swin_b_model(num_classes=17).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "        # Early stopping 설정\n",
    "        early_stopping = EarlyStopping(patience=7, verbose=True, delta=0.001, path='cluster_model.pth')\n",
    "\n",
    "\n",
    "        # 모델 구조 출력\n",
    "        # print(f\"\\nModel structure of Swin-B for cluster {cluster}:\")\n",
    "        # print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "        # 학습 루프\n",
    "        best_val_f1 = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc, train_f1 = train_one_epoch(train_cluster_loader, model, optimizer, loss_fn, device)\n",
    "            val_loss, val_acc, val_f1 = validate(val_cluster_loader, model, loss_fn, device)\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f\"swin_b_model_cluster_{cluster}.pth\")\n",
    "                print(f\"New best F1 score: {best_val_f1:.4f}\")\n",
    "\n",
    "            #조기 종료 체크 (validation 에러 기준)\n",
    "            early_stopping(val_loss, val_f1, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping. Best validation loss: {early_stopping.val_loss_min:.6f}, \"\n",
    "                    f\"Best F1 score: {early_stopping.best_f1:.6f}\")\n",
    "                break\n",
    "            \n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    print(\"\\nPerforming inference on test data\")\n",
    "    test_preds = []\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        model = load_swin_b_model(num_classes=17).to(device)\n",
    "        model.load_state_dict(torch.load(f\"swin_b_model_cluster_{cluster}.pth\"))\n",
    "        model.eval()\n",
    "        \n",
    "        cluster_preds = []\n",
    "        for image, _, _ in tqdm(test_loader, desc=f\"Predicting cluster {cluster}\"):\n",
    "            image = image.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(image)\n",
    "            cluster_preds.extend(preds.detach().cpu().numpy())\n",
    "        \n",
    "        test_preds.append(cluster_preds)\n",
    "    \n",
    "    # 모든 클러스터의 예측을 결합\n",
    "    final_preds = np.mean(test_preds, axis=0)\n",
    "    final_preds = np.argmax(final_preds, axis=1)\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = final_preds\n",
    "    pred_df.to_csv(\"swin_pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블\n",
    "CNN 모델(ConvNeXt V2 Large)과 Transformer 모델(ViT Large)을 결합한 앙상블 모델\n",
    "- CNN 모델로 'convnextv2_large'를 사용합니다.\n",
    "- Transformer 모델로 'vit_large_patch16_224'를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convnext v2 + vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 앙상블 모델 클래스 정의\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.model1(x)\n",
    "        out2 = self.model2(x)\n",
    "        return (out1 + out2) / 2\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '../data/'\n",
    "img_size = 224\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "num_workers = 4\n",
    "\n",
    "# 데이터 증강 설정\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터 로드 및 분할\n",
    "df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 모델 설정\n",
    "model1 = timm.create_model('convnextv2_large', pretrained=True, num_classes=17)\n",
    "model2 = timm.create_model('vit_large_patch16_224', pretrained=True, num_classes=17)\n",
    "\n",
    "ensemble_model = EnsembleModel(model1, model2).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(ensemble_model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# 학습 루프\n",
    "best_val_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(train_loader, ensemble_model, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc, val_f1 = validate(val_loader, ensemble_model, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(ensemble_model.state_dict(), \"best_ensemble_model.pth\")\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "ensemble_model.load_state_dict(torch.load(\"best_ensemble_model.pth\"))\n",
    "ensemble_model.eval()\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = ensemble_model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"ensemble_pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to ensemble_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 모델 II - 리더 보드 제출용\n",
    "CNN 모델(ConvNeXt V2 Large)과 Transformer 모델(Swin Transformers)을 결합한 앙상블 모델\n",
    "- Hyper Parameter tunning이 전혀 되어 있지 않는 기본 모델 : 향후 최적화 필요\n",
    "- CNN 모델로 'convnextv2_large'를 사용합니다.\n",
    "- Transformer 모델로 'swin_large_patch4_window7_224'를 사용합니다.\n",
    "- software voting(기존 저장된 pth 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 첫 번째 모델과 두 번째 모델 로드\n",
    "model1 = timm.create_model('convnextv2_large', pretrained=False, num_classes=17).to(device)\n",
    "model2 = timm.create_model('swin_large_patch4_window7_224', pretrained=False, num_classes=17).to(device)\n",
    "\n",
    "# 모델 가중치 로드\n",
    "model1.load_state_dict(torch.load('convNext_model.pth'))\n",
    "model2.load_state_dict(torch.load('swin_t_model.pth'))\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 소프트 보팅을 통한 예측\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        preds1 = model1(image)\n",
    "        preds2 = model2(image)\n",
    "        \n",
    "        # 소프트 보팅: 예측 확률의 평균\n",
    "        preds_avg = (torch.softmax(preds1, dim=1) + torch.softmax(preds2, dim=1)) / 2\n",
    "        preds_list.extend(preds_avg.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"ensemble_pred.csv\", index=False)\n",
    "print(\"Ensemble prediction completed and saved to ensemble_pred.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert OCR 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.58it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.1928, Val Loss: 1.5697, Val F1: 0.5721\n",
      "New best model saved with F1 score: 0.5721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.58it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.2582, Val Loss: 0.9615, Val F1: 0.7330\n",
      "New best model saved with F1 score: 0.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.57it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.7290, Val Loss: 0.6976, Val F1: 0.7689\n",
      "New best model saved with F1 score: 0.7689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.57it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.4384, Val Loss: 0.5219, Val F1: 0.8383\n",
      "New best model saved with F1 score: 0.8383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.2738, Val Loss: 0.4419, Val F1: 0.8665\n",
      "New best model saved with F1 score: 0.8665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.1888, Val Loss: 0.4458, Val F1: 0.8719\n",
      "New best model saved with F1 score: 0.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.1101, Val Loss: 0.3717, Val F1: 0.8775\n",
      "New best model saved with F1 score: 0.8775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0795, Val Loss: 0.3791, Val F1: 0.8857\n",
      "New best model saved with F1 score: 0.8857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.0624, Val Loss: 0.3843, Val F1: 0.8852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.0545, Val Loss: 0.3738, Val F1: 0.8909\n",
      "New best model saved with F1 score: 0.8909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.0427, Val Loss: 0.3874, Val F1: 0.8789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.0382, Val Loss: 0.4022, Val F1: 0.8836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.0355, Val Loss: 0.3852, Val F1: 0.8920\n",
      "New best model saved with F1 score: 0.8920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.0329, Val Loss: 0.3719, Val F1: 0.9023\n",
      "New best model saved with F1 score: 0.9023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.0308, Val Loss: 0.3879, Val F1: 0.8955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.36it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.0300, Val Loss: 0.4085, Val F1: 0.8952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.0270, Val Loss: 0.4117, Val F1: 0.8870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.0263, Val Loss: 0.4073, Val F1: 0.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.0253, Val Loss: 0.4008, Val F1: 0.8923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.0258, Val Loss: 0.3905, Val F1: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.0246, Val Loss: 0.3973, Val F1: 0.8896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 0.0236, Val Loss: 0.3983, Val F1: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 0.0249, Val Loss: 0.3965, Val F1: 0.8943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss: 0.0240, Val Loss: 0.3943, Val F1: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.33it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss: 0.0230, Val Loss: 0.3978, Val F1: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss: 0.0225, Val Loss: 0.3953, Val F1: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss: 0.0221, Val Loss: 0.3965, Val F1: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss: 0.0220, Val Loss: 0.3967, Val F1: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss: 0.0227, Val Loss: 0.3970, Val F1: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss: 0.0224, Val Loss: 0.3970, Val F1: 0.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting test data: 100%|██████████| 99/99 [00:25<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to bert_text_pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# BERT 모델 정의\n",
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('klue/bert-base')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 데이터셋 클래스\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, csv_text_data, tokenizer=None, max_len=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.csv_text_data = csv_text_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        text = self.csv_text_data.get(img_name, \"\")\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.df.iloc[idx, 1], dtype=torch.long) if 'target' in self.df.columns else torch.tensor(0)\n",
    "        }\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    return avg_loss, f1, predictions, true_labels\n",
    "\n",
    "# CSV에서 텍스트 데이터 로드\n",
    "def load_text_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return {row['image']: ' '.join([text for text in eval(row['texts']) if text != '<extra_id_0>']) for _, row in df.iterrows()}\n",
    "\n",
    "# 메인 함수\n",
    "def main():\n",
    "    # 데이터 준비\n",
    "    data_path = '../data/'\n",
    "    train_csv_path = './corrected_train_texts.csv'\n",
    "    test_csv_path = './corrected_test_texts.csv' \n",
    "\n",
    "    train_csv_text_data = load_text_from_csv(train_csv_path)\n",
    "    test_csv_text_data = load_text_from_csv(test_csv_path)\n",
    "\n",
    "    df = pd.read_csv(f\"{data_path}/train_correct_labeling.csv\")\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    # 토크나이저 준비\n",
    "    tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "    # 데이터셋 및 데이터로더 준비\n",
    "    train_dataset = TextDataset(train_df, train_csv_text_data, tokenizer)\n",
    "    val_dataset = TextDataset(val_df, train_csv_text_data, tokenizer)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    num_classes = len(df['target'].unique())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # BERT 모델 학습\n",
    "    bert_model = BERTModel(num_classes).to(device)\n",
    "    bert_optimizer = torch.optim.AdamW(bert_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    bert_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(bert_optimizer, T_max=30)\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(30):\n",
    "        train_loss = train(bert_model, train_loader, criterion, bert_optimizer, bert_scheduler, device)\n",
    "        val_loss, val_f1, _, _ = evaluate(bert_model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(bert_model.state_dict(), 'best_bert_model.pth')\n",
    "            print(f\"New best model saved with F1 score: {best_val_f1:.4f}\")\n",
    "\n",
    "    # 테스트 데이터 준비\n",
    "    test_df = pd.read_csv(f\"{data_path}/sample_submission.csv\")\n",
    "    test_dataset = TextDataset(test_df, test_csv_text_data, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    bert_model.load_state_dict(torch.load('best_bert_model.pth'))\n",
    "    bert_model.eval()\n",
    "    test_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting test data\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = bert_model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    submission_df = pd.DataFrame({'ID': test_df['ID'], 'target': test_predictions})\n",
    "    submission_df.to_csv(\"bert_text_pred.csv\", index=False)\n",
    "    print(\"Test predictions saved to bert_text_pred.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3가지 모델 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting test data: 100%|██████████| 99/99 [00:58<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to ensemble_pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from timm import create_model\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# 멀티모달 데이터셋 클래스\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, csv_text_data, transform=None, tokenizer=None, max_len=512):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.csv_text_data = csv_text_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        img_path = f\"{self.image_dir}/{img_name}\"\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        text = self.csv_text_data.get(img_name, \"\")\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "# CSV에서 텍스트 데이터 로드\n",
    "def load_text_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return {row['image']: ' '.join([text for text in eval(row['texts']) if text != '<extra_id_0>']) for _, row in df.iterrows()}\n",
    "\n",
    "# state_dict 키 변경 함수\n",
    "def rename_keys(state_dict, key_map):\n",
    "    for old_key in list(state_dict.keys()):\n",
    "        if old_key in key_map:\n",
    "            state_dict[key_map[old_key]] = state_dict.pop(old_key)\n",
    "    return state_dict\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "def predict_test_data():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 데이터 준비\n",
    "    data_path = '../data/'\n",
    "    test_csv_path = './corrected_test_texts.csv'\n",
    "    test_csv_text_data = load_text_from_csv(test_csv_path)\n",
    "    test_df = pd.read_csv(f\"{data_path}/sample_submission.csv\")\n",
    "\n",
    "    # 모델 로드\n",
    "    num_classes = 17  # 클래스 수에 맞게 조정\n",
    "    bert_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels=num_classes)\n",
    "    state_dict = torch.load('best_bert_model.pth')\n",
    "    \n",
    "    # 키 변경 맵 정의\n",
    "    key_map = {\n",
    "        \"fc.weight\": \"classifier.weight\",\n",
    "        \"fc.bias\": \"classifier.bias\"\n",
    "    }\n",
    "    \n",
    "    # state_dict 키 변경\n",
    "    state_dict = rename_keys(state_dict, key_map)\n",
    "    \n",
    "    # state_dict 로드\n",
    "    bert_model.load_state_dict(state_dict)\n",
    "    bert_model.to(device)\n",
    "    bert_model.eval()\n",
    "\n",
    "    swin_model = create_model('swin_large_patch4_window7_224', pretrained=False, num_classes=num_classes)\n",
    "    swin_model.load_state_dict(torch.load('swin_large.pt'))\n",
    "    swin_model.to(device)\n",
    "    swin_model.eval()\n",
    "\n",
    "    convnext_model = create_model('convnextv2_large', pretrained=False, num_classes=num_classes)\n",
    "    convnext_model.load_state_dict(torch.load('convnextv2_large.pt'))\n",
    "    convnext_model.to(device)\n",
    "    convnext_model.eval()\n",
    "\n",
    "    # 토크나이저 및 변환 준비\n",
    "    tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 데이터셋 및 데이터로더 준비\n",
    "    test_dataset = MultimodalDataset(test_df, f\"{data_path}/test_preprocessed\", test_csv_text_data, transform, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    test_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting test data\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            texts = batch['text']\n",
    "\n",
    "            bert_outputs = bert_model(input_ids, attention_mask).logits\n",
    "            swin_outputs = swin_model(images)\n",
    "            convnext_outputs = convnext_model(images)\n",
    "\n",
    "            for i in range(len(texts)):\n",
    "                word_count = len(texts[i].split())\n",
    "                if word_count >= 20:\n",
    "                    ensemble_outputs = 0.4 * bert_outputs[i] + 0.4 * swin_outputs[i] + 0.2 * convnext_outputs[i]\n",
    "                else:\n",
    "                    ensemble_outputs = 0.7 * swin_outputs[i] + 0.3 * convnext_outputs[i]\n",
    "\n",
    "                _, predicted = torch.max(ensemble_outputs, 0)\n",
    "                test_predictions.append(predicted.item())\n",
    "\n",
    "    # 결과 저장\n",
    "    submission_df = pd.DataFrame({'ID': test_df['ID'], 'target': test_predictions})\n",
    "    submission_df.to_csv(\"ensemble_pred.csv\", index=False)\n",
    "    print(\"Test predictions saved to ensemble_pred.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_test_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

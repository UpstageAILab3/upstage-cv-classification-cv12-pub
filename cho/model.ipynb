{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 기반 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple EfficientNet-B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '../data/'\n",
    "model_name = 'efficientnet_b0'\n",
    "img_size = 384\n",
    "LR = 1e-3\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 4\n",
    "\n",
    "# 데이터 증강 설정\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 분할\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "train_dataset = ImageDataset(train_df, \"../data/train_preprocessed/\", transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, \"../data/train_preprocessed/\", transform=val_transform)\n",
    "test_dataset = ImageDataset(\"../data/sample_submission.csv\", \"../data/test_preprocessed/\", transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model structure of efficientnet_b0:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 192, 192]             864\n",
      "          Identity-2         [-1, 32, 192, 192]               0\n",
      "              SiLU-3         [-1, 32, 192, 192]               0\n",
      "    BatchNormAct2d-4         [-1, 32, 192, 192]              64\n",
      "            Conv2d-5         [-1, 32, 192, 192]             288\n",
      "          Identity-6         [-1, 32, 192, 192]               0\n",
      "              SiLU-7         [-1, 32, 192, 192]               0\n",
      "    BatchNormAct2d-8         [-1, 32, 192, 192]              64\n",
      "            Conv2d-9              [-1, 8, 1, 1]             264\n",
      "             SiLU-10              [-1, 8, 1, 1]               0\n",
      "           Conv2d-11             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-12             [-1, 32, 1, 1]               0\n",
      "    SqueezeExcite-13         [-1, 32, 192, 192]               0\n",
      "           Conv2d-14         [-1, 16, 192, 192]             512\n",
      "         Identity-15         [-1, 16, 192, 192]               0\n",
      "         Identity-16         [-1, 16, 192, 192]               0\n",
      "   BatchNormAct2d-17         [-1, 16, 192, 192]              32\n",
      "DepthwiseSeparableConv-18         [-1, 16, 192, 192]               0\n",
      "           Conv2d-19         [-1, 96, 192, 192]           1,536\n",
      "         Identity-20         [-1, 96, 192, 192]               0\n",
      "             SiLU-21         [-1, 96, 192, 192]               0\n",
      "   BatchNormAct2d-22         [-1, 96, 192, 192]             192\n",
      "           Conv2d-23           [-1, 96, 96, 96]             864\n",
      "         Identity-24           [-1, 96, 96, 96]               0\n",
      "             SiLU-25           [-1, 96, 96, 96]               0\n",
      "   BatchNormAct2d-26           [-1, 96, 96, 96]             192\n",
      "           Conv2d-27              [-1, 4, 1, 1]             388\n",
      "             SiLU-28              [-1, 4, 1, 1]               0\n",
      "           Conv2d-29             [-1, 96, 1, 1]             480\n",
      "          Sigmoid-30             [-1, 96, 1, 1]               0\n",
      "    SqueezeExcite-31           [-1, 96, 96, 96]               0\n",
      "           Conv2d-32           [-1, 24, 96, 96]           2,304\n",
      "         Identity-33           [-1, 24, 96, 96]               0\n",
      "         Identity-34           [-1, 24, 96, 96]               0\n",
      "   BatchNormAct2d-35           [-1, 24, 96, 96]              48\n",
      " InvertedResidual-36           [-1, 24, 96, 96]               0\n",
      "           Conv2d-37          [-1, 144, 96, 96]           3,456\n",
      "         Identity-38          [-1, 144, 96, 96]               0\n",
      "             SiLU-39          [-1, 144, 96, 96]               0\n",
      "   BatchNormAct2d-40          [-1, 144, 96, 96]             288\n",
      "           Conv2d-41          [-1, 144, 96, 96]           1,296\n",
      "         Identity-42          [-1, 144, 96, 96]               0\n",
      "             SiLU-43          [-1, 144, 96, 96]               0\n",
      "   BatchNormAct2d-44          [-1, 144, 96, 96]             288\n",
      "           Conv2d-45              [-1, 6, 1, 1]             870\n",
      "             SiLU-46              [-1, 6, 1, 1]               0\n",
      "           Conv2d-47            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-48            [-1, 144, 1, 1]               0\n",
      "    SqueezeExcite-49          [-1, 144, 96, 96]               0\n",
      "           Conv2d-50           [-1, 24, 96, 96]           3,456\n",
      "         Identity-51           [-1, 24, 96, 96]               0\n",
      "         Identity-52           [-1, 24, 96, 96]               0\n",
      "   BatchNormAct2d-53           [-1, 24, 96, 96]              48\n",
      "         Identity-54           [-1, 24, 96, 96]               0\n",
      " InvertedResidual-55           [-1, 24, 96, 96]               0\n",
      "           Conv2d-56          [-1, 144, 96, 96]           3,456\n",
      "         Identity-57          [-1, 144, 96, 96]               0\n",
      "             SiLU-58          [-1, 144, 96, 96]               0\n",
      "   BatchNormAct2d-59          [-1, 144, 96, 96]             288\n",
      "           Conv2d-60          [-1, 144, 48, 48]           3,600\n",
      "         Identity-61          [-1, 144, 48, 48]               0\n",
      "             SiLU-62          [-1, 144, 48, 48]               0\n",
      "   BatchNormAct2d-63          [-1, 144, 48, 48]             288\n",
      "           Conv2d-64              [-1, 6, 1, 1]             870\n",
      "             SiLU-65              [-1, 6, 1, 1]               0\n",
      "           Conv2d-66            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-67            [-1, 144, 1, 1]               0\n",
      "    SqueezeExcite-68          [-1, 144, 48, 48]               0\n",
      "           Conv2d-69           [-1, 40, 48, 48]           5,760\n",
      "         Identity-70           [-1, 40, 48, 48]               0\n",
      "         Identity-71           [-1, 40, 48, 48]               0\n",
      "   BatchNormAct2d-72           [-1, 40, 48, 48]              80\n",
      " InvertedResidual-73           [-1, 40, 48, 48]               0\n",
      "           Conv2d-74          [-1, 240, 48, 48]           9,600\n",
      "         Identity-75          [-1, 240, 48, 48]               0\n",
      "             SiLU-76          [-1, 240, 48, 48]               0\n",
      "   BatchNormAct2d-77          [-1, 240, 48, 48]             480\n",
      "           Conv2d-78          [-1, 240, 48, 48]           6,000\n",
      "         Identity-79          [-1, 240, 48, 48]               0\n",
      "             SiLU-80          [-1, 240, 48, 48]               0\n",
      "   BatchNormAct2d-81          [-1, 240, 48, 48]             480\n",
      "           Conv2d-82             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-83             [-1, 10, 1, 1]               0\n",
      "           Conv2d-84            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-85            [-1, 240, 1, 1]               0\n",
      "    SqueezeExcite-86          [-1, 240, 48, 48]               0\n",
      "           Conv2d-87           [-1, 40, 48, 48]           9,600\n",
      "         Identity-88           [-1, 40, 48, 48]               0\n",
      "         Identity-89           [-1, 40, 48, 48]               0\n",
      "   BatchNormAct2d-90           [-1, 40, 48, 48]              80\n",
      "         Identity-91           [-1, 40, 48, 48]               0\n",
      " InvertedResidual-92           [-1, 40, 48, 48]               0\n",
      "           Conv2d-93          [-1, 240, 48, 48]           9,600\n",
      "         Identity-94          [-1, 240, 48, 48]               0\n",
      "             SiLU-95          [-1, 240, 48, 48]               0\n",
      "   BatchNormAct2d-96          [-1, 240, 48, 48]             480\n",
      "           Conv2d-97          [-1, 240, 24, 24]           2,160\n",
      "         Identity-98          [-1, 240, 24, 24]               0\n",
      "             SiLU-99          [-1, 240, 24, 24]               0\n",
      "  BatchNormAct2d-100          [-1, 240, 24, 24]             480\n",
      "          Conv2d-101             [-1, 10, 1, 1]           2,410\n",
      "            SiLU-102             [-1, 10, 1, 1]               0\n",
      "          Conv2d-103            [-1, 240, 1, 1]           2,640\n",
      "         Sigmoid-104            [-1, 240, 1, 1]               0\n",
      "   SqueezeExcite-105          [-1, 240, 24, 24]               0\n",
      "          Conv2d-106           [-1, 80, 24, 24]          19,200\n",
      "        Identity-107           [-1, 80, 24, 24]               0\n",
      "        Identity-108           [-1, 80, 24, 24]               0\n",
      "  BatchNormAct2d-109           [-1, 80, 24, 24]             160\n",
      "InvertedResidual-110           [-1, 80, 24, 24]               0\n",
      "          Conv2d-111          [-1, 480, 24, 24]          38,400\n",
      "        Identity-112          [-1, 480, 24, 24]               0\n",
      "            SiLU-113          [-1, 480, 24, 24]               0\n",
      "  BatchNormAct2d-114          [-1, 480, 24, 24]             960\n",
      "          Conv2d-115          [-1, 480, 24, 24]           4,320\n",
      "        Identity-116          [-1, 480, 24, 24]               0\n",
      "            SiLU-117          [-1, 480, 24, 24]               0\n",
      "  BatchNormAct2d-118          [-1, 480, 24, 24]             960\n",
      "          Conv2d-119             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-120             [-1, 20, 1, 1]               0\n",
      "          Conv2d-121            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-122            [-1, 480, 1, 1]               0\n",
      "   SqueezeExcite-123          [-1, 480, 24, 24]               0\n",
      "          Conv2d-124           [-1, 80, 24, 24]          38,400\n",
      "        Identity-125           [-1, 80, 24, 24]               0\n",
      "        Identity-126           [-1, 80, 24, 24]               0\n",
      "  BatchNormAct2d-127           [-1, 80, 24, 24]             160\n",
      "        Identity-128           [-1, 80, 24, 24]               0\n",
      "InvertedResidual-129           [-1, 80, 24, 24]               0\n",
      "          Conv2d-130          [-1, 480, 24, 24]          38,400\n",
      "        Identity-131          [-1, 480, 24, 24]               0\n",
      "            SiLU-132          [-1, 480, 24, 24]               0\n",
      "  BatchNormAct2d-133          [-1, 480, 24, 24]             960\n",
      "          Conv2d-134          [-1, 480, 24, 24]           4,320\n",
      "        Identity-135          [-1, 480, 24, 24]               0\n",
      "            SiLU-136          [-1, 480, 24, 24]               0\n",
      "  BatchNormAct2d-137          [-1, 480, 24, 24]             960\n",
      "          Conv2d-138             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-139             [-1, 20, 1, 1]               0\n",
      "          Conv2d-140            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-141            [-1, 480, 1, 1]               0\n",
      "   SqueezeExcite-142          [-1, 480, 24, 24]               0\n",
      "          Conv2d-143           [-1, 80, 24, 24]          38,400\n",
      "        Identity-144           [-1, 80, 24, 24]               0\n",
      "        Identity-145           [-1, 80, 24, 24]               0\n",
      "  BatchNormAct2d-146           [-1, 80, 24, 24]             160\n",
      "        Identity-147           [-1, 80, 24, 24]               0\n",
      "InvertedResidual-148           [-1, 80, 24, 24]               0\n",
      "          Conv2d-149          [-1, 480, 24, 24]          38,400\n",
      "        Identity-150          [-1, 480, 24, 24]               0\n",
      "            SiLU-151          [-1, 480, 24, 24]               0\n",
      "  BatchNormAct2d-152          [-1, 480, 24, 24]             960\n",
      "          Conv2d-153          [-1, 480, 24, 24]          12,000\n",
      "        Identity-154          [-1, 480, 24, 24]               0\n",
      "            SiLU-155          [-1, 480, 24, 24]               0\n",
      "  BatchNormAct2d-156          [-1, 480, 24, 24]             960\n",
      "          Conv2d-157             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-158             [-1, 20, 1, 1]               0\n",
      "          Conv2d-159            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-160            [-1, 480, 1, 1]               0\n",
      "   SqueezeExcite-161          [-1, 480, 24, 24]               0\n",
      "          Conv2d-162          [-1, 112, 24, 24]          53,760\n",
      "        Identity-163          [-1, 112, 24, 24]               0\n",
      "        Identity-164          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-165          [-1, 112, 24, 24]             224\n",
      "InvertedResidual-166          [-1, 112, 24, 24]               0\n",
      "          Conv2d-167          [-1, 672, 24, 24]          75,264\n",
      "        Identity-168          [-1, 672, 24, 24]               0\n",
      "            SiLU-169          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-170          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-171          [-1, 672, 24, 24]          16,800\n",
      "        Identity-172          [-1, 672, 24, 24]               0\n",
      "            SiLU-173          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-174          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-175             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-176             [-1, 28, 1, 1]               0\n",
      "          Conv2d-177            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-178            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-179          [-1, 672, 24, 24]               0\n",
      "          Conv2d-180          [-1, 112, 24, 24]          75,264\n",
      "        Identity-181          [-1, 112, 24, 24]               0\n",
      "        Identity-182          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-183          [-1, 112, 24, 24]             224\n",
      "        Identity-184          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-185          [-1, 112, 24, 24]               0\n",
      "          Conv2d-186          [-1, 672, 24, 24]          75,264\n",
      "        Identity-187          [-1, 672, 24, 24]               0\n",
      "            SiLU-188          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-189          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-190          [-1, 672, 24, 24]          16,800\n",
      "        Identity-191          [-1, 672, 24, 24]               0\n",
      "            SiLU-192          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-193          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-194             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-195             [-1, 28, 1, 1]               0\n",
      "          Conv2d-196            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-197            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-198          [-1, 672, 24, 24]               0\n",
      "          Conv2d-199          [-1, 112, 24, 24]          75,264\n",
      "        Identity-200          [-1, 112, 24, 24]               0\n",
      "        Identity-201          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-202          [-1, 112, 24, 24]             224\n",
      "        Identity-203          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-204          [-1, 112, 24, 24]               0\n",
      "          Conv2d-205          [-1, 672, 24, 24]          75,264\n",
      "        Identity-206          [-1, 672, 24, 24]               0\n",
      "            SiLU-207          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-208          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-209          [-1, 672, 12, 12]          16,800\n",
      "        Identity-210          [-1, 672, 12, 12]               0\n",
      "            SiLU-211          [-1, 672, 12, 12]               0\n",
      "  BatchNormAct2d-212          [-1, 672, 12, 12]           1,344\n",
      "          Conv2d-213             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-214             [-1, 28, 1, 1]               0\n",
      "          Conv2d-215            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-216            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-217          [-1, 672, 12, 12]               0\n",
      "          Conv2d-218          [-1, 192, 12, 12]         129,024\n",
      "        Identity-219          [-1, 192, 12, 12]               0\n",
      "        Identity-220          [-1, 192, 12, 12]               0\n",
      "  BatchNormAct2d-221          [-1, 192, 12, 12]             384\n",
      "InvertedResidual-222          [-1, 192, 12, 12]               0\n",
      "          Conv2d-223         [-1, 1152, 12, 12]         221,184\n",
      "        Identity-224         [-1, 1152, 12, 12]               0\n",
      "            SiLU-225         [-1, 1152, 12, 12]               0\n",
      "  BatchNormAct2d-226         [-1, 1152, 12, 12]           2,304\n",
      "          Conv2d-227         [-1, 1152, 12, 12]          28,800\n",
      "        Identity-228         [-1, 1152, 12, 12]               0\n",
      "            SiLU-229         [-1, 1152, 12, 12]               0\n",
      "  BatchNormAct2d-230         [-1, 1152, 12, 12]           2,304\n",
      "          Conv2d-231             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-232             [-1, 48, 1, 1]               0\n",
      "          Conv2d-233           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-234           [-1, 1152, 1, 1]               0\n",
      "   SqueezeExcite-235         [-1, 1152, 12, 12]               0\n",
      "          Conv2d-236          [-1, 192, 12, 12]         221,184\n",
      "        Identity-237          [-1, 192, 12, 12]               0\n",
      "        Identity-238          [-1, 192, 12, 12]               0\n",
      "  BatchNormAct2d-239          [-1, 192, 12, 12]             384\n",
      "        Identity-240          [-1, 192, 12, 12]               0\n",
      "InvertedResidual-241          [-1, 192, 12, 12]               0\n",
      "          Conv2d-242         [-1, 1152, 12, 12]         221,184\n",
      "        Identity-243         [-1, 1152, 12, 12]               0\n",
      "            SiLU-244         [-1, 1152, 12, 12]               0\n",
      "  BatchNormAct2d-245         [-1, 1152, 12, 12]           2,304\n",
      "          Conv2d-246         [-1, 1152, 12, 12]          28,800\n",
      "        Identity-247         [-1, 1152, 12, 12]               0\n",
      "            SiLU-248         [-1, 1152, 12, 12]               0\n",
      "  BatchNormAct2d-249         [-1, 1152, 12, 12]           2,304\n",
      "          Conv2d-250             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-251             [-1, 48, 1, 1]               0\n",
      "          Conv2d-252           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-253           [-1, 1152, 1, 1]               0\n",
      "   SqueezeExcite-254         [-1, 1152, 12, 12]               0\n",
      "          Conv2d-255          [-1, 192, 12, 12]         221,184\n",
      "        Identity-256          [-1, 192, 12, 12]               0\n",
      "        Identity-257          [-1, 192, 12, 12]               0\n",
      "  BatchNormAct2d-258          [-1, 192, 12, 12]             384\n",
      "        Identity-259          [-1, 192, 12, 12]               0\n",
      "InvertedResidual-260          [-1, 192, 12, 12]               0\n",
      "          Conv2d-261         [-1, 1152, 12, 12]         221,184\n",
      "        Identity-262         [-1, 1152, 12, 12]               0\n",
      "            SiLU-263         [-1, 1152, 12, 12]               0\n",
      "  BatchNormAct2d-264         [-1, 1152, 12, 12]           2,304\n",
      "          Conv2d-265         [-1, 1152, 12, 12]          28,800\n",
      "        Identity-266         [-1, 1152, 12, 12]               0\n",
      "            SiLU-267         [-1, 1152, 12, 12]               0\n",
      "  BatchNormAct2d-268         [-1, 1152, 12, 12]           2,304\n",
      "          Conv2d-269             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-270             [-1, 48, 1, 1]               0\n",
      "          Conv2d-271           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-272           [-1, 1152, 1, 1]               0\n",
      "   SqueezeExcite-273         [-1, 1152, 12, 12]               0\n",
      "          Conv2d-274          [-1, 192, 12, 12]         221,184\n",
      "        Identity-275          [-1, 192, 12, 12]               0\n",
      "        Identity-276          [-1, 192, 12, 12]               0\n",
      "  BatchNormAct2d-277          [-1, 192, 12, 12]             384\n",
      "        Identity-278          [-1, 192, 12, 12]               0\n",
      "InvertedResidual-279          [-1, 192, 12, 12]               0\n",
      "          Conv2d-280         [-1, 1152, 12, 12]         221,184\n",
      "        Identity-281         [-1, 1152, 12, 12]               0\n",
      "            SiLU-282         [-1, 1152, 12, 12]               0\n",
      "  BatchNormAct2d-283         [-1, 1152, 12, 12]           2,304\n",
      "          Conv2d-284         [-1, 1152, 12, 12]          10,368\n",
      "        Identity-285         [-1, 1152, 12, 12]               0\n",
      "            SiLU-286         [-1, 1152, 12, 12]               0\n",
      "  BatchNormAct2d-287         [-1, 1152, 12, 12]           2,304\n",
      "          Conv2d-288             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-289             [-1, 48, 1, 1]               0\n",
      "          Conv2d-290           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-291           [-1, 1152, 1, 1]               0\n",
      "   SqueezeExcite-292         [-1, 1152, 12, 12]               0\n",
      "          Conv2d-293          [-1, 320, 12, 12]         368,640\n",
      "        Identity-294          [-1, 320, 12, 12]               0\n",
      "        Identity-295          [-1, 320, 12, 12]               0\n",
      "  BatchNormAct2d-296          [-1, 320, 12, 12]             640\n",
      "InvertedResidual-297          [-1, 320, 12, 12]               0\n",
      "          Conv2d-298         [-1, 1280, 12, 12]         409,600\n",
      "        Identity-299         [-1, 1280, 12, 12]               0\n",
      "            SiLU-300         [-1, 1280, 12, 12]               0\n",
      "  BatchNormAct2d-301         [-1, 1280, 12, 12]           2,560\n",
      "AdaptiveAvgPool2d-302           [-1, 1280, 1, 1]               0\n",
      "         Flatten-303                 [-1, 1280]               0\n",
      "SelectAdaptivePool2d-304                 [-1, 1280]               0\n",
      "          Linear-305                   [-1, 17]          21,777\n",
      "================================================================\n",
      "Total params: 4,029,325\n",
      "Trainable params: 4,029,325\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.69\n",
      "Forward/backward pass size (MB): 673.87\n",
      "Params size (MB): 15.37\n",
      "Estimated Total Size (MB): 690.93\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Model architecture:\n",
      "EfficientNet(\n",
      "  (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNormAct2d(\n",
      "    32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "    (drop): Identity()\n",
      "    (act): SiLU(inplace=True)\n",
      "  )\n",
      "  (blocks): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): DepthwiseSeparableConv(\n",
      "        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): InvertedResidual(\n",
      "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNormAct2d(\n",
      "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "        (bn2): BatchNormAct2d(\n",
      "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (se): SqueezeExcite(\n",
      "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act1): SiLU(inplace=True)\n",
      "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (gate): Sigmoid()\n",
      "        )\n",
      "        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNormAct2d(\n",
      "          320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (drop): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn2): BatchNormAct2d(\n",
      "    1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "    (drop): Identity()\n",
      "    (act): SiLU(inplace=True)\n",
      "  )\n",
      "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "  (classifier): Linear(in_features=1280, out_features=17, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "    \n",
    "# 모델 구조 출력\n",
    "print(f\"\\nModel structure of {model_name}:\")\n",
    "print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "# 모델 아키텍처 출력\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5717: 100%|██████████| 40/40 [00:05<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.9814, Train Acc: 0.6998, Train F1: 0.6748\n",
      "Val Loss: 0.6068, Val Acc: 0.8248, Val F1: 0.8014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2583: 100%|██████████| 40/40 [00:04<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.3801, Train Acc: 0.8774, Train F1: 0.8633\n",
      "Val Loss: 0.5170, Val Acc: 0.8599, Val F1: 0.8494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2990: 100%|██████████| 40/40 [00:04<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.3013, Train Acc: 0.8854, Train F1: 0.8729\n",
      "Val Loss: 0.3384, Val Acc: 0.8917, Val F1: 0.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1913: 100%|██████████| 40/40 [00:04<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.2596, Train Acc: 0.9005, Train F1: 0.8936\n",
      "Val Loss: 0.2837, Val Acc: 0.9076, Val F1: 0.9004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0928: 100%|██████████| 40/40 [00:04<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.1830, Train Acc: 0.9299, Train F1: 0.9253\n",
      "Val Loss: 0.3366, Val Acc: 0.8981, Val F1: 0.8972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2278: 100%|██████████| 40/40 [00:04<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.2071, Train Acc: 0.9268, Train F1: 0.9223\n",
      "Val Loss: 0.4114, Val Acc: 0.8885, Val F1: 0.8767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2854: 100%|██████████| 40/40 [00:04<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.1562, Train Acc: 0.9530, Train F1: 0.9498\n",
      "Val Loss: 0.3123, Val Acc: 0.9013, Val F1: 0.8946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0753: 100%|██████████| 40/40 [00:04<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.1782, Train Acc: 0.9411, Train F1: 0.9375\n",
      "Val Loss: 0.2348, Val Acc: 0.9363, Val F1: 0.9331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0269: 100%|██████████| 40/40 [00:04<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0971, Train Acc: 0.9658, Train F1: 0.9649\n",
      "Val Loss: 0.3019, Val Acc: 0.9013, Val F1: 0.8947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0446: 100%|██████████| 40/40 [00:04<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0920, Train Acc: 0.9682, Train F1: 0.9656\n",
      "Val Loss: 0.2258, Val Acc: 0.9236, Val F1: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0209: 100%|██████████| 40/40 [00:04<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "Train Loss: 0.0641, Train Acc: 0.9801, Train F1: 0.9809\n",
      "Val Loss: 0.2959, Val Acc: 0.9172, Val F1: 0.9142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0052: 100%|██████████| 40/40 [00:04<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "Train Loss: 0.0577, Train Acc: 0.9809, Train F1: 0.9800\n",
      "Val Loss: 0.2169, Val Acc: 0.9268, Val F1: 0.9249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0020: 100%|██████████| 40/40 [00:04<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "Train Loss: 0.0424, Train Acc: 0.9857, Train F1: 0.9844\n",
      "Val Loss: 0.2254, Val Acc: 0.9459, Val F1: 0.9411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0041: 100%|██████████| 40/40 [00:04<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "Train Loss: 0.0198, Train Acc: 0.9928, Train F1: 0.9922\n",
      "Val Loss: 0.2187, Val Acc: 0.9363, Val F1: 0.9296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009: 100%|██████████| 40/40 [00:04<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "Train Loss: 0.0213, Train Acc: 0.9936, Train F1: 0.9920\n",
      "Val Loss: 0.2422, Val Acc: 0.9395, Val F1: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0615: 100%|██████████| 40/40 [00:04<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "Train Loss: 0.0148, Train Acc: 0.9952, Train F1: 0.9956\n",
      "Val Loss: 0.2497, Val Acc: 0.9363, Val F1: 0.9334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 40/40 [00:04<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "Train Loss: 0.0127, Train Acc: 0.9960, Train F1: 0.9952\n",
      "Val Loss: 0.2342, Val Acc: 0.9522, Val F1: 0.9504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0056: 100%|██████████| 40/40 [00:04<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "Train Loss: 0.0084, Train Acc: 0.9960, Train F1: 0.9959\n",
      "Val Loss: 0.2615, Val Acc: 0.9363, Val F1: 0.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 40/40 [00:04<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "Train Loss: 0.0085, Train Acc: 0.9968, Train F1: 0.9967\n",
      "Val Loss: 0.2679, Val Acc: 0.9395, Val F1: 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0309: 100%|██████████| 40/40 [00:04<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "Train Loss: 0.0111, Train Acc: 0.9968, Train F1: 0.9967\n",
      "Val Loss: 0.2587, Val Acc: 0.9395, Val F1: 0.9378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0079: 100%|██████████| 40/40 [00:04<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "Train Loss: 0.0076, Train Acc: 0.9976, Train F1: 0.9974\n",
      "Val Loss: 0.2453, Val Acc: 0.9490, Val F1: 0.9465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0425: 100%|██████████| 40/40 [00:04<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "Train Loss: 0.0051, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.2542, Val Acc: 0.9490, Val F1: 0.9468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0113: 100%|██████████| 40/40 [00:04<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "Train Loss: 0.0127, Train Acc: 0.9960, Train F1: 0.9963\n",
      "Val Loss: 0.2502, Val Acc: 0.9459, Val F1: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0999: 100%|██████████| 40/40 [00:04<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "Train Loss: 0.0095, Train Acc: 0.9968, Train F1: 0.9964\n",
      "Val Loss: 0.2298, Val Acc: 0.9459, Val F1: 0.9434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0020: 100%|██████████| 40/40 [00:04<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "Train Loss: 0.0046, Train Acc: 0.9984, Train F1: 0.9982\n",
      "Val Loss: 0.2431, Val Acc: 0.9459, Val F1: 0.9462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0016: 100%|██████████| 40/40 [00:04<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "Train Loss: 0.0056, Train Acc: 0.9976, Train F1: 0.9967\n",
      "Val Loss: 0.2382, Val Acc: 0.9427, Val F1: 0.9424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2913: 100%|██████████| 40/40 [00:04<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "Train Loss: 0.0131, Train Acc: 0.9976, Train F1: 0.9971\n",
      "Val Loss: 0.2442, Val Acc: 0.9427, Val F1: 0.9430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 40/40 [00:04<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "Train Loss: 0.0028, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.2432, Val Acc: 0.9427, Val F1: 0.9424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0032: 100%|██████████| 40/40 [00:04<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "Train Loss: 0.0035, Train Acc: 0.9992, Train F1: 0.9993\n",
      "Val Loss: 0.2408, Val Acc: 0.9427, Val F1: 0.9424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 40/40 [00:04<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "Train Loss: 0.0019, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2418, Val Acc: 0.9427, Val F1: 0.9433\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프\n",
    "best_val_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 추론\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet-B4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfd73643f6a4f3b925b965d70165cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/77.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model structure of efficientnet_b4:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 48, 190, 190]           1,296\n",
      "          Identity-2         [-1, 48, 190, 190]               0\n",
      "              SiLU-3         [-1, 48, 190, 190]               0\n",
      "    BatchNormAct2d-4         [-1, 48, 190, 190]              96\n",
      "            Conv2d-5         [-1, 48, 190, 190]             432\n",
      "          Identity-6         [-1, 48, 190, 190]               0\n",
      "              SiLU-7         [-1, 48, 190, 190]               0\n",
      "    BatchNormAct2d-8         [-1, 48, 190, 190]              96\n",
      "            Conv2d-9             [-1, 12, 1, 1]             588\n",
      "             SiLU-10             [-1, 12, 1, 1]               0\n",
      "           Conv2d-11             [-1, 48, 1, 1]             624\n",
      "          Sigmoid-12             [-1, 48, 1, 1]               0\n",
      "    SqueezeExcite-13         [-1, 48, 190, 190]               0\n",
      "           Conv2d-14         [-1, 24, 190, 190]           1,152\n",
      "         Identity-15         [-1, 24, 190, 190]               0\n",
      "         Identity-16         [-1, 24, 190, 190]               0\n",
      "   BatchNormAct2d-17         [-1, 24, 190, 190]              48\n",
      "DepthwiseSeparableConv-18         [-1, 24, 190, 190]               0\n",
      "           Conv2d-19         [-1, 24, 190, 190]             216\n",
      "         Identity-20         [-1, 24, 190, 190]               0\n",
      "             SiLU-21         [-1, 24, 190, 190]               0\n",
      "   BatchNormAct2d-22         [-1, 24, 190, 190]              48\n",
      "           Conv2d-23              [-1, 6, 1, 1]             150\n",
      "             SiLU-24              [-1, 6, 1, 1]               0\n",
      "           Conv2d-25             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-26             [-1, 24, 1, 1]               0\n",
      "    SqueezeExcite-27         [-1, 24, 190, 190]               0\n",
      "           Conv2d-28         [-1, 24, 190, 190]             576\n",
      "         Identity-29         [-1, 24, 190, 190]               0\n",
      "         Identity-30         [-1, 24, 190, 190]               0\n",
      "   BatchNormAct2d-31         [-1, 24, 190, 190]              48\n",
      "         Identity-32         [-1, 24, 190, 190]               0\n",
      "DepthwiseSeparableConv-33         [-1, 24, 190, 190]               0\n",
      "           Conv2d-34        [-1, 144, 190, 190]           3,456\n",
      "         Identity-35        [-1, 144, 190, 190]               0\n",
      "             SiLU-36        [-1, 144, 190, 190]               0\n",
      "   BatchNormAct2d-37        [-1, 144, 190, 190]             288\n",
      "           Conv2d-38          [-1, 144, 95, 95]           1,296\n",
      "         Identity-39          [-1, 144, 95, 95]               0\n",
      "             SiLU-40          [-1, 144, 95, 95]               0\n",
      "   BatchNormAct2d-41          [-1, 144, 95, 95]             288\n",
      "           Conv2d-42              [-1, 6, 1, 1]             870\n",
      "             SiLU-43              [-1, 6, 1, 1]               0\n",
      "           Conv2d-44            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-45            [-1, 144, 1, 1]               0\n",
      "    SqueezeExcite-46          [-1, 144, 95, 95]               0\n",
      "           Conv2d-47           [-1, 32, 95, 95]           4,608\n",
      "         Identity-48           [-1, 32, 95, 95]               0\n",
      "         Identity-49           [-1, 32, 95, 95]               0\n",
      "   BatchNormAct2d-50           [-1, 32, 95, 95]              64\n",
      " InvertedResidual-51           [-1, 32, 95, 95]               0\n",
      "           Conv2d-52          [-1, 192, 95, 95]           6,144\n",
      "         Identity-53          [-1, 192, 95, 95]               0\n",
      "             SiLU-54          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-55          [-1, 192, 95, 95]             384\n",
      "           Conv2d-56          [-1, 192, 95, 95]           1,728\n",
      "         Identity-57          [-1, 192, 95, 95]               0\n",
      "             SiLU-58          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-59          [-1, 192, 95, 95]             384\n",
      "           Conv2d-60              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-61              [-1, 8, 1, 1]               0\n",
      "           Conv2d-62            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-63            [-1, 192, 1, 1]               0\n",
      "    SqueezeExcite-64          [-1, 192, 95, 95]               0\n",
      "           Conv2d-65           [-1, 32, 95, 95]           6,144\n",
      "         Identity-66           [-1, 32, 95, 95]               0\n",
      "         Identity-67           [-1, 32, 95, 95]               0\n",
      "   BatchNormAct2d-68           [-1, 32, 95, 95]              64\n",
      "         Identity-69           [-1, 32, 95, 95]               0\n",
      " InvertedResidual-70           [-1, 32, 95, 95]               0\n",
      "           Conv2d-71          [-1, 192, 95, 95]           6,144\n",
      "         Identity-72          [-1, 192, 95, 95]               0\n",
      "             SiLU-73          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-74          [-1, 192, 95, 95]             384\n",
      "           Conv2d-75          [-1, 192, 95, 95]           1,728\n",
      "         Identity-76          [-1, 192, 95, 95]               0\n",
      "             SiLU-77          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-78          [-1, 192, 95, 95]             384\n",
      "           Conv2d-79              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-80              [-1, 8, 1, 1]               0\n",
      "           Conv2d-81            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-82            [-1, 192, 1, 1]               0\n",
      "    SqueezeExcite-83          [-1, 192, 95, 95]               0\n",
      "           Conv2d-84           [-1, 32, 95, 95]           6,144\n",
      "         Identity-85           [-1, 32, 95, 95]               0\n",
      "         Identity-86           [-1, 32, 95, 95]               0\n",
      "   BatchNormAct2d-87           [-1, 32, 95, 95]              64\n",
      "         Identity-88           [-1, 32, 95, 95]               0\n",
      " InvertedResidual-89           [-1, 32, 95, 95]               0\n",
      "           Conv2d-90          [-1, 192, 95, 95]           6,144\n",
      "         Identity-91          [-1, 192, 95, 95]               0\n",
      "             SiLU-92          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-93          [-1, 192, 95, 95]             384\n",
      "           Conv2d-94          [-1, 192, 95, 95]           1,728\n",
      "         Identity-95          [-1, 192, 95, 95]               0\n",
      "             SiLU-96          [-1, 192, 95, 95]               0\n",
      "   BatchNormAct2d-97          [-1, 192, 95, 95]             384\n",
      "           Conv2d-98              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-99              [-1, 8, 1, 1]               0\n",
      "          Conv2d-100            [-1, 192, 1, 1]           1,728\n",
      "         Sigmoid-101            [-1, 192, 1, 1]               0\n",
      "   SqueezeExcite-102          [-1, 192, 95, 95]               0\n",
      "          Conv2d-103           [-1, 32, 95, 95]           6,144\n",
      "        Identity-104           [-1, 32, 95, 95]               0\n",
      "        Identity-105           [-1, 32, 95, 95]               0\n",
      "  BatchNormAct2d-106           [-1, 32, 95, 95]              64\n",
      "        Identity-107           [-1, 32, 95, 95]               0\n",
      "InvertedResidual-108           [-1, 32, 95, 95]               0\n",
      "          Conv2d-109          [-1, 192, 95, 95]           6,144\n",
      "        Identity-110          [-1, 192, 95, 95]               0\n",
      "            SiLU-111          [-1, 192, 95, 95]               0\n",
      "  BatchNormAct2d-112          [-1, 192, 95, 95]             384\n",
      "          Conv2d-113          [-1, 192, 48, 48]           4,800\n",
      "        Identity-114          [-1, 192, 48, 48]               0\n",
      "            SiLU-115          [-1, 192, 48, 48]               0\n",
      "  BatchNormAct2d-116          [-1, 192, 48, 48]             384\n",
      "          Conv2d-117              [-1, 8, 1, 1]           1,544\n",
      "            SiLU-118              [-1, 8, 1, 1]               0\n",
      "          Conv2d-119            [-1, 192, 1, 1]           1,728\n",
      "         Sigmoid-120            [-1, 192, 1, 1]               0\n",
      "   SqueezeExcite-121          [-1, 192, 48, 48]               0\n",
      "          Conv2d-122           [-1, 56, 48, 48]          10,752\n",
      "        Identity-123           [-1, 56, 48, 48]               0\n",
      "        Identity-124           [-1, 56, 48, 48]               0\n",
      "  BatchNormAct2d-125           [-1, 56, 48, 48]             112\n",
      "InvertedResidual-126           [-1, 56, 48, 48]               0\n",
      "          Conv2d-127          [-1, 336, 48, 48]          18,816\n",
      "        Identity-128          [-1, 336, 48, 48]               0\n",
      "            SiLU-129          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-130          [-1, 336, 48, 48]             672\n",
      "          Conv2d-131          [-1, 336, 48, 48]           8,400\n",
      "        Identity-132          [-1, 336, 48, 48]               0\n",
      "            SiLU-133          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-134          [-1, 336, 48, 48]             672\n",
      "          Conv2d-135             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-136             [-1, 14, 1, 1]               0\n",
      "          Conv2d-137            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-138            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-139          [-1, 336, 48, 48]               0\n",
      "          Conv2d-140           [-1, 56, 48, 48]          18,816\n",
      "        Identity-141           [-1, 56, 48, 48]               0\n",
      "        Identity-142           [-1, 56, 48, 48]               0\n",
      "  BatchNormAct2d-143           [-1, 56, 48, 48]             112\n",
      "        Identity-144           [-1, 56, 48, 48]               0\n",
      "InvertedResidual-145           [-1, 56, 48, 48]               0\n",
      "          Conv2d-146          [-1, 336, 48, 48]          18,816\n",
      "        Identity-147          [-1, 336, 48, 48]               0\n",
      "            SiLU-148          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-149          [-1, 336, 48, 48]             672\n",
      "          Conv2d-150          [-1, 336, 48, 48]           8,400\n",
      "        Identity-151          [-1, 336, 48, 48]               0\n",
      "            SiLU-152          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-153          [-1, 336, 48, 48]             672\n",
      "          Conv2d-154             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-155             [-1, 14, 1, 1]               0\n",
      "          Conv2d-156            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-157            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-158          [-1, 336, 48, 48]               0\n",
      "          Conv2d-159           [-1, 56, 48, 48]          18,816\n",
      "        Identity-160           [-1, 56, 48, 48]               0\n",
      "        Identity-161           [-1, 56, 48, 48]               0\n",
      "  BatchNormAct2d-162           [-1, 56, 48, 48]             112\n",
      "        Identity-163           [-1, 56, 48, 48]               0\n",
      "InvertedResidual-164           [-1, 56, 48, 48]               0\n",
      "          Conv2d-165          [-1, 336, 48, 48]          18,816\n",
      "        Identity-166          [-1, 336, 48, 48]               0\n",
      "            SiLU-167          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-168          [-1, 336, 48, 48]             672\n",
      "          Conv2d-169          [-1, 336, 48, 48]           8,400\n",
      "        Identity-170          [-1, 336, 48, 48]               0\n",
      "            SiLU-171          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-172          [-1, 336, 48, 48]             672\n",
      "          Conv2d-173             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-174             [-1, 14, 1, 1]               0\n",
      "          Conv2d-175            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-176            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-177          [-1, 336, 48, 48]               0\n",
      "          Conv2d-178           [-1, 56, 48, 48]          18,816\n",
      "        Identity-179           [-1, 56, 48, 48]               0\n",
      "        Identity-180           [-1, 56, 48, 48]               0\n",
      "  BatchNormAct2d-181           [-1, 56, 48, 48]             112\n",
      "        Identity-182           [-1, 56, 48, 48]               0\n",
      "InvertedResidual-183           [-1, 56, 48, 48]               0\n",
      "          Conv2d-184          [-1, 336, 48, 48]          18,816\n",
      "        Identity-185          [-1, 336, 48, 48]               0\n",
      "            SiLU-186          [-1, 336, 48, 48]               0\n",
      "  BatchNormAct2d-187          [-1, 336, 48, 48]             672\n",
      "          Conv2d-188          [-1, 336, 24, 24]           3,024\n",
      "        Identity-189          [-1, 336, 24, 24]               0\n",
      "            SiLU-190          [-1, 336, 24, 24]               0\n",
      "  BatchNormAct2d-191          [-1, 336, 24, 24]             672\n",
      "          Conv2d-192             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-193             [-1, 14, 1, 1]               0\n",
      "          Conv2d-194            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-195            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-196          [-1, 336, 24, 24]               0\n",
      "          Conv2d-197          [-1, 112, 24, 24]          37,632\n",
      "        Identity-198          [-1, 112, 24, 24]               0\n",
      "        Identity-199          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-200          [-1, 112, 24, 24]             224\n",
      "InvertedResidual-201          [-1, 112, 24, 24]               0\n",
      "          Conv2d-202          [-1, 672, 24, 24]          75,264\n",
      "        Identity-203          [-1, 672, 24, 24]               0\n",
      "            SiLU-204          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-205          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-206          [-1, 672, 24, 24]           6,048\n",
      "        Identity-207          [-1, 672, 24, 24]               0\n",
      "            SiLU-208          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-209          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-210             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-211             [-1, 28, 1, 1]               0\n",
      "          Conv2d-212            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-213            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-214          [-1, 672, 24, 24]               0\n",
      "          Conv2d-215          [-1, 112, 24, 24]          75,264\n",
      "        Identity-216          [-1, 112, 24, 24]               0\n",
      "        Identity-217          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-218          [-1, 112, 24, 24]             224\n",
      "        Identity-219          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-220          [-1, 112, 24, 24]               0\n",
      "          Conv2d-221          [-1, 672, 24, 24]          75,264\n",
      "        Identity-222          [-1, 672, 24, 24]               0\n",
      "            SiLU-223          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-224          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-225          [-1, 672, 24, 24]           6,048\n",
      "        Identity-226          [-1, 672, 24, 24]               0\n",
      "            SiLU-227          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-228          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-229             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-230             [-1, 28, 1, 1]               0\n",
      "          Conv2d-231            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-232            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-233          [-1, 672, 24, 24]               0\n",
      "          Conv2d-234          [-1, 112, 24, 24]          75,264\n",
      "        Identity-235          [-1, 112, 24, 24]               0\n",
      "        Identity-236          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-237          [-1, 112, 24, 24]             224\n",
      "        Identity-238          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-239          [-1, 112, 24, 24]               0\n",
      "          Conv2d-240          [-1, 672, 24, 24]          75,264\n",
      "        Identity-241          [-1, 672, 24, 24]               0\n",
      "            SiLU-242          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-243          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-244          [-1, 672, 24, 24]           6,048\n",
      "        Identity-245          [-1, 672, 24, 24]               0\n",
      "            SiLU-246          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-247          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-248             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-249             [-1, 28, 1, 1]               0\n",
      "          Conv2d-250            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-251            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-252          [-1, 672, 24, 24]               0\n",
      "          Conv2d-253          [-1, 112, 24, 24]          75,264\n",
      "        Identity-254          [-1, 112, 24, 24]               0\n",
      "        Identity-255          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-256          [-1, 112, 24, 24]             224\n",
      "        Identity-257          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-258          [-1, 112, 24, 24]               0\n",
      "          Conv2d-259          [-1, 672, 24, 24]          75,264\n",
      "        Identity-260          [-1, 672, 24, 24]               0\n",
      "            SiLU-261          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-262          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-263          [-1, 672, 24, 24]           6,048\n",
      "        Identity-264          [-1, 672, 24, 24]               0\n",
      "            SiLU-265          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-266          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-267             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-268             [-1, 28, 1, 1]               0\n",
      "          Conv2d-269            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-270            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-271          [-1, 672, 24, 24]               0\n",
      "          Conv2d-272          [-1, 112, 24, 24]          75,264\n",
      "        Identity-273          [-1, 112, 24, 24]               0\n",
      "        Identity-274          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-275          [-1, 112, 24, 24]             224\n",
      "        Identity-276          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-277          [-1, 112, 24, 24]               0\n",
      "          Conv2d-278          [-1, 672, 24, 24]          75,264\n",
      "        Identity-279          [-1, 672, 24, 24]               0\n",
      "            SiLU-280          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-281          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-282          [-1, 672, 24, 24]           6,048\n",
      "        Identity-283          [-1, 672, 24, 24]               0\n",
      "            SiLU-284          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-285          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-286             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-287             [-1, 28, 1, 1]               0\n",
      "          Conv2d-288            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-289            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-290          [-1, 672, 24, 24]               0\n",
      "          Conv2d-291          [-1, 112, 24, 24]          75,264\n",
      "        Identity-292          [-1, 112, 24, 24]               0\n",
      "        Identity-293          [-1, 112, 24, 24]               0\n",
      "  BatchNormAct2d-294          [-1, 112, 24, 24]             224\n",
      "        Identity-295          [-1, 112, 24, 24]               0\n",
      "InvertedResidual-296          [-1, 112, 24, 24]               0\n",
      "          Conv2d-297          [-1, 672, 24, 24]          75,264\n",
      "        Identity-298          [-1, 672, 24, 24]               0\n",
      "            SiLU-299          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-300          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-301          [-1, 672, 24, 24]          16,800\n",
      "        Identity-302          [-1, 672, 24, 24]               0\n",
      "            SiLU-303          [-1, 672, 24, 24]               0\n",
      "  BatchNormAct2d-304          [-1, 672, 24, 24]           1,344\n",
      "          Conv2d-305             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-306             [-1, 28, 1, 1]               0\n",
      "          Conv2d-307            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-308            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-309          [-1, 672, 24, 24]               0\n",
      "          Conv2d-310          [-1, 160, 24, 24]         107,520\n",
      "        Identity-311          [-1, 160, 24, 24]               0\n",
      "        Identity-312          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-313          [-1, 160, 24, 24]             320\n",
      "InvertedResidual-314          [-1, 160, 24, 24]               0\n",
      "          Conv2d-315          [-1, 960, 24, 24]         153,600\n",
      "        Identity-316          [-1, 960, 24, 24]               0\n",
      "            SiLU-317          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-318          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-319          [-1, 960, 24, 24]          24,000\n",
      "        Identity-320          [-1, 960, 24, 24]               0\n",
      "            SiLU-321          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-322          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-323             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-324             [-1, 40, 1, 1]               0\n",
      "          Conv2d-325            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-326            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-327          [-1, 960, 24, 24]               0\n",
      "          Conv2d-328          [-1, 160, 24, 24]         153,600\n",
      "        Identity-329          [-1, 160, 24, 24]               0\n",
      "        Identity-330          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-331          [-1, 160, 24, 24]             320\n",
      "        Identity-332          [-1, 160, 24, 24]               0\n",
      "InvertedResidual-333          [-1, 160, 24, 24]               0\n",
      "          Conv2d-334          [-1, 960, 24, 24]         153,600\n",
      "        Identity-335          [-1, 960, 24, 24]               0\n",
      "            SiLU-336          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-337          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-338          [-1, 960, 24, 24]          24,000\n",
      "        Identity-339          [-1, 960, 24, 24]               0\n",
      "            SiLU-340          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-341          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-342             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-343             [-1, 40, 1, 1]               0\n",
      "          Conv2d-344            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-345            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-346          [-1, 960, 24, 24]               0\n",
      "          Conv2d-347          [-1, 160, 24, 24]         153,600\n",
      "        Identity-348          [-1, 160, 24, 24]               0\n",
      "        Identity-349          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-350          [-1, 160, 24, 24]             320\n",
      "        Identity-351          [-1, 160, 24, 24]               0\n",
      "InvertedResidual-352          [-1, 160, 24, 24]               0\n",
      "          Conv2d-353          [-1, 960, 24, 24]         153,600\n",
      "        Identity-354          [-1, 960, 24, 24]               0\n",
      "            SiLU-355          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-356          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-357          [-1, 960, 24, 24]          24,000\n",
      "        Identity-358          [-1, 960, 24, 24]               0\n",
      "            SiLU-359          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-360          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-361             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-362             [-1, 40, 1, 1]               0\n",
      "          Conv2d-363            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-364            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-365          [-1, 960, 24, 24]               0\n",
      "          Conv2d-366          [-1, 160, 24, 24]         153,600\n",
      "        Identity-367          [-1, 160, 24, 24]               0\n",
      "        Identity-368          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-369          [-1, 160, 24, 24]             320\n",
      "        Identity-370          [-1, 160, 24, 24]               0\n",
      "InvertedResidual-371          [-1, 160, 24, 24]               0\n",
      "          Conv2d-372          [-1, 960, 24, 24]         153,600\n",
      "        Identity-373          [-1, 960, 24, 24]               0\n",
      "            SiLU-374          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-375          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-376          [-1, 960, 24, 24]          24,000\n",
      "        Identity-377          [-1, 960, 24, 24]               0\n",
      "            SiLU-378          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-379          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-380             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-381             [-1, 40, 1, 1]               0\n",
      "          Conv2d-382            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-383            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-384          [-1, 960, 24, 24]               0\n",
      "          Conv2d-385          [-1, 160, 24, 24]         153,600\n",
      "        Identity-386          [-1, 160, 24, 24]               0\n",
      "        Identity-387          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-388          [-1, 160, 24, 24]             320\n",
      "        Identity-389          [-1, 160, 24, 24]               0\n",
      "InvertedResidual-390          [-1, 160, 24, 24]               0\n",
      "          Conv2d-391          [-1, 960, 24, 24]         153,600\n",
      "        Identity-392          [-1, 960, 24, 24]               0\n",
      "            SiLU-393          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-394          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-395          [-1, 960, 24, 24]          24,000\n",
      "        Identity-396          [-1, 960, 24, 24]               0\n",
      "            SiLU-397          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-398          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-399             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-400             [-1, 40, 1, 1]               0\n",
      "          Conv2d-401            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-402            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-403          [-1, 960, 24, 24]               0\n",
      "          Conv2d-404          [-1, 160, 24, 24]         153,600\n",
      "        Identity-405          [-1, 160, 24, 24]               0\n",
      "        Identity-406          [-1, 160, 24, 24]               0\n",
      "  BatchNormAct2d-407          [-1, 160, 24, 24]             320\n",
      "        Identity-408          [-1, 160, 24, 24]               0\n",
      "InvertedResidual-409          [-1, 160, 24, 24]               0\n",
      "          Conv2d-410          [-1, 960, 24, 24]         153,600\n",
      "        Identity-411          [-1, 960, 24, 24]               0\n",
      "            SiLU-412          [-1, 960, 24, 24]               0\n",
      "  BatchNormAct2d-413          [-1, 960, 24, 24]           1,920\n",
      "          Conv2d-414          [-1, 960, 12, 12]          24,000\n",
      "        Identity-415          [-1, 960, 12, 12]               0\n",
      "            SiLU-416          [-1, 960, 12, 12]               0\n",
      "  BatchNormAct2d-417          [-1, 960, 12, 12]           1,920\n",
      "          Conv2d-418             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-419             [-1, 40, 1, 1]               0\n",
      "          Conv2d-420            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-421            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-422          [-1, 960, 12, 12]               0\n",
      "          Conv2d-423          [-1, 272, 12, 12]         261,120\n",
      "        Identity-424          [-1, 272, 12, 12]               0\n",
      "        Identity-425          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-426          [-1, 272, 12, 12]             544\n",
      "InvertedResidual-427          [-1, 272, 12, 12]               0\n",
      "          Conv2d-428         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-429         [-1, 1632, 12, 12]               0\n",
      "            SiLU-430         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-431         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-432         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-433         [-1, 1632, 12, 12]               0\n",
      "            SiLU-434         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-435         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-436             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-437             [-1, 68, 1, 1]               0\n",
      "          Conv2d-438           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-439           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-440         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-441          [-1, 272, 12, 12]         443,904\n",
      "        Identity-442          [-1, 272, 12, 12]               0\n",
      "        Identity-443          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-444          [-1, 272, 12, 12]             544\n",
      "        Identity-445          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-446          [-1, 272, 12, 12]               0\n",
      "          Conv2d-447         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-448         [-1, 1632, 12, 12]               0\n",
      "            SiLU-449         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-450         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-451         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-452         [-1, 1632, 12, 12]               0\n",
      "            SiLU-453         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-454         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-455             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-456             [-1, 68, 1, 1]               0\n",
      "          Conv2d-457           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-458           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-459         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-460          [-1, 272, 12, 12]         443,904\n",
      "        Identity-461          [-1, 272, 12, 12]               0\n",
      "        Identity-462          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-463          [-1, 272, 12, 12]             544\n",
      "        Identity-464          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-465          [-1, 272, 12, 12]               0\n",
      "          Conv2d-466         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-467         [-1, 1632, 12, 12]               0\n",
      "            SiLU-468         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-469         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-470         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-471         [-1, 1632, 12, 12]               0\n",
      "            SiLU-472         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-473         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-474             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-475             [-1, 68, 1, 1]               0\n",
      "          Conv2d-476           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-477           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-478         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-479          [-1, 272, 12, 12]         443,904\n",
      "        Identity-480          [-1, 272, 12, 12]               0\n",
      "        Identity-481          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-482          [-1, 272, 12, 12]             544\n",
      "        Identity-483          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-484          [-1, 272, 12, 12]               0\n",
      "          Conv2d-485         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-486         [-1, 1632, 12, 12]               0\n",
      "            SiLU-487         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-488         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-489         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-490         [-1, 1632, 12, 12]               0\n",
      "            SiLU-491         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-492         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-493             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-494             [-1, 68, 1, 1]               0\n",
      "          Conv2d-495           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-496           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-497         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-498          [-1, 272, 12, 12]         443,904\n",
      "        Identity-499          [-1, 272, 12, 12]               0\n",
      "        Identity-500          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-501          [-1, 272, 12, 12]             544\n",
      "        Identity-502          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-503          [-1, 272, 12, 12]               0\n",
      "          Conv2d-504         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-505         [-1, 1632, 12, 12]               0\n",
      "            SiLU-506         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-507         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-508         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-509         [-1, 1632, 12, 12]               0\n",
      "            SiLU-510         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-511         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-512             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-513             [-1, 68, 1, 1]               0\n",
      "          Conv2d-514           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-515           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-516         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-517          [-1, 272, 12, 12]         443,904\n",
      "        Identity-518          [-1, 272, 12, 12]               0\n",
      "        Identity-519          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-520          [-1, 272, 12, 12]             544\n",
      "        Identity-521          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-522          [-1, 272, 12, 12]               0\n",
      "          Conv2d-523         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-524         [-1, 1632, 12, 12]               0\n",
      "            SiLU-525         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-526         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-527         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-528         [-1, 1632, 12, 12]               0\n",
      "            SiLU-529         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-530         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-531             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-532             [-1, 68, 1, 1]               0\n",
      "          Conv2d-533           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-534           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-535         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-536          [-1, 272, 12, 12]         443,904\n",
      "        Identity-537          [-1, 272, 12, 12]               0\n",
      "        Identity-538          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-539          [-1, 272, 12, 12]             544\n",
      "        Identity-540          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-541          [-1, 272, 12, 12]               0\n",
      "          Conv2d-542         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-543         [-1, 1632, 12, 12]               0\n",
      "            SiLU-544         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-545         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-546         [-1, 1632, 12, 12]          40,800\n",
      "        Identity-547         [-1, 1632, 12, 12]               0\n",
      "            SiLU-548         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-549         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-550             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-551             [-1, 68, 1, 1]               0\n",
      "          Conv2d-552           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-553           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-554         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-555          [-1, 272, 12, 12]         443,904\n",
      "        Identity-556          [-1, 272, 12, 12]               0\n",
      "        Identity-557          [-1, 272, 12, 12]               0\n",
      "  BatchNormAct2d-558          [-1, 272, 12, 12]             544\n",
      "        Identity-559          [-1, 272, 12, 12]               0\n",
      "InvertedResidual-560          [-1, 272, 12, 12]               0\n",
      "          Conv2d-561         [-1, 1632, 12, 12]         443,904\n",
      "        Identity-562         [-1, 1632, 12, 12]               0\n",
      "            SiLU-563         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-564         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-565         [-1, 1632, 12, 12]          14,688\n",
      "        Identity-566         [-1, 1632, 12, 12]               0\n",
      "            SiLU-567         [-1, 1632, 12, 12]               0\n",
      "  BatchNormAct2d-568         [-1, 1632, 12, 12]           3,264\n",
      "          Conv2d-569             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-570             [-1, 68, 1, 1]               0\n",
      "          Conv2d-571           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-572           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-573         [-1, 1632, 12, 12]               0\n",
      "          Conv2d-574          [-1, 448, 12, 12]         731,136\n",
      "        Identity-575          [-1, 448, 12, 12]               0\n",
      "        Identity-576          [-1, 448, 12, 12]               0\n",
      "  BatchNormAct2d-577          [-1, 448, 12, 12]             896\n",
      "InvertedResidual-578          [-1, 448, 12, 12]               0\n",
      "          Conv2d-579         [-1, 2688, 12, 12]       1,204,224\n",
      "        Identity-580         [-1, 2688, 12, 12]               0\n",
      "            SiLU-581         [-1, 2688, 12, 12]               0\n",
      "  BatchNormAct2d-582         [-1, 2688, 12, 12]           5,376\n",
      "          Conv2d-583         [-1, 2688, 12, 12]          24,192\n",
      "        Identity-584         [-1, 2688, 12, 12]               0\n",
      "            SiLU-585         [-1, 2688, 12, 12]               0\n",
      "  BatchNormAct2d-586         [-1, 2688, 12, 12]           5,376\n",
      "          Conv2d-587            [-1, 112, 1, 1]         301,168\n",
      "            SiLU-588            [-1, 112, 1, 1]               0\n",
      "          Conv2d-589           [-1, 2688, 1, 1]         303,744\n",
      "         Sigmoid-590           [-1, 2688, 1, 1]               0\n",
      "   SqueezeExcite-591         [-1, 2688, 12, 12]               0\n",
      "          Conv2d-592          [-1, 448, 12, 12]       1,204,224\n",
      "        Identity-593          [-1, 448, 12, 12]               0\n",
      "        Identity-594          [-1, 448, 12, 12]               0\n",
      "  BatchNormAct2d-595          [-1, 448, 12, 12]             896\n",
      "        Identity-596          [-1, 448, 12, 12]               0\n",
      "InvertedResidual-597          [-1, 448, 12, 12]               0\n",
      "          Conv2d-598         [-1, 1792, 12, 12]         802,816\n",
      "        Identity-599         [-1, 1792, 12, 12]               0\n",
      "            SiLU-600         [-1, 1792, 12, 12]               0\n",
      "  BatchNormAct2d-601         [-1, 1792, 12, 12]           3,584\n",
      "AdaptiveAvgPool2d-602           [-1, 1792, 1, 1]               0\n",
      "         Flatten-603                 [-1, 1792]               0\n",
      "SelectAdaptivePool2d-604                 [-1, 1792]               0\n",
      "          Linear-605                   [-1, 17]          30,481\n",
      "================================================================\n",
      "Total params: 17,579,097\n",
      "Trainable params: 17,579,097\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.65\n",
      "Forward/backward pass size (MB): 1720.25\n",
      "Params size (MB): 67.06\n",
      "Estimated Total Size (MB): 1788.96\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3497: 100%|██████████| 79/79 [00:13<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.1318, Train Acc: 0.6553, Train F1: 0.6248\n",
      "Val Loss: 0.4453, Val Acc: 0.8567, Val F1: 0.7981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.3697: 100%|██████████| 79/79 [00:12<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.3227, Train Acc: 0.8941, Train F1: 0.8863\n",
      "Val Loss: 0.2545, Val Acc: 0.9076, Val F1: 0.9035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0292: 100%|██████████| 79/79 [00:12<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.2107, Train Acc: 0.9275, Train F1: 0.9217\n",
      "Val Loss: 0.2675, Val Acc: 0.8981, Val F1: 0.8948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1284: 100%|██████████| 79/79 [00:12<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1355, Train Acc: 0.9538, Train F1: 0.9504\n",
      "Val Loss: 0.2736, Val Acc: 0.8949, Val F1: 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0188: 100%|██████████| 79/79 [00:12<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.1085, Train Acc: 0.9594, Train F1: 0.9569\n",
      "Val Loss: 0.3185, Val Acc: 0.9045, Val F1: 0.8976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1565: 100%|██████████| 79/79 [00:12<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.1098, Train Acc: 0.9650, Train F1: 0.9619\n",
      "Val Loss: 0.2225, Val Acc: 0.9299, Val F1: 0.9259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2154: 100%|██████████| 79/79 [00:12<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0748, Train Acc: 0.9753, Train F1: 0.9743\n",
      "Val Loss: 0.2327, Val Acc: 0.9204, Val F1: 0.9157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0337: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0514, Train Acc: 0.9769, Train F1: 0.9762\n",
      "Val Loss: 0.2508, Val Acc: 0.9140, Val F1: 0.9106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0258: 100%|██████████| 79/79 [00:12<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0599, Train Acc: 0.9801, Train F1: 0.9805\n",
      "Val Loss: 0.2796, Val Acc: 0.9172, Val F1: 0.9117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0517: 100%|██████████| 79/79 [00:12<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0374, Train Acc: 0.9904, Train F1: 0.9895\n",
      "Val Loss: 0.2305, Val Acc: 0.9459, Val F1: 0.9446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0350: 100%|██████████| 79/79 [00:12<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "Train Loss: 0.0282, Train Acc: 0.9896, Train F1: 0.9888\n",
      "Val Loss: 0.2369, Val Acc: 0.9299, Val F1: 0.9262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0249: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "Train Loss: 0.0381, Train Acc: 0.9881, Train F1: 0.9865\n",
      "Val Loss: 0.3124, Val Acc: 0.9140, Val F1: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2908: 100%|██████████| 79/79 [00:12<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "Train Loss: 0.0244, Train Acc: 0.9920, Train F1: 0.9914\n",
      "Val Loss: 0.2506, Val Acc: 0.9363, Val F1: 0.9304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009: 100%|██████████| 79/79 [00:12<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "Train Loss: 0.0287, Train Acc: 0.9912, Train F1: 0.9915\n",
      "Val Loss: 0.2410, Val Acc: 0.9204, Val F1: 0.9136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "Train Loss: 0.0075, Train Acc: 0.9976, Train F1: 0.9975\n",
      "Val Loss: 0.2818, Val Acc: 0.9363, Val F1: 0.9292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 79/79 [00:12<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "Train Loss: 0.0065, Train Acc: 0.9976, Train F1: 0.9978\n",
      "Val Loss: 0.2787, Val Acc: 0.9363, Val F1: 0.9323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0670: 100%|██████████| 79/79 [00:12<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "Train Loss: 0.0107, Train Acc: 0.9968, Train F1: 0.9966\n",
      "Val Loss: 0.3376, Val Acc: 0.9331, Val F1: 0.9271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0272: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "Train Loss: 0.0241, Train Acc: 0.9912, Train F1: 0.9901\n",
      "Val Loss: 0.2828, Val Acc: 0.9299, Val F1: 0.9257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "Train Loss: 0.0138, Train Acc: 0.9960, Train F1: 0.9960\n",
      "Val Loss: 0.3082, Val Acc: 0.9395, Val F1: 0.9339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0074: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "Train Loss: 0.0074, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.2741, Val Acc: 0.9331, Val F1: 0.9296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0008: 100%|██████████| 79/79 [00:12<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "Train Loss: 0.0034, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.2727, Val Acc: 0.9268, Val F1: 0.9207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0008: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "Train Loss: 0.0071, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.2840, Val Acc: 0.9331, Val F1: 0.9279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0016: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "Train Loss: 0.0050, Train Acc: 0.9992, Train F1: 0.9989\n",
      "Val Loss: 0.3020, Val Acc: 0.9268, Val F1: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0082: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "Train Loss: 0.0033, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2958, Val Acc: 0.9331, Val F1: 0.9270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 79/79 [00:12<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "Train Loss: 0.0017, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.3029, Val Acc: 0.9268, Val F1: 0.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "Train Loss: 0.0036, Train Acc: 0.9984, Train F1: 0.9978\n",
      "Val Loss: 0.2988, Val Acc: 0.9299, Val F1: 0.9214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0721: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "Train Loss: 0.0021, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2997, Val Acc: 0.9331, Val F1: 0.9265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 79/79 [00:12<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "Train Loss: 0.0039, Train Acc: 0.9992, Train F1: 0.9993\n",
      "Val Loss: 0.3103, Val Acc: 0.9299, Val F1: 0.9255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 79/79 [00:12<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "Train Loss: 0.0018, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.3224, Val Acc: 0.9268, Val F1: 0.9205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0012: 100%|██████████| 79/79 [00:12<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "Train Loss: 0.0019, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.3243, Val Acc: 0.9299, Val F1: 0.9229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:08<00:00, 22.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((380, 380, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'efficientnet_b4'\n",
    "    img_size = 380  # EfficientNet-B4에 적합한 이미지 크기\n",
    "    LR = 5e-4  # 학습률 조정\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 16  # 배치 크기 감소\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNext V2 Large 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9863d94ffa49dead19103ff68d3ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/792M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model structure of convnextv2_large:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 192, 56, 56]           9,408\n",
      "       LayerNorm2d-2          [-1, 192, 56, 56]             384\n",
      "          Identity-3          [-1, 192, 56, 56]               0\n",
      "            Conv2d-4          [-1, 192, 56, 56]           9,600\n",
      "         LayerNorm-5          [-1, 56, 56, 192]             384\n",
      "            Linear-6          [-1, 56, 56, 768]         148,224\n",
      "              GELU-7          [-1, 56, 56, 768]               0\n",
      "           Dropout-8          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-9          [-1, 56, 56, 768]           1,536\n",
      "           Linear-10          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-11          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-12          [-1, 56, 56, 192]               0\n",
      "         Identity-13          [-1, 192, 56, 56]               0\n",
      "         Identity-14          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-15          [-1, 192, 56, 56]               0\n",
      "           Conv2d-16          [-1, 192, 56, 56]           9,600\n",
      "        LayerNorm-17          [-1, 56, 56, 192]             384\n",
      "           Linear-18          [-1, 56, 56, 768]         148,224\n",
      "             GELU-19          [-1, 56, 56, 768]               0\n",
      "          Dropout-20          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-21          [-1, 56, 56, 768]           1,536\n",
      "           Linear-22          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-23          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-24          [-1, 56, 56, 192]               0\n",
      "         Identity-25          [-1, 192, 56, 56]               0\n",
      "         Identity-26          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-27          [-1, 192, 56, 56]               0\n",
      "           Conv2d-28          [-1, 192, 56, 56]           9,600\n",
      "        LayerNorm-29          [-1, 56, 56, 192]             384\n",
      "           Linear-30          [-1, 56, 56, 768]         148,224\n",
      "             GELU-31          [-1, 56, 56, 768]               0\n",
      "          Dropout-32          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-33          [-1, 56, 56, 768]           1,536\n",
      "           Linear-34          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-35          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-36          [-1, 56, 56, 192]               0\n",
      "         Identity-37          [-1, 192, 56, 56]               0\n",
      "         Identity-38          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-39          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtStage-40          [-1, 192, 56, 56]               0\n",
      "      LayerNorm2d-41          [-1, 192, 56, 56]             384\n",
      "           Conv2d-42          [-1, 384, 28, 28]         295,296\n",
      "           Conv2d-43          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-44          [-1, 28, 28, 384]             768\n",
      "           Linear-45         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-46         [-1, 28, 28, 1536]               0\n",
      "          Dropout-47         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-48         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-49          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-50          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-51          [-1, 28, 28, 384]               0\n",
      "         Identity-52          [-1, 384, 28, 28]               0\n",
      "         Identity-53          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-54          [-1, 384, 28, 28]               0\n",
      "           Conv2d-55          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-56          [-1, 28, 28, 384]             768\n",
      "           Linear-57         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-58         [-1, 28, 28, 1536]               0\n",
      "          Dropout-59         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-60         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-61          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-62          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-63          [-1, 28, 28, 384]               0\n",
      "         Identity-64          [-1, 384, 28, 28]               0\n",
      "         Identity-65          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-66          [-1, 384, 28, 28]               0\n",
      "           Conv2d-67          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-68          [-1, 28, 28, 384]             768\n",
      "           Linear-69         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-70         [-1, 28, 28, 1536]               0\n",
      "          Dropout-71         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-72         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-73          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-74          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-75          [-1, 28, 28, 384]               0\n",
      "         Identity-76          [-1, 384, 28, 28]               0\n",
      "         Identity-77          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-78          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtStage-79          [-1, 384, 28, 28]               0\n",
      "      LayerNorm2d-80          [-1, 384, 28, 28]             768\n",
      "           Conv2d-81          [-1, 768, 14, 14]       1,180,416\n",
      "           Conv2d-82          [-1, 768, 14, 14]          38,400\n",
      "        LayerNorm-83          [-1, 14, 14, 768]           1,536\n",
      "           Linear-84         [-1, 14, 14, 3072]       2,362,368\n",
      "             GELU-85         [-1, 14, 14, 3072]               0\n",
      "          Dropout-86         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-87         [-1, 14, 14, 3072]           6,144\n",
      "           Linear-88          [-1, 14, 14, 768]       2,360,064\n",
      "          Dropout-89          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-90          [-1, 14, 14, 768]               0\n",
      "         Identity-91          [-1, 768, 14, 14]               0\n",
      "         Identity-92          [-1, 768, 14, 14]               0\n",
      "    ConvNeXtBlock-93          [-1, 768, 14, 14]               0\n",
      "           Conv2d-94          [-1, 768, 14, 14]          38,400\n",
      "        LayerNorm-95          [-1, 14, 14, 768]           1,536\n",
      "           Linear-96         [-1, 14, 14, 3072]       2,362,368\n",
      "             GELU-97         [-1, 14, 14, 3072]               0\n",
      "          Dropout-98         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-99         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-100          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-101          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-102          [-1, 14, 14, 768]               0\n",
      "        Identity-103          [-1, 768, 14, 14]               0\n",
      "        Identity-104          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-105          [-1, 768, 14, 14]               0\n",
      "          Conv2d-106          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-107          [-1, 14, 14, 768]           1,536\n",
      "          Linear-108         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-109         [-1, 14, 14, 3072]               0\n",
      "         Dropout-110         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-111         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-112          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-113          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-114          [-1, 14, 14, 768]               0\n",
      "        Identity-115          [-1, 768, 14, 14]               0\n",
      "        Identity-116          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-117          [-1, 768, 14, 14]               0\n",
      "          Conv2d-118          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-119          [-1, 14, 14, 768]           1,536\n",
      "          Linear-120         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-121         [-1, 14, 14, 3072]               0\n",
      "         Dropout-122         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-123         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-124          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-125          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-126          [-1, 14, 14, 768]               0\n",
      "        Identity-127          [-1, 768, 14, 14]               0\n",
      "        Identity-128          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-129          [-1, 768, 14, 14]               0\n",
      "          Conv2d-130          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-131          [-1, 14, 14, 768]           1,536\n",
      "          Linear-132         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-133         [-1, 14, 14, 3072]               0\n",
      "         Dropout-134         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-135         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-136          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-137          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-138          [-1, 14, 14, 768]               0\n",
      "        Identity-139          [-1, 768, 14, 14]               0\n",
      "        Identity-140          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-141          [-1, 768, 14, 14]               0\n",
      "          Conv2d-142          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-143          [-1, 14, 14, 768]           1,536\n",
      "          Linear-144         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-145         [-1, 14, 14, 3072]               0\n",
      "         Dropout-146         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-147         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-148          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-149          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-150          [-1, 14, 14, 768]               0\n",
      "        Identity-151          [-1, 768, 14, 14]               0\n",
      "        Identity-152          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-153          [-1, 768, 14, 14]               0\n",
      "          Conv2d-154          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-155          [-1, 14, 14, 768]           1,536\n",
      "          Linear-156         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-157         [-1, 14, 14, 3072]               0\n",
      "         Dropout-158         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-159         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-160          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-161          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-162          [-1, 14, 14, 768]               0\n",
      "        Identity-163          [-1, 768, 14, 14]               0\n",
      "        Identity-164          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-165          [-1, 768, 14, 14]               0\n",
      "          Conv2d-166          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-167          [-1, 14, 14, 768]           1,536\n",
      "          Linear-168         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-169         [-1, 14, 14, 3072]               0\n",
      "         Dropout-170         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-171         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-172          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-173          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-174          [-1, 14, 14, 768]               0\n",
      "        Identity-175          [-1, 768, 14, 14]               0\n",
      "        Identity-176          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-177          [-1, 768, 14, 14]               0\n",
      "          Conv2d-178          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-179          [-1, 14, 14, 768]           1,536\n",
      "          Linear-180         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-181         [-1, 14, 14, 3072]               0\n",
      "         Dropout-182         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-183         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-184          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-185          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-186          [-1, 14, 14, 768]               0\n",
      "        Identity-187          [-1, 768, 14, 14]               0\n",
      "        Identity-188          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-189          [-1, 768, 14, 14]               0\n",
      "          Conv2d-190          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-191          [-1, 14, 14, 768]           1,536\n",
      "          Linear-192         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-193         [-1, 14, 14, 3072]               0\n",
      "         Dropout-194         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-195         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-196          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-197          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-198          [-1, 14, 14, 768]               0\n",
      "        Identity-199          [-1, 768, 14, 14]               0\n",
      "        Identity-200          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-201          [-1, 768, 14, 14]               0\n",
      "          Conv2d-202          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-203          [-1, 14, 14, 768]           1,536\n",
      "          Linear-204         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-205         [-1, 14, 14, 3072]               0\n",
      "         Dropout-206         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-207         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-208          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-209          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-210          [-1, 14, 14, 768]               0\n",
      "        Identity-211          [-1, 768, 14, 14]               0\n",
      "        Identity-212          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-213          [-1, 768, 14, 14]               0\n",
      "          Conv2d-214          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-215          [-1, 14, 14, 768]           1,536\n",
      "          Linear-216         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-217         [-1, 14, 14, 3072]               0\n",
      "         Dropout-218         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-219         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-220          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-221          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-222          [-1, 14, 14, 768]               0\n",
      "        Identity-223          [-1, 768, 14, 14]               0\n",
      "        Identity-224          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-225          [-1, 768, 14, 14]               0\n",
      "          Conv2d-226          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-227          [-1, 14, 14, 768]           1,536\n",
      "          Linear-228         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-229         [-1, 14, 14, 3072]               0\n",
      "         Dropout-230         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-231         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-232          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-233          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-234          [-1, 14, 14, 768]               0\n",
      "        Identity-235          [-1, 768, 14, 14]               0\n",
      "        Identity-236          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-237          [-1, 768, 14, 14]               0\n",
      "          Conv2d-238          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-239          [-1, 14, 14, 768]           1,536\n",
      "          Linear-240         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-241         [-1, 14, 14, 3072]               0\n",
      "         Dropout-242         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-243         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-244          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-245          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-246          [-1, 14, 14, 768]               0\n",
      "        Identity-247          [-1, 768, 14, 14]               0\n",
      "        Identity-248          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-249          [-1, 768, 14, 14]               0\n",
      "          Conv2d-250          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-251          [-1, 14, 14, 768]           1,536\n",
      "          Linear-252         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-253         [-1, 14, 14, 3072]               0\n",
      "         Dropout-254         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-255         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-256          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-257          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-258          [-1, 14, 14, 768]               0\n",
      "        Identity-259          [-1, 768, 14, 14]               0\n",
      "        Identity-260          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-261          [-1, 768, 14, 14]               0\n",
      "          Conv2d-262          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-263          [-1, 14, 14, 768]           1,536\n",
      "          Linear-264         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-265         [-1, 14, 14, 3072]               0\n",
      "         Dropout-266         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-267         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-268          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-269          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-270          [-1, 14, 14, 768]               0\n",
      "        Identity-271          [-1, 768, 14, 14]               0\n",
      "        Identity-272          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-273          [-1, 768, 14, 14]               0\n",
      "          Conv2d-274          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-275          [-1, 14, 14, 768]           1,536\n",
      "          Linear-276         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-277         [-1, 14, 14, 3072]               0\n",
      "         Dropout-278         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-279         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-280          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-281          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-282          [-1, 14, 14, 768]               0\n",
      "        Identity-283          [-1, 768, 14, 14]               0\n",
      "        Identity-284          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-285          [-1, 768, 14, 14]               0\n",
      "          Conv2d-286          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-287          [-1, 14, 14, 768]           1,536\n",
      "          Linear-288         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-289         [-1, 14, 14, 3072]               0\n",
      "         Dropout-290         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-291         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-292          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-293          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-294          [-1, 14, 14, 768]               0\n",
      "        Identity-295          [-1, 768, 14, 14]               0\n",
      "        Identity-296          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-297          [-1, 768, 14, 14]               0\n",
      "          Conv2d-298          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-299          [-1, 14, 14, 768]           1,536\n",
      "          Linear-300         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-301         [-1, 14, 14, 3072]               0\n",
      "         Dropout-302         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-303         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-304          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-305          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-306          [-1, 14, 14, 768]               0\n",
      "        Identity-307          [-1, 768, 14, 14]               0\n",
      "        Identity-308          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-309          [-1, 768, 14, 14]               0\n",
      "          Conv2d-310          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-311          [-1, 14, 14, 768]           1,536\n",
      "          Linear-312         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-313         [-1, 14, 14, 3072]               0\n",
      "         Dropout-314         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-315         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-316          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-317          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-318          [-1, 14, 14, 768]               0\n",
      "        Identity-319          [-1, 768, 14, 14]               0\n",
      "        Identity-320          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-321          [-1, 768, 14, 14]               0\n",
      "          Conv2d-322          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-323          [-1, 14, 14, 768]           1,536\n",
      "          Linear-324         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-325         [-1, 14, 14, 3072]               0\n",
      "         Dropout-326         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-327         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-328          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-329          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-330          [-1, 14, 14, 768]               0\n",
      "        Identity-331          [-1, 768, 14, 14]               0\n",
      "        Identity-332          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-333          [-1, 768, 14, 14]               0\n",
      "          Conv2d-334          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-335          [-1, 14, 14, 768]           1,536\n",
      "          Linear-336         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-337         [-1, 14, 14, 3072]               0\n",
      "         Dropout-338         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-339         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-340          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-341          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-342          [-1, 14, 14, 768]               0\n",
      "        Identity-343          [-1, 768, 14, 14]               0\n",
      "        Identity-344          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-345          [-1, 768, 14, 14]               0\n",
      "          Conv2d-346          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-347          [-1, 14, 14, 768]           1,536\n",
      "          Linear-348         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-349         [-1, 14, 14, 3072]               0\n",
      "         Dropout-350         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-351         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-352          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-353          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-354          [-1, 14, 14, 768]               0\n",
      "        Identity-355          [-1, 768, 14, 14]               0\n",
      "        Identity-356          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-357          [-1, 768, 14, 14]               0\n",
      "          Conv2d-358          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-359          [-1, 14, 14, 768]           1,536\n",
      "          Linear-360         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-361         [-1, 14, 14, 3072]               0\n",
      "         Dropout-362         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-363         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-364          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-365          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-366          [-1, 14, 14, 768]               0\n",
      "        Identity-367          [-1, 768, 14, 14]               0\n",
      "        Identity-368          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-369          [-1, 768, 14, 14]               0\n",
      "          Conv2d-370          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-371          [-1, 14, 14, 768]           1,536\n",
      "          Linear-372         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-373         [-1, 14, 14, 3072]               0\n",
      "         Dropout-374         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-375         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-376          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-377          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-378          [-1, 14, 14, 768]               0\n",
      "        Identity-379          [-1, 768, 14, 14]               0\n",
      "        Identity-380          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-381          [-1, 768, 14, 14]               0\n",
      "          Conv2d-382          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-383          [-1, 14, 14, 768]           1,536\n",
      "          Linear-384         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-385         [-1, 14, 14, 3072]               0\n",
      "         Dropout-386         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-387         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-388          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-389          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-390          [-1, 14, 14, 768]               0\n",
      "        Identity-391          [-1, 768, 14, 14]               0\n",
      "        Identity-392          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-393          [-1, 768, 14, 14]               0\n",
      "          Conv2d-394          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-395          [-1, 14, 14, 768]           1,536\n",
      "          Linear-396         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-397         [-1, 14, 14, 3072]               0\n",
      "         Dropout-398         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-399         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-400          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-401          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-402          [-1, 14, 14, 768]               0\n",
      "        Identity-403          [-1, 768, 14, 14]               0\n",
      "        Identity-404          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-405          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtStage-406          [-1, 768, 14, 14]               0\n",
      "     LayerNorm2d-407          [-1, 768, 14, 14]           1,536\n",
      "          Conv2d-408           [-1, 1536, 7, 7]       4,720,128\n",
      "          Conv2d-409           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-410           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-411           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-412           [-1, 7, 7, 6144]               0\n",
      "         Dropout-413           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-414           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-415           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-416           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-417           [-1, 7, 7, 1536]               0\n",
      "        Identity-418           [-1, 1536, 7, 7]               0\n",
      "        Identity-419           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-420           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-421           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-422           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-423           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-424           [-1, 7, 7, 6144]               0\n",
      "         Dropout-425           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-426           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-427           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-428           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-429           [-1, 7, 7, 1536]               0\n",
      "        Identity-430           [-1, 1536, 7, 7]               0\n",
      "        Identity-431           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-432           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-433           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-434           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-435           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-436           [-1, 7, 7, 6144]               0\n",
      "         Dropout-437           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-438           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-439           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-440           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-441           [-1, 7, 7, 1536]               0\n",
      "        Identity-442           [-1, 1536, 7, 7]               0\n",
      "        Identity-443           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-444           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtStage-445           [-1, 1536, 7, 7]               0\n",
      "        Identity-446           [-1, 1536, 7, 7]               0\n",
      "AdaptiveAvgPool2d-447           [-1, 1536, 1, 1]               0\n",
      "        Identity-448           [-1, 1536, 1, 1]               0\n",
      "SelectAdaptivePool2d-449           [-1, 1536, 1, 1]               0\n",
      "     LayerNorm2d-450           [-1, 1536, 1, 1]           3,072\n",
      "         Flatten-451                 [-1, 1536]               0\n",
      "        Identity-452                 [-1, 1536]               0\n",
      "         Dropout-453                 [-1, 1536]               0\n",
      "          Linear-454                   [-1, 17]          26,129\n",
      "NormMlpClassifierHead-455                   [-1, 17]               0\n",
      "================================================================\n",
      "Total params: 196,445,969\n",
      "Trainable params: 196,445,969\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 1316.77\n",
      "Params size (MB): 749.38\n",
      "Estimated Total Size (MB): 2066.72\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2964: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.9600, Train Acc: 0.6959, Train F1: 0.6740\n",
      "Val Loss: 0.4322, Val Acc: 0.8631, Val F1: 0.8098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2529: 100%|██████████| 40/40 [00:29<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.3619, Train Acc: 0.8646, Train F1: 0.8471\n",
      "Val Loss: 0.2831, Val Acc: 0.8854, Val F1: 0.8503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1453: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.2100, Train Acc: 0.9156, Train F1: 0.9062\n",
      "Val Loss: 0.2002, Val Acc: 0.9045, Val F1: 0.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1877: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1640, Train Acc: 0.9355, Train F1: 0.9296\n",
      "Val Loss: 0.2879, Val Acc: 0.9013, Val F1: 0.8905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1187: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.1153, Train Acc: 0.9514, Train F1: 0.9525\n",
      "Val Loss: 0.1906, Val Acc: 0.9299, Val F1: 0.9265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0913: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.1035, Train Acc: 0.9594, Train F1: 0.9576\n",
      "Val Loss: 0.2150, Val Acc: 0.9204, Val F1: 0.9159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0023: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0806, Train Acc: 0.9682, Train F1: 0.9666\n",
      "Val Loss: 0.2323, Val Acc: 0.9140, Val F1: 0.9072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0121: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0602, Train Acc: 0.9777, Train F1: 0.9782\n",
      "Val Loss: 0.1830, Val Acc: 0.9363, Val F1: 0.9342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0047: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0543, Train Acc: 0.9785, Train F1: 0.9783\n",
      "Val Loss: 0.2719, Val Acc: 0.9045, Val F1: 0.8931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0010: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0550, Train Acc: 0.9761, Train F1: 0.9752\n",
      "Val Loss: 0.2324, Val Acc: 0.9108, Val F1: 0.8992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2174: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "Train Loss: 0.0490, Train Acc: 0.9849, Train F1: 0.9856\n",
      "Val Loss: 0.1743, Val Acc: 0.9363, Val F1: 0.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6106: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "Train Loss: 0.0573, Train Acc: 0.9849, Train F1: 0.9850\n",
      "Val Loss: 0.1767, Val Acc: 0.9395, Val F1: 0.9392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1281: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "Train Loss: 0.0872, Train Acc: 0.9745, Train F1: 0.9751\n",
      "Val Loss: 0.2215, Val Acc: 0.9172, Val F1: 0.9072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "Train Loss: 0.0412, Train Acc: 0.9889, Train F1: 0.9886\n",
      "Val Loss: 0.2170, Val Acc: 0.9331, Val F1: 0.9309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "Train Loss: 0.0256, Train Acc: 0.9912, Train F1: 0.9908\n",
      "Val Loss: 0.2424, Val Acc: 0.9299, Val F1: 0.9262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0707: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "Train Loss: 0.0138, Train Acc: 0.9952, Train F1: 0.9946\n",
      "Val Loss: 0.2308, Val Acc: 0.9268, Val F1: 0.9233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "Train Loss: 0.0098, Train Acc: 0.9976, Train F1: 0.9978\n",
      "Val Loss: 0.2616, Val Acc: 0.9204, Val F1: 0.9159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0017: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "Train Loss: 0.0083, Train Acc: 0.9976, Train F1: 0.9975\n",
      "Val Loss: 0.2414, Val Acc: 0.9236, Val F1: 0.9230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "Train Loss: 0.0101, Train Acc: 0.9968, Train F1: 0.9967\n",
      "Val Loss: 0.2392, Val Acc: 0.9268, Val F1: 0.9243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0020: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "Train Loss: 0.0043, Train Acc: 0.9992, Train F1: 0.9989\n",
      "Val Loss: 0.2168, Val Acc: 0.9331, Val F1: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "Train Loss: 0.0036, Train Acc: 0.9984, Train F1: 0.9982\n",
      "Val Loss: 0.2205, Val Acc: 0.9299, Val F1: 0.9298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "Train Loss: 0.0039, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.2330, Val Acc: 0.9268, Val F1: 0.9257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0010: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "Train Loss: 0.0026, Train Acc: 0.9992, Train F1: 0.9993\n",
      "Val Loss: 0.2459, Val Acc: 0.9268, Val F1: 0.9246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "Train Loss: 0.0016, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2468, Val Acc: 0.9268, Val F1: 0.9246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0017: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "Train Loss: 0.0010, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2471, Val Acc: 0.9299, Val F1: 0.9289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "Train Loss: 0.0015, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2464, Val Acc: 0.9268, Val F1: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "Train Loss: 0.0014, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2477, Val Acc: 0.9268, Val F1: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "Train Loss: 0.0011, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2466, Val Acc: 0.9268, Val F1: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "Train Loss: 0.0017, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2470, Val Acc: 0.9268, Val F1: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 40/40 [00:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "Train Loss: 0.0008, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2472, Val Acc: 0.9268, Val F1: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:18<00:00,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'convnextv2_large'\n",
    "    img_size = 224  # ConvNeXt V2 Large에 적합한 이미지 크기\n",
    "    LR = 1e-4  # 학습률 조정\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32  # 배치 크기 조정\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"convNext_model.pth\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"convNext_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters Tunning With CNN Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNeXt V2 Large 모델 + Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import optuna\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# Optuna를 이용한 하이퍼파라미터 최적화 함수\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 탐색 공간 정의\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "\n",
    "    # 데이터 로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 학습 및 검증\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "\n",
    "    return best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '../data/'\n",
    "model_name = 'convnextv2_large'\n",
    "img_size = 224  # ConvNeXt V2에 적합한 이미지 크기\n",
    "EPOCHS = 30\n",
    "num_workers = 4\n",
    "\n",
    "# 데이터 증강 설정\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터 로드 및 분할\n",
    "df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "# Optuna를 이용한 하이퍼파라미터 최적화\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 최종 모델 학습\n",
    "best_lr = best_params['lr']\n",
    "best_batch_size = best_params['batch_size']\n",
    "best_weight_decay = best_params['weight_decay']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# 모델 구조 출력\n",
    "print(f\"\\nModel structure of {model_name}:\")\n",
    "print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "# 학습 루프\n",
    "best_val_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNext V2 Large + WanDB Sweep\n",
    "- pip install wandb\n",
    "- wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "#class ImageDataset(Dataset):\n",
    "\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "#def validate(loader, model, loss_fn, device):\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "#def print_model_summary(model, input_size):\n",
    "\n",
    "\n",
    "# wandb sweep을 위한 학습 함수\n",
    "def train():\n",
    "    # wandb 초기화\n",
    "    run = wandb.init(entity=\"cho\") #사용자에 따라 자신의 도메인 네임 설정!!!\n",
    "    config = wandb.config\n",
    "\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = config.model_name\n",
    "    img_size = config.img_size\n",
    "    LR = config.learning_rate\n",
    "    EPOCHS = config.epochs\n",
    "    BATCH_SIZE = config.batch_size\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=config.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # wandb에 로그 기록\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            wandb.run.summary[\"best_val_f1\"] = best_val_f1\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# wandb sweep 설정\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'val_f1',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'model_name': {\n",
    "            'values': ['convnextv2_large', 'efficientnet_b4']\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-3\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'img_size': {\n",
    "            'values': [224, 256, 288]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [1e-5, 1e-4, 1e-3]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 30\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# wandb sweep 실행 및 최고 성능 모델 찾기\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"cvmodel\",entity=\"cho\")\n",
    "wandb.agent(sweep_id, train, count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 최고 성능 모델의 설정 가져오기\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"dl-12/cvmodel/{sweep_id}\")\n",
    "best_run = sweep.best_run()\n",
    "best_config = best_run.config\n",
    "\n",
    "  \n",
    "# 최고 성능 모델의 설정 사용\n",
    "model_name = best_config['model_name']\n",
    "img_size = best_config['img_size']\n",
    "BATCH_SIZE = best_config['batch_size']\n",
    "num_workers = 4\n",
    "\n",
    "# 테스트 데이터 변환\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 테스트 데이터셋 및 데이터로더 생성\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 최고 성능 모델 생성\n",
    "model = timm.create_model(model_name, pretrained=False, num_classes=17).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to pred.csv\")\n",
    "\n",
    "# wandb에 결과 업로드\n",
    "wandb.init(project=\"cvmodel\", name=\"best_model_prediction\", entity=\"cho\")\n",
    "wandb.config.update(best_config)\n",
    "wandb.save(\"pred.csv\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 기반 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b819fe15d47d448b8ec440cbf59ffdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/788M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model structure of swin_large_patch4_window7_224:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 192, 56, 56]           9,408\n",
      "         LayerNorm-2          [-1, 56, 56, 192]             384\n",
      "        PatchEmbed-3          [-1, 56, 56, 192]               0\n",
      "          Identity-4          [-1, 56, 56, 192]               0\n",
      "         LayerNorm-5          [-1, 56, 56, 192]             384\n",
      "            Linear-6              [-1, 49, 576]         111,168\n",
      "           Softmax-7            [-1, 6, 49, 49]               0\n",
      "           Dropout-8            [-1, 6, 49, 49]               0\n",
      "            Linear-9              [-1, 49, 192]          37,056\n",
      "          Dropout-10              [-1, 49, 192]               0\n",
      "  WindowAttention-11              [-1, 49, 192]               0\n",
      "         Identity-12          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-13            [-1, 3136, 192]             384\n",
      "           Linear-14            [-1, 3136, 768]         148,224\n",
      "             GELU-15            [-1, 3136, 768]               0\n",
      "          Dropout-16            [-1, 3136, 768]               0\n",
      "         Identity-17            [-1, 3136, 768]               0\n",
      "           Linear-18            [-1, 3136, 192]         147,648\n",
      "          Dropout-19            [-1, 3136, 192]               0\n",
      "              Mlp-20            [-1, 3136, 192]               0\n",
      "         Identity-21            [-1, 3136, 192]               0\n",
      "SwinTransformerBlock-22          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-23          [-1, 56, 56, 192]             384\n",
      "           Linear-24              [-1, 49, 576]         111,168\n",
      "          Softmax-25            [-1, 6, 49, 49]               0\n",
      "          Dropout-26            [-1, 6, 49, 49]               0\n",
      "           Linear-27              [-1, 49, 192]          37,056\n",
      "          Dropout-28              [-1, 49, 192]               0\n",
      "  WindowAttention-29              [-1, 49, 192]               0\n",
      "         DropPath-30          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-31            [-1, 3136, 192]             384\n",
      "           Linear-32            [-1, 3136, 768]         148,224\n",
      "             GELU-33            [-1, 3136, 768]               0\n",
      "          Dropout-34            [-1, 3136, 768]               0\n",
      "         Identity-35            [-1, 3136, 768]               0\n",
      "           Linear-36            [-1, 3136, 192]         147,648\n",
      "          Dropout-37            [-1, 3136, 192]               0\n",
      "              Mlp-38            [-1, 3136, 192]               0\n",
      "         DropPath-39            [-1, 3136, 192]               0\n",
      "SwinTransformerBlock-40          [-1, 56, 56, 192]               0\n",
      "SwinTransformerStage-41          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-42          [-1, 28, 28, 768]           1,536\n",
      "           Linear-43          [-1, 28, 28, 384]         294,912\n",
      "     PatchMerging-44          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-45          [-1, 28, 28, 384]             768\n",
      "           Linear-46             [-1, 49, 1152]         443,520\n",
      "          Softmax-47           [-1, 12, 49, 49]               0\n",
      "          Dropout-48           [-1, 12, 49, 49]               0\n",
      "           Linear-49              [-1, 49, 384]         147,840\n",
      "          Dropout-50              [-1, 49, 384]               0\n",
      "  WindowAttention-51              [-1, 49, 384]               0\n",
      "         DropPath-52          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-53             [-1, 784, 384]             768\n",
      "           Linear-54            [-1, 784, 1536]         591,360\n",
      "             GELU-55            [-1, 784, 1536]               0\n",
      "          Dropout-56            [-1, 784, 1536]               0\n",
      "         Identity-57            [-1, 784, 1536]               0\n",
      "           Linear-58             [-1, 784, 384]         590,208\n",
      "          Dropout-59             [-1, 784, 384]               0\n",
      "              Mlp-60             [-1, 784, 384]               0\n",
      "         DropPath-61             [-1, 784, 384]               0\n",
      "SwinTransformerBlock-62          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-63          [-1, 28, 28, 384]             768\n",
      "           Linear-64             [-1, 49, 1152]         443,520\n",
      "          Softmax-65           [-1, 12, 49, 49]               0\n",
      "          Dropout-66           [-1, 12, 49, 49]               0\n",
      "           Linear-67              [-1, 49, 384]         147,840\n",
      "          Dropout-68              [-1, 49, 384]               0\n",
      "  WindowAttention-69              [-1, 49, 384]               0\n",
      "         DropPath-70          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-71             [-1, 784, 384]             768\n",
      "           Linear-72            [-1, 784, 1536]         591,360\n",
      "             GELU-73            [-1, 784, 1536]               0\n",
      "          Dropout-74            [-1, 784, 1536]               0\n",
      "         Identity-75            [-1, 784, 1536]               0\n",
      "           Linear-76             [-1, 784, 384]         590,208\n",
      "          Dropout-77             [-1, 784, 384]               0\n",
      "              Mlp-78             [-1, 784, 384]               0\n",
      "         DropPath-79             [-1, 784, 384]               0\n",
      "SwinTransformerBlock-80          [-1, 28, 28, 384]               0\n",
      "SwinTransformerStage-81          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-82         [-1, 14, 14, 1536]           3,072\n",
      "           Linear-83          [-1, 14, 14, 768]       1,179,648\n",
      "     PatchMerging-84          [-1, 14, 14, 768]               0\n",
      "        LayerNorm-85          [-1, 14, 14, 768]           1,536\n",
      "           Linear-86             [-1, 49, 2304]       1,771,776\n",
      "          Softmax-87           [-1, 24, 49, 49]               0\n",
      "          Dropout-88           [-1, 24, 49, 49]               0\n",
      "           Linear-89              [-1, 49, 768]         590,592\n",
      "          Dropout-90              [-1, 49, 768]               0\n",
      "  WindowAttention-91              [-1, 49, 768]               0\n",
      "         DropPath-92          [-1, 14, 14, 768]               0\n",
      "        LayerNorm-93             [-1, 196, 768]           1,536\n",
      "           Linear-94            [-1, 196, 3072]       2,362,368\n",
      "             GELU-95            [-1, 196, 3072]               0\n",
      "          Dropout-96            [-1, 196, 3072]               0\n",
      "         Identity-97            [-1, 196, 3072]               0\n",
      "           Linear-98             [-1, 196, 768]       2,360,064\n",
      "          Dropout-99             [-1, 196, 768]               0\n",
      "             Mlp-100             [-1, 196, 768]               0\n",
      "        DropPath-101             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-102          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-103          [-1, 14, 14, 768]           1,536\n",
      "          Linear-104             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-105           [-1, 24, 49, 49]               0\n",
      "         Dropout-106           [-1, 24, 49, 49]               0\n",
      "          Linear-107              [-1, 49, 768]         590,592\n",
      "         Dropout-108              [-1, 49, 768]               0\n",
      " WindowAttention-109              [-1, 49, 768]               0\n",
      "        DropPath-110          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-111             [-1, 196, 768]           1,536\n",
      "          Linear-112            [-1, 196, 3072]       2,362,368\n",
      "            GELU-113            [-1, 196, 3072]               0\n",
      "         Dropout-114            [-1, 196, 3072]               0\n",
      "        Identity-115            [-1, 196, 3072]               0\n",
      "          Linear-116             [-1, 196, 768]       2,360,064\n",
      "         Dropout-117             [-1, 196, 768]               0\n",
      "             Mlp-118             [-1, 196, 768]               0\n",
      "        DropPath-119             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-120          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-121          [-1, 14, 14, 768]           1,536\n",
      "          Linear-122             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-123           [-1, 24, 49, 49]               0\n",
      "         Dropout-124           [-1, 24, 49, 49]               0\n",
      "          Linear-125              [-1, 49, 768]         590,592\n",
      "         Dropout-126              [-1, 49, 768]               0\n",
      " WindowAttention-127              [-1, 49, 768]               0\n",
      "        DropPath-128          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-129             [-1, 196, 768]           1,536\n",
      "          Linear-130            [-1, 196, 3072]       2,362,368\n",
      "            GELU-131            [-1, 196, 3072]               0\n",
      "         Dropout-132            [-1, 196, 3072]               0\n",
      "        Identity-133            [-1, 196, 3072]               0\n",
      "          Linear-134             [-1, 196, 768]       2,360,064\n",
      "         Dropout-135             [-1, 196, 768]               0\n",
      "             Mlp-136             [-1, 196, 768]               0\n",
      "        DropPath-137             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-138          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-139          [-1, 14, 14, 768]           1,536\n",
      "          Linear-140             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-141           [-1, 24, 49, 49]               0\n",
      "         Dropout-142           [-1, 24, 49, 49]               0\n",
      "          Linear-143              [-1, 49, 768]         590,592\n",
      "         Dropout-144              [-1, 49, 768]               0\n",
      " WindowAttention-145              [-1, 49, 768]               0\n",
      "        DropPath-146          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-147             [-1, 196, 768]           1,536\n",
      "          Linear-148            [-1, 196, 3072]       2,362,368\n",
      "            GELU-149            [-1, 196, 3072]               0\n",
      "         Dropout-150            [-1, 196, 3072]               0\n",
      "        Identity-151            [-1, 196, 3072]               0\n",
      "          Linear-152             [-1, 196, 768]       2,360,064\n",
      "         Dropout-153             [-1, 196, 768]               0\n",
      "             Mlp-154             [-1, 196, 768]               0\n",
      "        DropPath-155             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-156          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-157          [-1, 14, 14, 768]           1,536\n",
      "          Linear-158             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-159           [-1, 24, 49, 49]               0\n",
      "         Dropout-160           [-1, 24, 49, 49]               0\n",
      "          Linear-161              [-1, 49, 768]         590,592\n",
      "         Dropout-162              [-1, 49, 768]               0\n",
      " WindowAttention-163              [-1, 49, 768]               0\n",
      "        DropPath-164          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-165             [-1, 196, 768]           1,536\n",
      "          Linear-166            [-1, 196, 3072]       2,362,368\n",
      "            GELU-167            [-1, 196, 3072]               0\n",
      "         Dropout-168            [-1, 196, 3072]               0\n",
      "        Identity-169            [-1, 196, 3072]               0\n",
      "          Linear-170             [-1, 196, 768]       2,360,064\n",
      "         Dropout-171             [-1, 196, 768]               0\n",
      "             Mlp-172             [-1, 196, 768]               0\n",
      "        DropPath-173             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-174          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-175          [-1, 14, 14, 768]           1,536\n",
      "          Linear-176             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-177           [-1, 24, 49, 49]               0\n",
      "         Dropout-178           [-1, 24, 49, 49]               0\n",
      "          Linear-179              [-1, 49, 768]         590,592\n",
      "         Dropout-180              [-1, 49, 768]               0\n",
      " WindowAttention-181              [-1, 49, 768]               0\n",
      "        DropPath-182          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-183             [-1, 196, 768]           1,536\n",
      "          Linear-184            [-1, 196, 3072]       2,362,368\n",
      "            GELU-185            [-1, 196, 3072]               0\n",
      "         Dropout-186            [-1, 196, 3072]               0\n",
      "        Identity-187            [-1, 196, 3072]               0\n",
      "          Linear-188             [-1, 196, 768]       2,360,064\n",
      "         Dropout-189             [-1, 196, 768]               0\n",
      "             Mlp-190             [-1, 196, 768]               0\n",
      "        DropPath-191             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-192          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-193          [-1, 14, 14, 768]           1,536\n",
      "          Linear-194             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-195           [-1, 24, 49, 49]               0\n",
      "         Dropout-196           [-1, 24, 49, 49]               0\n",
      "          Linear-197              [-1, 49, 768]         590,592\n",
      "         Dropout-198              [-1, 49, 768]               0\n",
      " WindowAttention-199              [-1, 49, 768]               0\n",
      "        DropPath-200          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-201             [-1, 196, 768]           1,536\n",
      "          Linear-202            [-1, 196, 3072]       2,362,368\n",
      "            GELU-203            [-1, 196, 3072]               0\n",
      "         Dropout-204            [-1, 196, 3072]               0\n",
      "        Identity-205            [-1, 196, 3072]               0\n",
      "          Linear-206             [-1, 196, 768]       2,360,064\n",
      "         Dropout-207             [-1, 196, 768]               0\n",
      "             Mlp-208             [-1, 196, 768]               0\n",
      "        DropPath-209             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-210          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-211          [-1, 14, 14, 768]           1,536\n",
      "          Linear-212             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-213           [-1, 24, 49, 49]               0\n",
      "         Dropout-214           [-1, 24, 49, 49]               0\n",
      "          Linear-215              [-1, 49, 768]         590,592\n",
      "         Dropout-216              [-1, 49, 768]               0\n",
      " WindowAttention-217              [-1, 49, 768]               0\n",
      "        DropPath-218          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-219             [-1, 196, 768]           1,536\n",
      "          Linear-220            [-1, 196, 3072]       2,362,368\n",
      "            GELU-221            [-1, 196, 3072]               0\n",
      "         Dropout-222            [-1, 196, 3072]               0\n",
      "        Identity-223            [-1, 196, 3072]               0\n",
      "          Linear-224             [-1, 196, 768]       2,360,064\n",
      "         Dropout-225             [-1, 196, 768]               0\n",
      "             Mlp-226             [-1, 196, 768]               0\n",
      "        DropPath-227             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-228          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-229          [-1, 14, 14, 768]           1,536\n",
      "          Linear-230             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-231           [-1, 24, 49, 49]               0\n",
      "         Dropout-232           [-1, 24, 49, 49]               0\n",
      "          Linear-233              [-1, 49, 768]         590,592\n",
      "         Dropout-234              [-1, 49, 768]               0\n",
      " WindowAttention-235              [-1, 49, 768]               0\n",
      "        DropPath-236          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-237             [-1, 196, 768]           1,536\n",
      "          Linear-238            [-1, 196, 3072]       2,362,368\n",
      "            GELU-239            [-1, 196, 3072]               0\n",
      "         Dropout-240            [-1, 196, 3072]               0\n",
      "        Identity-241            [-1, 196, 3072]               0\n",
      "          Linear-242             [-1, 196, 768]       2,360,064\n",
      "         Dropout-243             [-1, 196, 768]               0\n",
      "             Mlp-244             [-1, 196, 768]               0\n",
      "        DropPath-245             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-246          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-247          [-1, 14, 14, 768]           1,536\n",
      "          Linear-248             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-249           [-1, 24, 49, 49]               0\n",
      "         Dropout-250           [-1, 24, 49, 49]               0\n",
      "          Linear-251              [-1, 49, 768]         590,592\n",
      "         Dropout-252              [-1, 49, 768]               0\n",
      " WindowAttention-253              [-1, 49, 768]               0\n",
      "        DropPath-254          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-255             [-1, 196, 768]           1,536\n",
      "          Linear-256            [-1, 196, 3072]       2,362,368\n",
      "            GELU-257            [-1, 196, 3072]               0\n",
      "         Dropout-258            [-1, 196, 3072]               0\n",
      "        Identity-259            [-1, 196, 3072]               0\n",
      "          Linear-260             [-1, 196, 768]       2,360,064\n",
      "         Dropout-261             [-1, 196, 768]               0\n",
      "             Mlp-262             [-1, 196, 768]               0\n",
      "        DropPath-263             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-264          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-265          [-1, 14, 14, 768]           1,536\n",
      "          Linear-266             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-267           [-1, 24, 49, 49]               0\n",
      "         Dropout-268           [-1, 24, 49, 49]               0\n",
      "          Linear-269              [-1, 49, 768]         590,592\n",
      "         Dropout-270              [-1, 49, 768]               0\n",
      " WindowAttention-271              [-1, 49, 768]               0\n",
      "        DropPath-272          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-273             [-1, 196, 768]           1,536\n",
      "          Linear-274            [-1, 196, 3072]       2,362,368\n",
      "            GELU-275            [-1, 196, 3072]               0\n",
      "         Dropout-276            [-1, 196, 3072]               0\n",
      "        Identity-277            [-1, 196, 3072]               0\n",
      "          Linear-278             [-1, 196, 768]       2,360,064\n",
      "         Dropout-279             [-1, 196, 768]               0\n",
      "             Mlp-280             [-1, 196, 768]               0\n",
      "        DropPath-281             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-282          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-283          [-1, 14, 14, 768]           1,536\n",
      "          Linear-284             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-285           [-1, 24, 49, 49]               0\n",
      "         Dropout-286           [-1, 24, 49, 49]               0\n",
      "          Linear-287              [-1, 49, 768]         590,592\n",
      "         Dropout-288              [-1, 49, 768]               0\n",
      " WindowAttention-289              [-1, 49, 768]               0\n",
      "        DropPath-290          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-291             [-1, 196, 768]           1,536\n",
      "          Linear-292            [-1, 196, 3072]       2,362,368\n",
      "            GELU-293            [-1, 196, 3072]               0\n",
      "         Dropout-294            [-1, 196, 3072]               0\n",
      "        Identity-295            [-1, 196, 3072]               0\n",
      "          Linear-296             [-1, 196, 768]       2,360,064\n",
      "         Dropout-297             [-1, 196, 768]               0\n",
      "             Mlp-298             [-1, 196, 768]               0\n",
      "        DropPath-299             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-300          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-301          [-1, 14, 14, 768]           1,536\n",
      "          Linear-302             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-303           [-1, 24, 49, 49]               0\n",
      "         Dropout-304           [-1, 24, 49, 49]               0\n",
      "          Linear-305              [-1, 49, 768]         590,592\n",
      "         Dropout-306              [-1, 49, 768]               0\n",
      " WindowAttention-307              [-1, 49, 768]               0\n",
      "        DropPath-308          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-309             [-1, 196, 768]           1,536\n",
      "          Linear-310            [-1, 196, 3072]       2,362,368\n",
      "            GELU-311            [-1, 196, 3072]               0\n",
      "         Dropout-312            [-1, 196, 3072]               0\n",
      "        Identity-313            [-1, 196, 3072]               0\n",
      "          Linear-314             [-1, 196, 768]       2,360,064\n",
      "         Dropout-315             [-1, 196, 768]               0\n",
      "             Mlp-316             [-1, 196, 768]               0\n",
      "        DropPath-317             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-318          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-319          [-1, 14, 14, 768]           1,536\n",
      "          Linear-320             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-321           [-1, 24, 49, 49]               0\n",
      "         Dropout-322           [-1, 24, 49, 49]               0\n",
      "          Linear-323              [-1, 49, 768]         590,592\n",
      "         Dropout-324              [-1, 49, 768]               0\n",
      " WindowAttention-325              [-1, 49, 768]               0\n",
      "        DropPath-326          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-327             [-1, 196, 768]           1,536\n",
      "          Linear-328            [-1, 196, 3072]       2,362,368\n",
      "            GELU-329            [-1, 196, 3072]               0\n",
      "         Dropout-330            [-1, 196, 3072]               0\n",
      "        Identity-331            [-1, 196, 3072]               0\n",
      "          Linear-332             [-1, 196, 768]       2,360,064\n",
      "         Dropout-333             [-1, 196, 768]               0\n",
      "             Mlp-334             [-1, 196, 768]               0\n",
      "        DropPath-335             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-336          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-337          [-1, 14, 14, 768]           1,536\n",
      "          Linear-338             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-339           [-1, 24, 49, 49]               0\n",
      "         Dropout-340           [-1, 24, 49, 49]               0\n",
      "          Linear-341              [-1, 49, 768]         590,592\n",
      "         Dropout-342              [-1, 49, 768]               0\n",
      " WindowAttention-343              [-1, 49, 768]               0\n",
      "        DropPath-344          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-345             [-1, 196, 768]           1,536\n",
      "          Linear-346            [-1, 196, 3072]       2,362,368\n",
      "            GELU-347            [-1, 196, 3072]               0\n",
      "         Dropout-348            [-1, 196, 3072]               0\n",
      "        Identity-349            [-1, 196, 3072]               0\n",
      "          Linear-350             [-1, 196, 768]       2,360,064\n",
      "         Dropout-351             [-1, 196, 768]               0\n",
      "             Mlp-352             [-1, 196, 768]               0\n",
      "        DropPath-353             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-354          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-355          [-1, 14, 14, 768]           1,536\n",
      "          Linear-356             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-357           [-1, 24, 49, 49]               0\n",
      "         Dropout-358           [-1, 24, 49, 49]               0\n",
      "          Linear-359              [-1, 49, 768]         590,592\n",
      "         Dropout-360              [-1, 49, 768]               0\n",
      " WindowAttention-361              [-1, 49, 768]               0\n",
      "        DropPath-362          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-363             [-1, 196, 768]           1,536\n",
      "          Linear-364            [-1, 196, 3072]       2,362,368\n",
      "            GELU-365            [-1, 196, 3072]               0\n",
      "         Dropout-366            [-1, 196, 3072]               0\n",
      "        Identity-367            [-1, 196, 3072]               0\n",
      "          Linear-368             [-1, 196, 768]       2,360,064\n",
      "         Dropout-369             [-1, 196, 768]               0\n",
      "             Mlp-370             [-1, 196, 768]               0\n",
      "        DropPath-371             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-372          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-373          [-1, 14, 14, 768]           1,536\n",
      "          Linear-374             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-375           [-1, 24, 49, 49]               0\n",
      "         Dropout-376           [-1, 24, 49, 49]               0\n",
      "          Linear-377              [-1, 49, 768]         590,592\n",
      "         Dropout-378              [-1, 49, 768]               0\n",
      " WindowAttention-379              [-1, 49, 768]               0\n",
      "        DropPath-380          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-381             [-1, 196, 768]           1,536\n",
      "          Linear-382            [-1, 196, 3072]       2,362,368\n",
      "            GELU-383            [-1, 196, 3072]               0\n",
      "         Dropout-384            [-1, 196, 3072]               0\n",
      "        Identity-385            [-1, 196, 3072]               0\n",
      "          Linear-386             [-1, 196, 768]       2,360,064\n",
      "         Dropout-387             [-1, 196, 768]               0\n",
      "             Mlp-388             [-1, 196, 768]               0\n",
      "        DropPath-389             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-390          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-391          [-1, 14, 14, 768]           1,536\n",
      "          Linear-392             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-393           [-1, 24, 49, 49]               0\n",
      "         Dropout-394           [-1, 24, 49, 49]               0\n",
      "          Linear-395              [-1, 49, 768]         590,592\n",
      "         Dropout-396              [-1, 49, 768]               0\n",
      " WindowAttention-397              [-1, 49, 768]               0\n",
      "        DropPath-398          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-399             [-1, 196, 768]           1,536\n",
      "          Linear-400            [-1, 196, 3072]       2,362,368\n",
      "            GELU-401            [-1, 196, 3072]               0\n",
      "         Dropout-402            [-1, 196, 3072]               0\n",
      "        Identity-403            [-1, 196, 3072]               0\n",
      "          Linear-404             [-1, 196, 768]       2,360,064\n",
      "         Dropout-405             [-1, 196, 768]               0\n",
      "             Mlp-406             [-1, 196, 768]               0\n",
      "        DropPath-407             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-408          [-1, 14, 14, 768]               0\n",
      "SwinTransformerStage-409          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-410           [-1, 7, 7, 3072]           6,144\n",
      "          Linear-411           [-1, 7, 7, 1536]       4,718,592\n",
      "    PatchMerging-412           [-1, 7, 7, 1536]               0\n",
      "       LayerNorm-413           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-414             [-1, 49, 4608]       7,082,496\n",
      "         Softmax-415           [-1, 48, 49, 49]               0\n",
      "         Dropout-416           [-1, 48, 49, 49]               0\n",
      "          Linear-417             [-1, 49, 1536]       2,360,832\n",
      "         Dropout-418             [-1, 49, 1536]               0\n",
      " WindowAttention-419             [-1, 49, 1536]               0\n",
      "        DropPath-420           [-1, 7, 7, 1536]               0\n",
      "       LayerNorm-421             [-1, 49, 1536]           3,072\n",
      "          Linear-422             [-1, 49, 6144]       9,443,328\n",
      "            GELU-423             [-1, 49, 6144]               0\n",
      "         Dropout-424             [-1, 49, 6144]               0\n",
      "        Identity-425             [-1, 49, 6144]               0\n",
      "          Linear-426             [-1, 49, 1536]       9,438,720\n",
      "         Dropout-427             [-1, 49, 1536]               0\n",
      "             Mlp-428             [-1, 49, 1536]               0\n",
      "        DropPath-429             [-1, 49, 1536]               0\n",
      "SwinTransformerBlock-430           [-1, 7, 7, 1536]               0\n",
      "       LayerNorm-431           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-432             [-1, 49, 4608]       7,082,496\n",
      "         Softmax-433           [-1, 48, 49, 49]               0\n",
      "         Dropout-434           [-1, 48, 49, 49]               0\n",
      "          Linear-435             [-1, 49, 1536]       2,360,832\n",
      "         Dropout-436             [-1, 49, 1536]               0\n",
      " WindowAttention-437             [-1, 49, 1536]               0\n",
      "        DropPath-438           [-1, 7, 7, 1536]               0\n",
      "       LayerNorm-439             [-1, 49, 1536]           3,072\n",
      "          Linear-440             [-1, 49, 6144]       9,443,328\n",
      "            GELU-441             [-1, 49, 6144]               0\n",
      "         Dropout-442             [-1, 49, 6144]               0\n",
      "        Identity-443             [-1, 49, 6144]               0\n",
      "          Linear-444             [-1, 49, 1536]       9,438,720\n",
      "         Dropout-445             [-1, 49, 1536]               0\n",
      "             Mlp-446             [-1, 49, 1536]               0\n",
      "        DropPath-447             [-1, 49, 1536]               0\n",
      "SwinTransformerBlock-448           [-1, 7, 7, 1536]               0\n",
      "SwinTransformerStage-449           [-1, 7, 7, 1536]               0\n",
      "       LayerNorm-450           [-1, 7, 7, 1536]           3,072\n",
      "FastAdaptiveAvgPool-451                 [-1, 1536]               0\n",
      "        Identity-452                 [-1, 1536]               0\n",
      "SelectAdaptivePool2d-453                 [-1, 1536]               0\n",
      "         Dropout-454                 [-1, 1536]               0\n",
      "          Linear-455                   [-1, 17]          26,129\n",
      "        Identity-456                   [-1, 17]               0\n",
      "  ClassifierHead-457                   [-1, 17]               0\n",
      "================================================================\n",
      "Total params: 194,926,289\n",
      "Trainable params: 194,926,289\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 959.27\n",
      "Params size (MB): 743.58\n",
      "Estimated Total Size (MB): 1703.43\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4612: 100%|██████████| 40/40 [00:20<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.0722, Train Acc: 0.6481, Train F1: 0.6209\n",
      "Val Loss: 0.3306, Val Acc: 0.8631, Val F1: 0.8431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1875: 100%|██████████| 40/40 [00:20<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.3223, Train Acc: 0.8814, Train F1: 0.8692\n",
      "Val Loss: 0.2665, Val Acc: 0.8981, Val F1: 0.8756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0757: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.2419, Train Acc: 0.9061, Train F1: 0.8955\n",
      "Val Loss: 0.2920, Val Acc: 0.8758, Val F1: 0.8568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0435: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1863, Train Acc: 0.9307, Train F1: 0.9238\n",
      "Val Loss: 0.2198, Val Acc: 0.9172, Val F1: 0.8998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0631: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.1419, Train Acc: 0.9451, Train F1: 0.9424\n",
      "Val Loss: 0.2341, Val Acc: 0.9172, Val F1: 0.9120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0065: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.1203, Train Acc: 0.9506, Train F1: 0.9470\n",
      "Val Loss: 0.2091, Val Acc: 0.9363, Val F1: 0.9265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0180: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.1162, Train Acc: 0.9530, Train F1: 0.9516\n",
      "Val Loss: 0.2048, Val Acc: 0.9268, Val F1: 0.9151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0998, Train Acc: 0.9602, Train F1: 0.9608\n",
      "Val Loss: 0.2000, Val Acc: 0.9363, Val F1: 0.9220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0749: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0846, Train Acc: 0.9658, Train F1: 0.9626\n",
      "Val Loss: 0.2467, Val Acc: 0.9172, Val F1: 0.9086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0144: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0530, Train Acc: 0.9793, Train F1: 0.9795\n",
      "Val Loss: 0.2025, Val Acc: 0.9331, Val F1: 0.9259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1492: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "Train Loss: 0.0527, Train Acc: 0.9817, Train F1: 0.9815\n",
      "Val Loss: 0.2261, Val Acc: 0.9268, Val F1: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0052: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "Train Loss: 0.0810, Train Acc: 0.9658, Train F1: 0.9652\n",
      "Val Loss: 0.2181, Val Acc: 0.9236, Val F1: 0.9159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "Train Loss: 0.0455, Train Acc: 0.9825, Train F1: 0.9830\n",
      "Val Loss: 0.2441, Val Acc: 0.9363, Val F1: 0.9295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0008: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "Train Loss: 0.0300, Train Acc: 0.9896, Train F1: 0.9893\n",
      "Val Loss: 0.2459, Val Acc: 0.9204, Val F1: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "Train Loss: 0.0431, Train Acc: 0.9857, Train F1: 0.9840\n",
      "Val Loss: 0.3954, Val Acc: 0.8917, Val F1: 0.8851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0023: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "Train Loss: 0.0323, Train Acc: 0.9920, Train F1: 0.9909\n",
      "Val Loss: 0.2798, Val Acc: 0.9268, Val F1: 0.9196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0013: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "Train Loss: 0.0217, Train Acc: 0.9912, Train F1: 0.9901\n",
      "Val Loss: 0.2950, Val Acc: 0.9299, Val F1: 0.9231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0069: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "Train Loss: 0.0322, Train Acc: 0.9849, Train F1: 0.9842\n",
      "Val Loss: 0.2850, Val Acc: 0.9172, Val F1: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0016: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "Train Loss: 0.0200, Train Acc: 0.9952, Train F1: 0.9956\n",
      "Val Loss: 0.2581, Val Acc: 0.9299, Val F1: 0.9223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "Train Loss: 0.0074, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.2754, Val Acc: 0.9331, Val F1: 0.9269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0015: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "Train Loss: 0.0083, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.2848, Val Acc: 0.9363, Val F1: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "Train Loss: 0.0053, Train Acc: 0.9992, Train F1: 0.9989\n",
      "Val Loss: 0.2906, Val Acc: 0.9299, Val F1: 0.9244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "Train Loss: 0.0047, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2852, Val Acc: 0.9331, Val F1: 0.9282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0016: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "Train Loss: 0.0048, Train Acc: 0.9984, Train F1: 0.9982\n",
      "Val Loss: 0.2906, Val Acc: 0.9331, Val F1: 0.9273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0030: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "Train Loss: 0.0045, Train Acc: 0.9984, Train F1: 0.9979\n",
      "Val Loss: 0.2823, Val Acc: 0.9363, Val F1: 0.9301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0057: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "Train Loss: 0.0043, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.2812, Val Acc: 0.9363, Val F1: 0.9310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "Train Loss: 0.0027, Train Acc: 0.9992, Train F1: 0.9989\n",
      "Val Loss: 0.2809, Val Acc: 0.9363, Val F1: 0.9310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "Train Loss: 0.0045, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.2802, Val Acc: 0.9363, Val F1: 0.9310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0023: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "Train Loss: 0.0032, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2809, Val Acc: 0.9395, Val F1: 0.9342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "Train Loss: 0.0024, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.2809, Val Acc: 0.9395, Val F1: 0.9342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:18<00:00,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'swin_large_patch4_window7_224'  # Swin Transformer Large 모델\n",
    "    img_size = 224  # Swin Transformer에 적합한 이미지 크기\n",
    "    LR = 1e-4  # 학습률 조정\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32  # 배치 크기 조정\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"swin_t_model.pth\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"swin_t_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred_swin.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred_swin.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin-T clustering & classification\n",
    "- 이미지를 유사한 이미지로 5개로 그룹핑 하고 분석하는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  65%|██████▍   | 1015/1570 [03:49<02:08,  4.33it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Swin-B 모델 로드\n",
    "def load_swin_b_model(num_classes=None):\n",
    "    model = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "# 특성 추출 함수\n",
    "def extract_features(img_path, model):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = np.array(img)\n",
    "    img = transform(image=img)['image']\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = model.forward_features(img)\n",
    "    return features.squeeze().numpy()\n",
    "\n",
    "# 이미지 클러스터링 함수\n",
    "def cluster_images(data_path, n_clusters=5):\n",
    "    feature_extractor = load_swin_b_model(num_classes=None)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    image_files = [f for f in os.listdir(data_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    features = []\n",
    "    for img_file in tqdm(image_files, desc=\"Extracting features\"):\n",
    "        img_path = os.path.join(data_path, img_file)\n",
    "        feature = extract_features(img_path, feature_extractor)\n",
    "        features.append(feature.reshape(-1))  # Flatten the feature array\n",
    "    \n",
    "    features = np.array(features)\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "    return dict(zip(image_files, clusters))\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None, cluster_dict=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.cluster_dict = cluster_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        \n",
    "        cluster = self.cluster_dict.get(name, -1) if self.cluster_dict else -1\n",
    "        return img, target, cluster\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets, _ in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets, _ in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    img_size = 224\n",
    "    LR = 2e-5\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32\n",
    "    num_workers = 4\n",
    "    n_clusters = 3\n",
    "\n",
    "    # 클러스터링 수행\n",
    "    print(\"Clustering images...\")\n",
    "    cluster_dict = cluster_images(os.path.join(data_path, \"train_preprocessed/\"), n_clusters)\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform, cluster_dict=cluster_dict)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform, cluster_dict=cluster_dict)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 각 클러스터에 대한 모델 학습\n",
    "    for cluster in range(n_clusters):\n",
    "        print(f\"\\nTraining model for cluster {cluster}\")\n",
    "        \n",
    "        # 클러스터에 해당하는 데이터만 선택\n",
    "        train_cluster = [data for data in train_dataset if data[2] == cluster]\n",
    "        val_cluster = [data for data in val_dataset if data[2] == cluster]\n",
    "        \n",
    "        if len(train_cluster) == 0 or len(val_cluster) == 0:\n",
    "            print(f\"Skipping cluster {cluster} due to insufficient data\")\n",
    "            continue\n",
    "        \n",
    "        train_cluster_loader = DataLoader(train_cluster, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        val_cluster_loader = DataLoader(val_cluster, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        # Swin-B 모델 설정\n",
    "        model = load_swin_b_model(num_classes=17).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "        # Early stopping 설정\n",
    "        early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "        # 모델 구조 출력\n",
    "        print(f\"\\nModel structure of Swin-B for cluster {cluster}:\")\n",
    "        print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "        # 학습 루프\n",
    "        best_val_f1 = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc, train_f1 = train_one_epoch(train_cluster_loader, model, optimizer, loss_fn, device)\n",
    "            val_loss, val_acc, val_f1 = validate(val_cluster_loader, model, loss_fn, device)\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f\"swin_b_model_cluster_{cluster}.pth\")\n",
    "\n",
    "            # Early stopping 체크\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    print(\"\\nPerforming inference on test data\")\n",
    "    test_preds = []\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        model = load_swin_b_model(num_classes=17).to(device)\n",
    "        model.load_state_dict(torch.load(f\"swin_b_model_cluster_{cluster}.pth\"))\n",
    "        model.eval()\n",
    "        \n",
    "        cluster_preds = []\n",
    "        for image, _, _ in tqdm(test_loader, desc=f\"Predicting cluster {cluster}\"):\n",
    "            image = image.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(image)\n",
    "            cluster_preds.extend(preds.detach().cpu().numpy())\n",
    "        \n",
    "        test_preds.append(cluster_preds)\n",
    "    \n",
    "    # 모든 클러스터의 예측을 결합\n",
    "    final_preds = np.mean(test_preds, axis=0)\n",
    "    final_preds = np.argmax(final_preds, axis=1)\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = final_preds\n",
    "    pred_df.to_csv(\"swin_pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블\n",
    "CNN 모델(ConvNeXt V2 Large)과 Transformer 모델(ViT Large)을 결합한 앙상블 모델\n",
    "- CNN 모델로 'convnextv2_large'를 사용합니다.\n",
    "- Transformer 모델로 'vit_large_patch16_224'를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convnext v2 + vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 앙상블 모델 클래스 정의\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.model1(x)\n",
    "        out2 = self.model2(x)\n",
    "        return (out1 + out2) / 2\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7253ae247a46f0ad2e8867cbe77b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4874: 100%|██████████| 79/79 [01:03<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.2447, Train Acc: 0.6210, Train F1: 0.5802\n",
      "Val Loss: 0.6365, Val Acc: 0.7930, Val F1: 0.7082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4748: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.4485, Train Acc: 0.8432, Train F1: 0.8210\n",
      "Val Loss: 0.3519, Val Acc: 0.8535, Val F1: 0.8308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0189: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.3265, Train Acc: 0.8734, Train F1: 0.8555\n",
      "Val Loss: 0.3140, Val Acc: 0.8854, Val F1: 0.8341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2191: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.2187, Train Acc: 0.9037, Train F1: 0.8929\n",
      "Val Loss: 0.2905, Val Acc: 0.8949, Val F1: 0.8817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4078: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.2068, Train Acc: 0.9236, Train F1: 0.9159\n",
      "Val Loss: 0.3957, Val Acc: 0.8726, Val F1: 0.8509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2037: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.2214, Train Acc: 0.9132, Train F1: 0.9023\n",
      "Val Loss: 0.3843, Val Acc: 0.8790, Val F1: 0.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0819: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.2072, Train Acc: 0.9204, Train F1: 0.9188\n",
      "Val Loss: 0.2120, Val Acc: 0.9268, Val F1: 0.9185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1178: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.1417, Train Acc: 0.9371, Train F1: 0.9341\n",
      "Val Loss: 0.2074, Val Acc: 0.9268, Val F1: 0.9205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0011: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.1199, Train Acc: 0.9538, Train F1: 0.9516\n",
      "Val Loss: 0.1952, Val Acc: 0.9236, Val F1: 0.9202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0478: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.1205, Train Acc: 0.9554, Train F1: 0.9560\n",
      "Val Loss: 0.1598, Val Acc: 0.9331, Val F1: 0.9301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "Train Loss: 0.0810, Train Acc: 0.9682, Train F1: 0.9671\n",
      "Val Loss: 0.2216, Val Acc: 0.9172, Val F1: 0.9040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0742: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "Train Loss: 0.0840, Train Acc: 0.9689, Train F1: 0.9682\n",
      "Val Loss: 0.1806, Val Acc: 0.9459, Val F1: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0168: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "Train Loss: 0.0546, Train Acc: 0.9793, Train F1: 0.9786\n",
      "Val Loss: 0.1571, Val Acc: 0.9554, Val F1: 0.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "Train Loss: 0.0565, Train Acc: 0.9809, Train F1: 0.9805\n",
      "Val Loss: 0.1615, Val Acc: 0.9331, Val F1: 0.9276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "Train Loss: 0.0368, Train Acc: 0.9857, Train F1: 0.9849\n",
      "Val Loss: 0.2248, Val Acc: 0.9268, Val F1: 0.9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3899: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "Train Loss: 0.0380, Train Acc: 0.9873, Train F1: 0.9871\n",
      "Val Loss: 0.1591, Val Acc: 0.9427, Val F1: 0.9399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0028: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "Train Loss: 0.0209, Train Acc: 0.9944, Train F1: 0.9949\n",
      "Val Loss: 0.1868, Val Acc: 0.9363, Val F1: 0.9316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4257: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "Train Loss: 0.0212, Train Acc: 0.9936, Train F1: 0.9934\n",
      "Val Loss: 0.1774, Val Acc: 0.9395, Val F1: 0.9348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0012: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "Train Loss: 0.0289, Train Acc: 0.9896, Train F1: 0.9892\n",
      "Val Loss: 0.1960, Val Acc: 0.9427, Val F1: 0.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "Train Loss: 0.0120, Train Acc: 0.9976, Train F1: 0.9978\n",
      "Val Loss: 0.1968, Val Acc: 0.9268, Val F1: 0.9216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0600: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "Train Loss: 0.0219, Train Acc: 0.9920, Train F1: 0.9918\n",
      "Val Loss: 0.1881, Val Acc: 0.9395, Val F1: 0.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "Train Loss: 0.0185, Train Acc: 0.9944, Train F1: 0.9945\n",
      "Val Loss: 0.1885, Val Acc: 0.9427, Val F1: 0.9389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0012: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "Train Loss: 0.0113, Train Acc: 0.9968, Train F1: 0.9967\n",
      "Val Loss: 0.2007, Val Acc: 0.9363, Val F1: 0.9315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0018: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "Train Loss: 0.0130, Train Acc: 0.9944, Train F1: 0.9945\n",
      "Val Loss: 0.2242, Val Acc: 0.9363, Val F1: 0.9308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0013: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "Train Loss: 0.0063, Train Acc: 0.9984, Train F1: 0.9978\n",
      "Val Loss: 0.1974, Val Acc: 0.9363, Val F1: 0.9305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0042: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "Train Loss: 0.0088, Train Acc: 0.9984, Train F1: 0.9982\n",
      "Val Loss: 0.1919, Val Acc: 0.9427, Val F1: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0152: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "Train Loss: 0.0079, Train Acc: 0.9992, Train F1: 0.9993\n",
      "Val Loss: 0.1917, Val Acc: 0.9459, Val F1: 0.9416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "Train Loss: 0.0062, Train Acc: 0.9984, Train F1: 0.9985\n",
      "Val Loss: 0.1923, Val Acc: 0.9427, Val F1: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0033: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "Train Loss: 0.0037, Train Acc: 0.9992, Train F1: 0.9989\n",
      "Val Loss: 0.1917, Val Acc: 0.9427, Val F1: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 79/79 [01:04<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "Train Loss: 0.0050, Train Acc: 0.9976, Train F1: 0.9967\n",
      "Val Loss: 0.1915, Val Acc: 0.9427, Val F1: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:46<00:00,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to ensemble_pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '../data/'\n",
    "img_size = 224\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "num_workers = 4\n",
    "\n",
    "# 데이터 증강 설정\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터 로드 및 분할\n",
    "df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 모델 설정\n",
    "model1 = timm.create_model('convnextv2_large', pretrained=True, num_classes=17)\n",
    "model2 = timm.create_model('vit_large_patch16_224', pretrained=True, num_classes=17)\n",
    "\n",
    "ensemble_model = EnsembleModel(model1, model2).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(ensemble_model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# 학습 루프\n",
    "best_val_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(train_loader, ensemble_model, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc, val_f1 = validate(val_loader, ensemble_model, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(ensemble_model.state_dict(), \"best_ensemble_model.pth\")\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "ensemble_model.load_state_dict(torch.load(\"best_ensemble_model.pth\"))\n",
    "ensemble_model.eval()\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = ensemble_model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"ensemble_pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to ensemble_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 모델 II - 리더 보드 제출용\n",
    "CNN 모델(ConvNeXt V2 Large)과 Transformer 모델(Swin Transformers)을 결합한 앙상블 모델\n",
    "- Hyper Parameter tunning이 전혀 되어 있지 않는 기본 모델 : 향후 최적화 필요\n",
    "- CNN 모델로 'convnextv2_large'를 사용합니다.\n",
    "- Transformer 모델로 'swin_large_patch4_window7_224'를 사용합니다.\n",
    "- software voting(기존 저장된 pth 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:36<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble prediction completed and saved to ensemble_pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 첫 번째 모델과 두 번째 모델 로드\n",
    "model1 = timm.create_model('convnextv2_large', pretrained=False, num_classes=17).to(device)\n",
    "model2 = timm.create_model('swin_large_patch4_window7_224', pretrained=False, num_classes=17).to(device)\n",
    "\n",
    "# 모델 가중치 로드\n",
    "model1.load_state_dict(torch.load('convNext_model.pth'))\n",
    "model2.load_state_dict(torch.load('swin_t_model.pth'))\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 소프트 보팅을 통한 예측\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        preds1 = model1(image)\n",
    "        preds2 = model2(image)\n",
    "        \n",
    "        # 소프트 보팅: 예측 확률의 평균\n",
    "        preds_avg = (torch.softmax(preds1, dim=1) + torch.softmax(preds2, dim=1)) / 2\n",
    "        preds_list.extend(preds_avg.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"ensemble_pred.csv\", index=False)\n",
    "print(\"Ensemble prediction completed and saved to ensemble_pred.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layoutLMv3 모델\n",
    "- 테스트용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "\n",
    "class DocumentDataset(Dataset):\n",
    "    def __init__(self, csv, path, processor, label_encoder=None, max_length=512):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv)\n",
    "        self.path = path\n",
    "        self.processor = processor\n",
    "        self.label_encoder = label_encoder\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        name = row['ID']\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        encoding = self.processor(image, return_tensors=\"pt\", max_length=self.max_length, padding=\"max_length\", truncation=True)\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        \n",
    "        if 'target_encoded' in row:\n",
    "            encoding['labels'] = torch.tensor(row['target_encoded'], dtype=torch.long)\n",
    "        else:\n",
    "            encoding['labels'] = torch.tensor(-1, dtype=torch.long)  # For test set\n",
    "        return encoding\n",
    "\n",
    "class CustomLayoutLMv3(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.layoutlmv3 = LayoutLMv3ForSequenceClassification.from_pretrained(\"microsoft/layoutlmv3-base\", num_labels=num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        outputs = self.layoutlmv3(**inputs)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(**batch)\n",
    "        loss = criterion(logits, batch['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        preds_list.extend(preds.cpu().numpy())\n",
    "        targets_list.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "def validate(loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**batch)\n",
    "            loss = criterion(logits, batch['labels'])\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            preds_list.extend(preds.cpu().numpy())\n",
    "            targets_list.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    LR = 2e-5\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 8\n",
    "    num_workers = 4\n",
    "\n",
    "    processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    \n",
    "    # 레이블 인코딩\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['target_encoded'] = label_encoder.fit_transform(df['target'])\n",
    "    num_labels = len(df['target'].unique())\n",
    "\n",
    "    print(f\"Number of unique classes: {num_labels}\")\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = DocumentDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), processor, label_encoder)\n",
    "    val_dataset = DocumentDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), processor, label_encoder)\n",
    "    test_dataset = DocumentDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), processor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    model = CustomLayoutLMv3(num_labels=num_labels)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, criterion, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, criterion, device)\n",
    "        scheduler.step(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            logits = model(**batch)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            preds_list.extend(preds.cpu().numpy())\n",
    "\n",
    "    pred_df = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n",
    "    pred_df['target'] = label_encoder.inverse_transform(preds_list)\n",
    "    pred_df.to_csv(\"pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "background 실행을 위해 .py 파일로 분리\n",
    "- image_text.py\n",
    "- image_bert.py\n",
    "- layoutlmv3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3가지 모델 앙상블"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

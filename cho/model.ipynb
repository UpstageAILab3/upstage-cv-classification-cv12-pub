{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 기반 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple EfficientNet-B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '../data/'\n",
    "model_name = 'efficientnet_b0'\n",
    "img_size = 384\n",
    "LR = 1e-3\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 4\n",
    "\n",
    "# 데이터 증강 설정\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 분할\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "train_dataset = ImageDataset(train_df, \"../data/train_preprocessed/\", transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, \"../data/train_preprocessed/\", transform=val_transform)\n",
    "test_dataset = ImageDataset(\"../data/sample_submission.csv\", \"../data/test_preprocessed/\", transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "    \n",
    "# 모델 구조 출력\n",
    "print(f\"\\nModel structure of {model_name}:\")\n",
    "print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "# 모델 아키텍처 출력\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "best_val_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 추론\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet-B4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((380, 380, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'efficientnet_b4'\n",
    "    img_size = 380  # EfficientNet-B4에 적합한 이미지 크기\n",
    "    LR = 5e-4  # 학습률 조정\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 16  # 배치 크기 감소\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNext V2 Large 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timm\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        self.best_f1 = -np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, f1_score, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, f1_score, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        self.best_f1 = f1_score\n",
    "\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'convnextv2_large'\n",
    "    img_size = 224  # ConvNeXt V2 Large에 적합한 이미지 크기\n",
    "    LR = 1e-4  # 학습률 조정\n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 32  # 배치 크기 조정\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"augmented_train.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"augmented_train/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"augmented_train/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    early_stopping = EarlyStopping(patience=7, verbose=True, delta=0.001, path='aug_conv_model.pth')\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    # print(f\"\\nModel structure of {model_name}:\")\n",
    "    # print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"aug_conv_model.pth\")\n",
    "        \n",
    "        early_stopping(val_loss, val_f1, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping. Best validation loss: {early_stopping.val_loss_min:.6f}, \"\n",
    "                  f\"Best F1 score: {early_stopping.best_f1:.6f}\")\n",
    "            break\n",
    "\n",
    "    # 최종 모델 저장\n",
    "    torch.save(model.state_dict(), \"aug_conv_model_final.pth\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"aug_conv_model_final.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"conv_pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convNext v2 + fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model structure of convnextv2_large:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 192, 56, 56]           9,408\n",
      "       LayerNorm2d-2          [-1, 192, 56, 56]             384\n",
      "          Identity-3          [-1, 192, 56, 56]               0\n",
      "            Conv2d-4          [-1, 192, 56, 56]           9,600\n",
      "         LayerNorm-5          [-1, 56, 56, 192]             384\n",
      "            Linear-6          [-1, 56, 56, 768]         148,224\n",
      "              GELU-7          [-1, 56, 56, 768]               0\n",
      "           Dropout-8          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-9          [-1, 56, 56, 768]           1,536\n",
      "           Linear-10          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-11          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-12          [-1, 56, 56, 192]               0\n",
      "         Identity-13          [-1, 192, 56, 56]               0\n",
      "         Identity-14          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-15          [-1, 192, 56, 56]               0\n",
      "           Conv2d-16          [-1, 192, 56, 56]           9,600\n",
      "        LayerNorm-17          [-1, 56, 56, 192]             384\n",
      "           Linear-18          [-1, 56, 56, 768]         148,224\n",
      "             GELU-19          [-1, 56, 56, 768]               0\n",
      "          Dropout-20          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-21          [-1, 56, 56, 768]           1,536\n",
      "           Linear-22          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-23          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-24          [-1, 56, 56, 192]               0\n",
      "         Identity-25          [-1, 192, 56, 56]               0\n",
      "         Identity-26          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-27          [-1, 192, 56, 56]               0\n",
      "           Conv2d-28          [-1, 192, 56, 56]           9,600\n",
      "        LayerNorm-29          [-1, 56, 56, 192]             384\n",
      "           Linear-30          [-1, 56, 56, 768]         148,224\n",
      "             GELU-31          [-1, 56, 56, 768]               0\n",
      "          Dropout-32          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-33          [-1, 56, 56, 768]           1,536\n",
      "           Linear-34          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-35          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-36          [-1, 56, 56, 192]               0\n",
      "         Identity-37          [-1, 192, 56, 56]               0\n",
      "         Identity-38          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-39          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtStage-40          [-1, 192, 56, 56]               0\n",
      "      LayerNorm2d-41          [-1, 192, 56, 56]             384\n",
      "           Conv2d-42          [-1, 384, 28, 28]         295,296\n",
      "           Conv2d-43          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-44          [-1, 28, 28, 384]             768\n",
      "           Linear-45         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-46         [-1, 28, 28, 1536]               0\n",
      "          Dropout-47         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-48         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-49          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-50          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-51          [-1, 28, 28, 384]               0\n",
      "         Identity-52          [-1, 384, 28, 28]               0\n",
      "         Identity-53          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-54          [-1, 384, 28, 28]               0\n",
      "           Conv2d-55          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-56          [-1, 28, 28, 384]             768\n",
      "           Linear-57         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-58         [-1, 28, 28, 1536]               0\n",
      "          Dropout-59         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-60         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-61          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-62          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-63          [-1, 28, 28, 384]               0\n",
      "         Identity-64          [-1, 384, 28, 28]               0\n",
      "         Identity-65          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-66          [-1, 384, 28, 28]               0\n",
      "           Conv2d-67          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-68          [-1, 28, 28, 384]             768\n",
      "           Linear-69         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-70         [-1, 28, 28, 1536]               0\n",
      "          Dropout-71         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-72         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-73          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-74          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-75          [-1, 28, 28, 384]               0\n",
      "         Identity-76          [-1, 384, 28, 28]               0\n",
      "         Identity-77          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-78          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtStage-79          [-1, 384, 28, 28]               0\n",
      "      LayerNorm2d-80          [-1, 384, 28, 28]             768\n",
      "           Conv2d-81          [-1, 768, 14, 14]       1,180,416\n",
      "           Conv2d-82          [-1, 768, 14, 14]          38,400\n",
      "        LayerNorm-83          [-1, 14, 14, 768]           1,536\n",
      "           Linear-84         [-1, 14, 14, 3072]       2,362,368\n",
      "             GELU-85         [-1, 14, 14, 3072]               0\n",
      "          Dropout-86         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-87         [-1, 14, 14, 3072]           6,144\n",
      "           Linear-88          [-1, 14, 14, 768]       2,360,064\n",
      "          Dropout-89          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-90          [-1, 14, 14, 768]               0\n",
      "         Identity-91          [-1, 768, 14, 14]               0\n",
      "         Identity-92          [-1, 768, 14, 14]               0\n",
      "    ConvNeXtBlock-93          [-1, 768, 14, 14]               0\n",
      "           Conv2d-94          [-1, 768, 14, 14]          38,400\n",
      "        LayerNorm-95          [-1, 14, 14, 768]           1,536\n",
      "           Linear-96         [-1, 14, 14, 3072]       2,362,368\n",
      "             GELU-97         [-1, 14, 14, 3072]               0\n",
      "          Dropout-98         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-99         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-100          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-101          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-102          [-1, 14, 14, 768]               0\n",
      "        Identity-103          [-1, 768, 14, 14]               0\n",
      "        Identity-104          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-105          [-1, 768, 14, 14]               0\n",
      "          Conv2d-106          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-107          [-1, 14, 14, 768]           1,536\n",
      "          Linear-108         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-109         [-1, 14, 14, 3072]               0\n",
      "         Dropout-110         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-111         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-112          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-113          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-114          [-1, 14, 14, 768]               0\n",
      "        Identity-115          [-1, 768, 14, 14]               0\n",
      "        Identity-116          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-117          [-1, 768, 14, 14]               0\n",
      "          Conv2d-118          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-119          [-1, 14, 14, 768]           1,536\n",
      "          Linear-120         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-121         [-1, 14, 14, 3072]               0\n",
      "         Dropout-122         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-123         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-124          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-125          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-126          [-1, 14, 14, 768]               0\n",
      "        Identity-127          [-1, 768, 14, 14]               0\n",
      "        Identity-128          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-129          [-1, 768, 14, 14]               0\n",
      "          Conv2d-130          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-131          [-1, 14, 14, 768]           1,536\n",
      "          Linear-132         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-133         [-1, 14, 14, 3072]               0\n",
      "         Dropout-134         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-135         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-136          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-137          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-138          [-1, 14, 14, 768]               0\n",
      "        Identity-139          [-1, 768, 14, 14]               0\n",
      "        Identity-140          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-141          [-1, 768, 14, 14]               0\n",
      "          Conv2d-142          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-143          [-1, 14, 14, 768]           1,536\n",
      "          Linear-144         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-145         [-1, 14, 14, 3072]               0\n",
      "         Dropout-146         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-147         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-148          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-149          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-150          [-1, 14, 14, 768]               0\n",
      "        Identity-151          [-1, 768, 14, 14]               0\n",
      "        Identity-152          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-153          [-1, 768, 14, 14]               0\n",
      "          Conv2d-154          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-155          [-1, 14, 14, 768]           1,536\n",
      "          Linear-156         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-157         [-1, 14, 14, 3072]               0\n",
      "         Dropout-158         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-159         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-160          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-161          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-162          [-1, 14, 14, 768]               0\n",
      "        Identity-163          [-1, 768, 14, 14]               0\n",
      "        Identity-164          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-165          [-1, 768, 14, 14]               0\n",
      "          Conv2d-166          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-167          [-1, 14, 14, 768]           1,536\n",
      "          Linear-168         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-169         [-1, 14, 14, 3072]               0\n",
      "         Dropout-170         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-171         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-172          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-173          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-174          [-1, 14, 14, 768]               0\n",
      "        Identity-175          [-1, 768, 14, 14]               0\n",
      "        Identity-176          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-177          [-1, 768, 14, 14]               0\n",
      "          Conv2d-178          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-179          [-1, 14, 14, 768]           1,536\n",
      "          Linear-180         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-181         [-1, 14, 14, 3072]               0\n",
      "         Dropout-182         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-183         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-184          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-185          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-186          [-1, 14, 14, 768]               0\n",
      "        Identity-187          [-1, 768, 14, 14]               0\n",
      "        Identity-188          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-189          [-1, 768, 14, 14]               0\n",
      "          Conv2d-190          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-191          [-1, 14, 14, 768]           1,536\n",
      "          Linear-192         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-193         [-1, 14, 14, 3072]               0\n",
      "         Dropout-194         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-195         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-196          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-197          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-198          [-1, 14, 14, 768]               0\n",
      "        Identity-199          [-1, 768, 14, 14]               0\n",
      "        Identity-200          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-201          [-1, 768, 14, 14]               0\n",
      "          Conv2d-202          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-203          [-1, 14, 14, 768]           1,536\n",
      "          Linear-204         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-205         [-1, 14, 14, 3072]               0\n",
      "         Dropout-206         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-207         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-208          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-209          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-210          [-1, 14, 14, 768]               0\n",
      "        Identity-211          [-1, 768, 14, 14]               0\n",
      "        Identity-212          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-213          [-1, 768, 14, 14]               0\n",
      "          Conv2d-214          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-215          [-1, 14, 14, 768]           1,536\n",
      "          Linear-216         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-217         [-1, 14, 14, 3072]               0\n",
      "         Dropout-218         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-219         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-220          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-221          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-222          [-1, 14, 14, 768]               0\n",
      "        Identity-223          [-1, 768, 14, 14]               0\n",
      "        Identity-224          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-225          [-1, 768, 14, 14]               0\n",
      "          Conv2d-226          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-227          [-1, 14, 14, 768]           1,536\n",
      "          Linear-228         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-229         [-1, 14, 14, 3072]               0\n",
      "         Dropout-230         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-231         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-232          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-233          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-234          [-1, 14, 14, 768]               0\n",
      "        Identity-235          [-1, 768, 14, 14]               0\n",
      "        Identity-236          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-237          [-1, 768, 14, 14]               0\n",
      "          Conv2d-238          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-239          [-1, 14, 14, 768]           1,536\n",
      "          Linear-240         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-241         [-1, 14, 14, 3072]               0\n",
      "         Dropout-242         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-243         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-244          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-245          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-246          [-1, 14, 14, 768]               0\n",
      "        Identity-247          [-1, 768, 14, 14]               0\n",
      "        Identity-248          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-249          [-1, 768, 14, 14]               0\n",
      "          Conv2d-250          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-251          [-1, 14, 14, 768]           1,536\n",
      "          Linear-252         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-253         [-1, 14, 14, 3072]               0\n",
      "         Dropout-254         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-255         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-256          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-257          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-258          [-1, 14, 14, 768]               0\n",
      "        Identity-259          [-1, 768, 14, 14]               0\n",
      "        Identity-260          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-261          [-1, 768, 14, 14]               0\n",
      "          Conv2d-262          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-263          [-1, 14, 14, 768]           1,536\n",
      "          Linear-264         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-265         [-1, 14, 14, 3072]               0\n",
      "         Dropout-266         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-267         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-268          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-269          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-270          [-1, 14, 14, 768]               0\n",
      "        Identity-271          [-1, 768, 14, 14]               0\n",
      "        Identity-272          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-273          [-1, 768, 14, 14]               0\n",
      "          Conv2d-274          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-275          [-1, 14, 14, 768]           1,536\n",
      "          Linear-276         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-277         [-1, 14, 14, 3072]               0\n",
      "         Dropout-278         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-279         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-280          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-281          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-282          [-1, 14, 14, 768]               0\n",
      "        Identity-283          [-1, 768, 14, 14]               0\n",
      "        Identity-284          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-285          [-1, 768, 14, 14]               0\n",
      "          Conv2d-286          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-287          [-1, 14, 14, 768]           1,536\n",
      "          Linear-288         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-289         [-1, 14, 14, 3072]               0\n",
      "         Dropout-290         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-291         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-292          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-293          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-294          [-1, 14, 14, 768]               0\n",
      "        Identity-295          [-1, 768, 14, 14]               0\n",
      "        Identity-296          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-297          [-1, 768, 14, 14]               0\n",
      "          Conv2d-298          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-299          [-1, 14, 14, 768]           1,536\n",
      "          Linear-300         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-301         [-1, 14, 14, 3072]               0\n",
      "         Dropout-302         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-303         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-304          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-305          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-306          [-1, 14, 14, 768]               0\n",
      "        Identity-307          [-1, 768, 14, 14]               0\n",
      "        Identity-308          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-309          [-1, 768, 14, 14]               0\n",
      "          Conv2d-310          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-311          [-1, 14, 14, 768]           1,536\n",
      "          Linear-312         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-313         [-1, 14, 14, 3072]               0\n",
      "         Dropout-314         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-315         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-316          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-317          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-318          [-1, 14, 14, 768]               0\n",
      "        Identity-319          [-1, 768, 14, 14]               0\n",
      "        Identity-320          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-321          [-1, 768, 14, 14]               0\n",
      "          Conv2d-322          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-323          [-1, 14, 14, 768]           1,536\n",
      "          Linear-324         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-325         [-1, 14, 14, 3072]               0\n",
      "         Dropout-326         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-327         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-328          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-329          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-330          [-1, 14, 14, 768]               0\n",
      "        Identity-331          [-1, 768, 14, 14]               0\n",
      "        Identity-332          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-333          [-1, 768, 14, 14]               0\n",
      "          Conv2d-334          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-335          [-1, 14, 14, 768]           1,536\n",
      "          Linear-336         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-337         [-1, 14, 14, 3072]               0\n",
      "         Dropout-338         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-339         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-340          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-341          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-342          [-1, 14, 14, 768]               0\n",
      "        Identity-343          [-1, 768, 14, 14]               0\n",
      "        Identity-344          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-345          [-1, 768, 14, 14]               0\n",
      "          Conv2d-346          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-347          [-1, 14, 14, 768]           1,536\n",
      "          Linear-348         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-349         [-1, 14, 14, 3072]               0\n",
      "         Dropout-350         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-351         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-352          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-353          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-354          [-1, 14, 14, 768]               0\n",
      "        Identity-355          [-1, 768, 14, 14]               0\n",
      "        Identity-356          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-357          [-1, 768, 14, 14]               0\n",
      "          Conv2d-358          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-359          [-1, 14, 14, 768]           1,536\n",
      "          Linear-360         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-361         [-1, 14, 14, 3072]               0\n",
      "         Dropout-362         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-363         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-364          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-365          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-366          [-1, 14, 14, 768]               0\n",
      "        Identity-367          [-1, 768, 14, 14]               0\n",
      "        Identity-368          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-369          [-1, 768, 14, 14]               0\n",
      "          Conv2d-370          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-371          [-1, 14, 14, 768]           1,536\n",
      "          Linear-372         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-373         [-1, 14, 14, 3072]               0\n",
      "         Dropout-374         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-375         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-376          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-377          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-378          [-1, 14, 14, 768]               0\n",
      "        Identity-379          [-1, 768, 14, 14]               0\n",
      "        Identity-380          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-381          [-1, 768, 14, 14]               0\n",
      "          Conv2d-382          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-383          [-1, 14, 14, 768]           1,536\n",
      "          Linear-384         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-385         [-1, 14, 14, 3072]               0\n",
      "         Dropout-386         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-387         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-388          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-389          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-390          [-1, 14, 14, 768]               0\n",
      "        Identity-391          [-1, 768, 14, 14]               0\n",
      "        Identity-392          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-393          [-1, 768, 14, 14]               0\n",
      "          Conv2d-394          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-395          [-1, 14, 14, 768]           1,536\n",
      "          Linear-396         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-397         [-1, 14, 14, 3072]               0\n",
      "         Dropout-398         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-399         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-400          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-401          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-402          [-1, 14, 14, 768]               0\n",
      "        Identity-403          [-1, 768, 14, 14]               0\n",
      "        Identity-404          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-405          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtStage-406          [-1, 768, 14, 14]               0\n",
      "     LayerNorm2d-407          [-1, 768, 14, 14]           1,536\n",
      "          Conv2d-408           [-1, 1536, 7, 7]       4,720,128\n",
      "          Conv2d-409           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-410           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-411           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-412           [-1, 7, 7, 6144]               0\n",
      "         Dropout-413           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-414           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-415           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-416           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-417           [-1, 7, 7, 1536]               0\n",
      "        Identity-418           [-1, 1536, 7, 7]               0\n",
      "        Identity-419           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-420           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-421           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-422           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-423           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-424           [-1, 7, 7, 6144]               0\n",
      "         Dropout-425           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-426           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-427           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-428           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-429           [-1, 7, 7, 1536]               0\n",
      "        Identity-430           [-1, 1536, 7, 7]               0\n",
      "        Identity-431           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-432           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-433           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-434           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-435           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-436           [-1, 7, 7, 6144]               0\n",
      "         Dropout-437           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-438           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-439           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-440           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-441           [-1, 7, 7, 1536]               0\n",
      "        Identity-442           [-1, 1536, 7, 7]               0\n",
      "        Identity-443           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-444           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtStage-445           [-1, 1536, 7, 7]               0\n",
      "        Identity-446           [-1, 1536, 7, 7]               0\n",
      "AdaptiveAvgPool2d-447           [-1, 1536, 1, 1]               0\n",
      "        Identity-448           [-1, 1536, 1, 1]               0\n",
      "SelectAdaptivePool2d-449           [-1, 1536, 1, 1]               0\n",
      "     LayerNorm2d-450           [-1, 1536, 1, 1]           3,072\n",
      "         Flatten-451                 [-1, 1536]               0\n",
      "        Identity-452                 [-1, 1536]               0\n",
      "         Dropout-453                 [-1, 1536]               0\n",
      "          Linear-454                   [-1, 17]          26,129\n",
      "NormMlpClassifierHead-455                   [-1, 17]               0\n",
      "================================================================\n",
      "Total params: 196,445,969\n",
      "Trainable params: 196,445,969\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 1316.77\n",
      "Params size (MB): 749.38\n",
      "Estimated Total Size (MB): 2066.72\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2666: 100%|██████████| 40/40 [00:29<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.9142, Train Acc: 0.7142, Train F1: 0.6951\n",
      "Val Loss: 0.3429, Val Acc: 0.8758, Val F1: 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0162: 100%|██████████| 40/40 [00:29<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.2789, Train Acc: 0.8941, Train F1: 0.8816\n",
      "Val Loss: 0.3312, Val Acc: 0.8662, Val F1: 0.8275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0052: 100%|██████████| 40/40 [00:29<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.2074, Train Acc: 0.9188, Train F1: 0.9083\n",
      "Val Loss: 0.2308, Val Acc: 0.9204, Val F1: 0.9089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5555: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1710, Train Acc: 0.9347, Train F1: 0.9280\n",
      "Val Loss: 0.2320, Val Acc: 0.9140, Val F1: 0.9007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2525: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.1384, Train Acc: 0.9443, Train F1: 0.9392\n",
      "Val Loss: 0.1470, Val Acc: 0.9459, Val F1: 0.9418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0816: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0985, Train Acc: 0.9642, Train F1: 0.9626\n",
      "Val Loss: 0.2139, Val Acc: 0.9268, Val F1: 0.9174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0811: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0926, Train Acc: 0.9697, Train F1: 0.9683\n",
      "Val Loss: 0.2329, Val Acc: 0.8981, Val F1: 0.8919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2371: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0625, Train Acc: 0.9809, Train F1: 0.9812\n",
      "Val Loss: 0.1600, Val Acc: 0.9459, Val F1: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0015: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0628, Train Acc: 0.9793, Train F1: 0.9774\n",
      "Val Loss: 0.2908, Val Acc: 0.9045, Val F1: 0.8956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0145: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0458, Train Acc: 0.9785, Train F1: 0.9777\n",
      "Val Loss: 0.2150, Val Acc: 0.9395, Val F1: 0.9314\n",
      "Early stopping triggered\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.2694: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 1/5\n",
      "Train Loss: 2.2694, Train Acc: 0.2632, Train F1: 0.1238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.2706: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 2/5\n",
      "Train Loss: 2.2706, Train Acc: 0.4211, Train F1: 0.3792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8660: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 3/5\n",
      "Train Loss: 0.8660, Train Acc: 0.7368, Train F1: 0.5988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3658: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 4/5\n",
      "Train Loss: 0.3658, Train Acc: 0.8947, Train F1: 0.8084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3274: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 5/5\n",
      "Train Loss: 0.3274, Train Acc: 0.8947, Train F1: 0.8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3086: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 6/5\n",
      "Train Loss: 0.3086, Train Acc: 0.8947, Train F1: 0.8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1315: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 7/5\n",
      "Train Loss: 0.1315, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2255: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 8/5\n",
      "Train Loss: 0.2255, Train Acc: 0.9474, Train F1: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1177: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 9/5\n",
      "Train Loss: 0.1177, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1283: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 10/5\n",
      "Train Loss: 0.1283, Train Acc: 0.9474, Train F1: 0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160351/3141216663.py:227: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"convNext_model_final.pth\"))\n",
      "100%|██████████| 99/99 [00:18<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    misclassified = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (image, targets) in enumerate(loader):\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_np = preds.argmax(dim=1).detach().cpu().numpy()\n",
    "            targets_np = targets.detach().cpu().numpy()\n",
    "            preds_list.extend(preds_np)\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            # 오분류된 데이터의 인덱스 저장\n",
    "            misclassified.extend(np.where(preds_np != targets_np)[0] + i * loader.batch_size)\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1, misclassified\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# EarlyStopping 클래스 정의\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'convnextv2_large'\n",
    "    img_size = 224\n",
    "    LR = 1e-4\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # EarlyStopping 초기화\n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1, misclassified = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"convNext_model.pth\")\n",
    "\n",
    "        # Early Stopping 체크\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # 오분류된 데이터로 fine-tuning\n",
    "    misclassified_df = val_df.iloc[misclassified]\n",
    "    misclassified_dataset = ImageDataset(misclassified_df, os.path.join(data_path, \"train/\"), transform=train_transform)\n",
    "    misclassified_loader = DataLoader(misclassified_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    print(\"\\nFine-tuning with misclassified data\")\n",
    "    for epoch in range(10):  # 10 에폭 동안 fine-tuning\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(misclassified_loader, model, optimizer, loss_fn, device)\n",
    "        print(f\"Fine-tuning Epoch {epoch+1}/5\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "    # 최종 모델 저장\n",
    "    torch.save(model.state_dict(), \"convNext_model_final.pth\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"convNext_model_final.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"conv_fine_pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convNext v2 + new augmentaion + K-Fold finetunning + ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/blur/transforms.py:184: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "\n",
      "Model structure of convnextv2_large:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 192, 56, 56]           9,408\n",
      "       LayerNorm2d-2          [-1, 192, 56, 56]             384\n",
      "          Identity-3          [-1, 192, 56, 56]               0\n",
      "            Conv2d-4          [-1, 192, 56, 56]           9,600\n",
      "         LayerNorm-5          [-1, 56, 56, 192]             384\n",
      "            Linear-6          [-1, 56, 56, 768]         148,224\n",
      "              GELU-7          [-1, 56, 56, 768]               0\n",
      "           Dropout-8          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-9          [-1, 56, 56, 768]           1,536\n",
      "           Linear-10          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-11          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-12          [-1, 56, 56, 192]               0\n",
      "         Identity-13          [-1, 192, 56, 56]               0\n",
      "         Identity-14          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-15          [-1, 192, 56, 56]               0\n",
      "           Conv2d-16          [-1, 192, 56, 56]           9,600\n",
      "        LayerNorm-17          [-1, 56, 56, 192]             384\n",
      "           Linear-18          [-1, 56, 56, 768]         148,224\n",
      "             GELU-19          [-1, 56, 56, 768]               0\n",
      "          Dropout-20          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-21          [-1, 56, 56, 768]           1,536\n",
      "           Linear-22          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-23          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-24          [-1, 56, 56, 192]               0\n",
      "         Identity-25          [-1, 192, 56, 56]               0\n",
      "         Identity-26          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-27          [-1, 192, 56, 56]               0\n",
      "           Conv2d-28          [-1, 192, 56, 56]           9,600\n",
      "        LayerNorm-29          [-1, 56, 56, 192]             384\n",
      "           Linear-30          [-1, 56, 56, 768]         148,224\n",
      "             GELU-31          [-1, 56, 56, 768]               0\n",
      "          Dropout-32          [-1, 56, 56, 768]               0\n",
      "GlobalResponseNorm-33          [-1, 56, 56, 768]           1,536\n",
      "           Linear-34          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-35          [-1, 56, 56, 192]               0\n",
      "GlobalResponseNormMlp-36          [-1, 56, 56, 192]               0\n",
      "         Identity-37          [-1, 192, 56, 56]               0\n",
      "         Identity-38          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtBlock-39          [-1, 192, 56, 56]               0\n",
      "    ConvNeXtStage-40          [-1, 192, 56, 56]               0\n",
      "      LayerNorm2d-41          [-1, 192, 56, 56]             384\n",
      "           Conv2d-42          [-1, 384, 28, 28]         295,296\n",
      "           Conv2d-43          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-44          [-1, 28, 28, 384]             768\n",
      "           Linear-45         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-46         [-1, 28, 28, 1536]               0\n",
      "          Dropout-47         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-48         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-49          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-50          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-51          [-1, 28, 28, 384]               0\n",
      "         Identity-52          [-1, 384, 28, 28]               0\n",
      "         Identity-53          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-54          [-1, 384, 28, 28]               0\n",
      "           Conv2d-55          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-56          [-1, 28, 28, 384]             768\n",
      "           Linear-57         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-58         [-1, 28, 28, 1536]               0\n",
      "          Dropout-59         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-60         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-61          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-62          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-63          [-1, 28, 28, 384]               0\n",
      "         Identity-64          [-1, 384, 28, 28]               0\n",
      "         Identity-65          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-66          [-1, 384, 28, 28]               0\n",
      "           Conv2d-67          [-1, 384, 28, 28]          19,200\n",
      "        LayerNorm-68          [-1, 28, 28, 384]             768\n",
      "           Linear-69         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-70         [-1, 28, 28, 1536]               0\n",
      "          Dropout-71         [-1, 28, 28, 1536]               0\n",
      "GlobalResponseNorm-72         [-1, 28, 28, 1536]           3,072\n",
      "           Linear-73          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-74          [-1, 28, 28, 384]               0\n",
      "GlobalResponseNormMlp-75          [-1, 28, 28, 384]               0\n",
      "         Identity-76          [-1, 384, 28, 28]               0\n",
      "         Identity-77          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtBlock-78          [-1, 384, 28, 28]               0\n",
      "    ConvNeXtStage-79          [-1, 384, 28, 28]               0\n",
      "      LayerNorm2d-80          [-1, 384, 28, 28]             768\n",
      "           Conv2d-81          [-1, 768, 14, 14]       1,180,416\n",
      "           Conv2d-82          [-1, 768, 14, 14]          38,400\n",
      "        LayerNorm-83          [-1, 14, 14, 768]           1,536\n",
      "           Linear-84         [-1, 14, 14, 3072]       2,362,368\n",
      "             GELU-85         [-1, 14, 14, 3072]               0\n",
      "          Dropout-86         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-87         [-1, 14, 14, 3072]           6,144\n",
      "           Linear-88          [-1, 14, 14, 768]       2,360,064\n",
      "          Dropout-89          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-90          [-1, 14, 14, 768]               0\n",
      "         Identity-91          [-1, 768, 14, 14]               0\n",
      "         Identity-92          [-1, 768, 14, 14]               0\n",
      "    ConvNeXtBlock-93          [-1, 768, 14, 14]               0\n",
      "           Conv2d-94          [-1, 768, 14, 14]          38,400\n",
      "        LayerNorm-95          [-1, 14, 14, 768]           1,536\n",
      "           Linear-96         [-1, 14, 14, 3072]       2,362,368\n",
      "             GELU-97         [-1, 14, 14, 3072]               0\n",
      "          Dropout-98         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-99         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-100          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-101          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-102          [-1, 14, 14, 768]               0\n",
      "        Identity-103          [-1, 768, 14, 14]               0\n",
      "        Identity-104          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-105          [-1, 768, 14, 14]               0\n",
      "          Conv2d-106          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-107          [-1, 14, 14, 768]           1,536\n",
      "          Linear-108         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-109         [-1, 14, 14, 3072]               0\n",
      "         Dropout-110         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-111         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-112          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-113          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-114          [-1, 14, 14, 768]               0\n",
      "        Identity-115          [-1, 768, 14, 14]               0\n",
      "        Identity-116          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-117          [-1, 768, 14, 14]               0\n",
      "          Conv2d-118          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-119          [-1, 14, 14, 768]           1,536\n",
      "          Linear-120         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-121         [-1, 14, 14, 3072]               0\n",
      "         Dropout-122         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-123         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-124          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-125          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-126          [-1, 14, 14, 768]               0\n",
      "        Identity-127          [-1, 768, 14, 14]               0\n",
      "        Identity-128          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-129          [-1, 768, 14, 14]               0\n",
      "          Conv2d-130          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-131          [-1, 14, 14, 768]           1,536\n",
      "          Linear-132         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-133         [-1, 14, 14, 3072]               0\n",
      "         Dropout-134         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-135         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-136          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-137          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-138          [-1, 14, 14, 768]               0\n",
      "        Identity-139          [-1, 768, 14, 14]               0\n",
      "        Identity-140          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-141          [-1, 768, 14, 14]               0\n",
      "          Conv2d-142          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-143          [-1, 14, 14, 768]           1,536\n",
      "          Linear-144         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-145         [-1, 14, 14, 3072]               0\n",
      "         Dropout-146         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-147         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-148          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-149          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-150          [-1, 14, 14, 768]               0\n",
      "        Identity-151          [-1, 768, 14, 14]               0\n",
      "        Identity-152          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-153          [-1, 768, 14, 14]               0\n",
      "          Conv2d-154          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-155          [-1, 14, 14, 768]           1,536\n",
      "          Linear-156         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-157         [-1, 14, 14, 3072]               0\n",
      "         Dropout-158         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-159         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-160          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-161          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-162          [-1, 14, 14, 768]               0\n",
      "        Identity-163          [-1, 768, 14, 14]               0\n",
      "        Identity-164          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-165          [-1, 768, 14, 14]               0\n",
      "          Conv2d-166          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-167          [-1, 14, 14, 768]           1,536\n",
      "          Linear-168         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-169         [-1, 14, 14, 3072]               0\n",
      "         Dropout-170         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-171         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-172          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-173          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-174          [-1, 14, 14, 768]               0\n",
      "        Identity-175          [-1, 768, 14, 14]               0\n",
      "        Identity-176          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-177          [-1, 768, 14, 14]               0\n",
      "          Conv2d-178          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-179          [-1, 14, 14, 768]           1,536\n",
      "          Linear-180         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-181         [-1, 14, 14, 3072]               0\n",
      "         Dropout-182         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-183         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-184          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-185          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-186          [-1, 14, 14, 768]               0\n",
      "        Identity-187          [-1, 768, 14, 14]               0\n",
      "        Identity-188          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-189          [-1, 768, 14, 14]               0\n",
      "          Conv2d-190          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-191          [-1, 14, 14, 768]           1,536\n",
      "          Linear-192         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-193         [-1, 14, 14, 3072]               0\n",
      "         Dropout-194         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-195         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-196          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-197          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-198          [-1, 14, 14, 768]               0\n",
      "        Identity-199          [-1, 768, 14, 14]               0\n",
      "        Identity-200          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-201          [-1, 768, 14, 14]               0\n",
      "          Conv2d-202          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-203          [-1, 14, 14, 768]           1,536\n",
      "          Linear-204         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-205         [-1, 14, 14, 3072]               0\n",
      "         Dropout-206         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-207         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-208          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-209          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-210          [-1, 14, 14, 768]               0\n",
      "        Identity-211          [-1, 768, 14, 14]               0\n",
      "        Identity-212          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-213          [-1, 768, 14, 14]               0\n",
      "          Conv2d-214          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-215          [-1, 14, 14, 768]           1,536\n",
      "          Linear-216         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-217         [-1, 14, 14, 3072]               0\n",
      "         Dropout-218         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-219         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-220          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-221          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-222          [-1, 14, 14, 768]               0\n",
      "        Identity-223          [-1, 768, 14, 14]               0\n",
      "        Identity-224          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-225          [-1, 768, 14, 14]               0\n",
      "          Conv2d-226          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-227          [-1, 14, 14, 768]           1,536\n",
      "          Linear-228         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-229         [-1, 14, 14, 3072]               0\n",
      "         Dropout-230         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-231         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-232          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-233          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-234          [-1, 14, 14, 768]               0\n",
      "        Identity-235          [-1, 768, 14, 14]               0\n",
      "        Identity-236          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-237          [-1, 768, 14, 14]               0\n",
      "          Conv2d-238          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-239          [-1, 14, 14, 768]           1,536\n",
      "          Linear-240         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-241         [-1, 14, 14, 3072]               0\n",
      "         Dropout-242         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-243         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-244          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-245          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-246          [-1, 14, 14, 768]               0\n",
      "        Identity-247          [-1, 768, 14, 14]               0\n",
      "        Identity-248          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-249          [-1, 768, 14, 14]               0\n",
      "          Conv2d-250          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-251          [-1, 14, 14, 768]           1,536\n",
      "          Linear-252         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-253         [-1, 14, 14, 3072]               0\n",
      "         Dropout-254         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-255         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-256          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-257          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-258          [-1, 14, 14, 768]               0\n",
      "        Identity-259          [-1, 768, 14, 14]               0\n",
      "        Identity-260          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-261          [-1, 768, 14, 14]               0\n",
      "          Conv2d-262          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-263          [-1, 14, 14, 768]           1,536\n",
      "          Linear-264         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-265         [-1, 14, 14, 3072]               0\n",
      "         Dropout-266         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-267         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-268          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-269          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-270          [-1, 14, 14, 768]               0\n",
      "        Identity-271          [-1, 768, 14, 14]               0\n",
      "        Identity-272          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-273          [-1, 768, 14, 14]               0\n",
      "          Conv2d-274          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-275          [-1, 14, 14, 768]           1,536\n",
      "          Linear-276         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-277         [-1, 14, 14, 3072]               0\n",
      "         Dropout-278         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-279         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-280          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-281          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-282          [-1, 14, 14, 768]               0\n",
      "        Identity-283          [-1, 768, 14, 14]               0\n",
      "        Identity-284          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-285          [-1, 768, 14, 14]               0\n",
      "          Conv2d-286          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-287          [-1, 14, 14, 768]           1,536\n",
      "          Linear-288         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-289         [-1, 14, 14, 3072]               0\n",
      "         Dropout-290         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-291         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-292          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-293          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-294          [-1, 14, 14, 768]               0\n",
      "        Identity-295          [-1, 768, 14, 14]               0\n",
      "        Identity-296          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-297          [-1, 768, 14, 14]               0\n",
      "          Conv2d-298          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-299          [-1, 14, 14, 768]           1,536\n",
      "          Linear-300         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-301         [-1, 14, 14, 3072]               0\n",
      "         Dropout-302         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-303         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-304          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-305          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-306          [-1, 14, 14, 768]               0\n",
      "        Identity-307          [-1, 768, 14, 14]               0\n",
      "        Identity-308          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-309          [-1, 768, 14, 14]               0\n",
      "          Conv2d-310          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-311          [-1, 14, 14, 768]           1,536\n",
      "          Linear-312         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-313         [-1, 14, 14, 3072]               0\n",
      "         Dropout-314         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-315         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-316          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-317          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-318          [-1, 14, 14, 768]               0\n",
      "        Identity-319          [-1, 768, 14, 14]               0\n",
      "        Identity-320          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-321          [-1, 768, 14, 14]               0\n",
      "          Conv2d-322          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-323          [-1, 14, 14, 768]           1,536\n",
      "          Linear-324         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-325         [-1, 14, 14, 3072]               0\n",
      "         Dropout-326         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-327         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-328          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-329          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-330          [-1, 14, 14, 768]               0\n",
      "        Identity-331          [-1, 768, 14, 14]               0\n",
      "        Identity-332          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-333          [-1, 768, 14, 14]               0\n",
      "          Conv2d-334          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-335          [-1, 14, 14, 768]           1,536\n",
      "          Linear-336         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-337         [-1, 14, 14, 3072]               0\n",
      "         Dropout-338         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-339         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-340          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-341          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-342          [-1, 14, 14, 768]               0\n",
      "        Identity-343          [-1, 768, 14, 14]               0\n",
      "        Identity-344          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-345          [-1, 768, 14, 14]               0\n",
      "          Conv2d-346          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-347          [-1, 14, 14, 768]           1,536\n",
      "          Linear-348         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-349         [-1, 14, 14, 3072]               0\n",
      "         Dropout-350         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-351         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-352          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-353          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-354          [-1, 14, 14, 768]               0\n",
      "        Identity-355          [-1, 768, 14, 14]               0\n",
      "        Identity-356          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-357          [-1, 768, 14, 14]               0\n",
      "          Conv2d-358          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-359          [-1, 14, 14, 768]           1,536\n",
      "          Linear-360         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-361         [-1, 14, 14, 3072]               0\n",
      "         Dropout-362         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-363         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-364          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-365          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-366          [-1, 14, 14, 768]               0\n",
      "        Identity-367          [-1, 768, 14, 14]               0\n",
      "        Identity-368          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-369          [-1, 768, 14, 14]               0\n",
      "          Conv2d-370          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-371          [-1, 14, 14, 768]           1,536\n",
      "          Linear-372         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-373         [-1, 14, 14, 3072]               0\n",
      "         Dropout-374         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-375         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-376          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-377          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-378          [-1, 14, 14, 768]               0\n",
      "        Identity-379          [-1, 768, 14, 14]               0\n",
      "        Identity-380          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-381          [-1, 768, 14, 14]               0\n",
      "          Conv2d-382          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-383          [-1, 14, 14, 768]           1,536\n",
      "          Linear-384         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-385         [-1, 14, 14, 3072]               0\n",
      "         Dropout-386         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-387         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-388          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-389          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-390          [-1, 14, 14, 768]               0\n",
      "        Identity-391          [-1, 768, 14, 14]               0\n",
      "        Identity-392          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-393          [-1, 768, 14, 14]               0\n",
      "          Conv2d-394          [-1, 768, 14, 14]          38,400\n",
      "       LayerNorm-395          [-1, 14, 14, 768]           1,536\n",
      "          Linear-396         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-397         [-1, 14, 14, 3072]               0\n",
      "         Dropout-398         [-1, 14, 14, 3072]               0\n",
      "GlobalResponseNorm-399         [-1, 14, 14, 3072]           6,144\n",
      "          Linear-400          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-401          [-1, 14, 14, 768]               0\n",
      "GlobalResponseNormMlp-402          [-1, 14, 14, 768]               0\n",
      "        Identity-403          [-1, 768, 14, 14]               0\n",
      "        Identity-404          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtBlock-405          [-1, 768, 14, 14]               0\n",
      "   ConvNeXtStage-406          [-1, 768, 14, 14]               0\n",
      "     LayerNorm2d-407          [-1, 768, 14, 14]           1,536\n",
      "          Conv2d-408           [-1, 1536, 7, 7]       4,720,128\n",
      "          Conv2d-409           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-410           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-411           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-412           [-1, 7, 7, 6144]               0\n",
      "         Dropout-413           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-414           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-415           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-416           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-417           [-1, 7, 7, 1536]               0\n",
      "        Identity-418           [-1, 1536, 7, 7]               0\n",
      "        Identity-419           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-420           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-421           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-422           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-423           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-424           [-1, 7, 7, 6144]               0\n",
      "         Dropout-425           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-426           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-427           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-428           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-429           [-1, 7, 7, 1536]               0\n",
      "        Identity-430           [-1, 1536, 7, 7]               0\n",
      "        Identity-431           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-432           [-1, 1536, 7, 7]               0\n",
      "          Conv2d-433           [-1, 1536, 7, 7]          76,800\n",
      "       LayerNorm-434           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-435           [-1, 7, 7, 6144]       9,443,328\n",
      "            GELU-436           [-1, 7, 7, 6144]               0\n",
      "         Dropout-437           [-1, 7, 7, 6144]               0\n",
      "GlobalResponseNorm-438           [-1, 7, 7, 6144]          12,288\n",
      "          Linear-439           [-1, 7, 7, 1536]       9,438,720\n",
      "         Dropout-440           [-1, 7, 7, 1536]               0\n",
      "GlobalResponseNormMlp-441           [-1, 7, 7, 1536]               0\n",
      "        Identity-442           [-1, 1536, 7, 7]               0\n",
      "        Identity-443           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtBlock-444           [-1, 1536, 7, 7]               0\n",
      "   ConvNeXtStage-445           [-1, 1536, 7, 7]               0\n",
      "        Identity-446           [-1, 1536, 7, 7]               0\n",
      "AdaptiveAvgPool2d-447           [-1, 1536, 1, 1]               0\n",
      "        Identity-448           [-1, 1536, 1, 1]               0\n",
      "SelectAdaptivePool2d-449           [-1, 1536, 1, 1]               0\n",
      "     LayerNorm2d-450           [-1, 1536, 1, 1]           3,072\n",
      "         Flatten-451                 [-1, 1536]               0\n",
      "        Identity-452                 [-1, 1536]               0\n",
      "         Dropout-453                 [-1, 1536]               0\n",
      "          Linear-454                   [-1, 17]          26,129\n",
      "NormMlpClassifierHead-455                   [-1, 17]               0\n",
      "================================================================\n",
      "Total params: 196,445,969\n",
      "Trainable params: 196,445,969\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 1316.77\n",
      "Params size (MB): 749.38\n",
      "Estimated Total Size (MB): 2066.72\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3000: 100%|██████████| 40/40 [00:29<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.8191, Train Acc: 0.7524, Train F1: 0.7394\n",
      "Val Loss: 0.3038, Val Acc: 0.8790, Val F1: 0.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0397: 100%|██████████| 40/40 [00:29<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.2373, Train Acc: 0.9053, Train F1: 0.8968\n",
      "Val Loss: 0.2177, Val Acc: 0.9045, Val F1: 0.8850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0291: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.1529, Train Acc: 0.9395, Train F1: 0.9360\n",
      "Val Loss: 0.1973, Val Acc: 0.9108, Val F1: 0.9005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0148: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.0879, Train Acc: 0.9674, Train F1: 0.9661\n",
      "Val Loss: 0.1966, Val Acc: 0.9331, Val F1: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.0640, Train Acc: 0.9777, Train F1: 0.9784\n",
      "Val Loss: 0.1732, Val Acc: 0.9490, Val F1: 0.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0454, Train Acc: 0.9841, Train F1: 0.9843\n",
      "Val Loss: 0.1889, Val Acc: 0.9522, Val F1: 0.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0022: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0230, Train Acc: 0.9928, Train F1: 0.9934\n",
      "Val Loss: 0.2811, Val Acc: 0.9363, Val F1: 0.9257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0028: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0569, Train Acc: 0.9801, Train F1: 0.9795\n",
      "Val Loss: 0.3390, Val Acc: 0.9045, Val F1: 0.8940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0018: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0415, Train Acc: 0.9833, Train F1: 0.9827\n",
      "Val Loss: 0.2686, Val Acc: 0.9299, Val F1: 0.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0239, Train Acc: 0.9904, Train F1: 0.9909\n",
      "Val Loss: 0.2953, Val Acc: 0.9140, Val F1: 0.9039\n",
      "Early stopping\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0044, Train Acc: 0.9960, Train F1: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.0010, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.0009, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.0003, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.0004, Train Acc: 1.0000, Train F1: 1.0000\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2489: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.9736, Train Acc: 0.6990, Train F1: 0.6796\n",
      "Val Loss: 0.4136, Val Acc: 0.8503, Val F1: 0.8294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0315: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.2272, Train Acc: 0.9156, Train F1: 0.9069\n",
      "Val Loss: 0.1553, Val Acc: 0.9268, Val F1: 0.9180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0192: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.1267, Train Acc: 0.9498, Train F1: 0.9486\n",
      "Val Loss: 0.2288, Val Acc: 0.9140, Val F1: 0.9112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0719: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1010, Train Acc: 0.9602, Train F1: 0.9585\n",
      "Val Loss: 0.1379, Val Acc: 0.9459, Val F1: 0.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0226: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.0870, Train Acc: 0.9650, Train F1: 0.9625\n",
      "Val Loss: 0.1803, Val Acc: 0.9395, Val F1: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1696: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0623, Train Acc: 0.9745, Train F1: 0.9749\n",
      "Val Loss: 0.1711, Val Acc: 0.9268, Val F1: 0.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2599: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0503, Train Acc: 0.9785, Train F1: 0.9782\n",
      "Val Loss: 0.1470, Val Acc: 0.9363, Val F1: 0.9358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0261: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0366, Train Acc: 0.9881, Train F1: 0.9881\n",
      "Val Loss: 0.1707, Val Acc: 0.9459, Val F1: 0.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0233, Train Acc: 0.9896, Train F1: 0.9893\n",
      "Val Loss: 0.2952, Val Acc: 0.9140, Val F1: 0.9135\n",
      "Early stopping\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0355: 100%|██████████| 7/7 [00:05<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0431, Train Acc: 0.9773, Train F1: 0.8854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0015: 100%|██████████| 7/7 [00:05<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.0030, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0031: 100%|██████████| 7/7 [00:05<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.0017, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.0009, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 7/7 [00:05<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.0004, Train Acc: 1.0000, Train F1: 1.0000\n",
      "\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3387: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.8628, Train Acc: 0.7357, Train F1: 0.7151\n",
      "Val Loss: 0.2835, Val Acc: 0.9013, Val F1: 0.8785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2889: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.2508, Train Acc: 0.8981, Train F1: 0.8829\n",
      "Val Loss: 0.2340, Val Acc: 0.9045, Val F1: 0.8899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0812: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.1691, Train Acc: 0.9363, Train F1: 0.9296\n",
      "Val Loss: 0.1590, Val Acc: 0.9459, Val F1: 0.9470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0016: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1134, Train Acc: 0.9498, Train F1: 0.9447\n",
      "Val Loss: 0.2332, Val Acc: 0.9076, Val F1: 0.9008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0057: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.0588, Train Acc: 0.9817, Train F1: 0.9810\n",
      "Val Loss: 0.1414, Val Acc: 0.9459, Val F1: 0.9432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0285, Train Acc: 0.9873, Train F1: 0.9871\n",
      "Val Loss: 0.1545, Val Acc: 0.9522, Val F1: 0.9537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0322, Train Acc: 0.9881, Train F1: 0.9874\n",
      "Val Loss: 0.2753, Val Acc: 0.9236, Val F1: 0.9242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0030: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0268, Train Acc: 0.9920, Train F1: 0.9919\n",
      "Val Loss: 0.1844, Val Acc: 0.9490, Val F1: 0.9537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0182: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0242, Train Acc: 0.9928, Train F1: 0.9930\n",
      "Val Loss: 0.2028, Val Acc: 0.9490, Val F1: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0158, Train Acc: 0.9920, Train F1: 0.9915\n",
      "Val Loss: 0.2649, Val Acc: 0.9427, Val F1: 0.9465\n",
      "Early stopping\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0233: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0192, Train Acc: 0.9953, Train F1: 0.9937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0121: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.0242, Train Acc: 0.9953, Train F1: 0.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.0079, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 7/7 [00:05<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.0006, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0017: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.0009, Train Acc: 1.0000, Train F1: 1.0000\n",
      "\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5893: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 1.1355, Train Acc: 0.6425, Train F1: 0.6094\n",
      "Val Loss: 0.3313, Val Acc: 0.8822, Val F1: 0.8322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2319: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.3021, Train Acc: 0.8822, Train F1: 0.8674\n",
      "Val Loss: 0.2439, Val Acc: 0.9108, Val F1: 0.8810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0319: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.1494, Train Acc: 0.9403, Train F1: 0.9340\n",
      "Val Loss: 0.2276, Val Acc: 0.9236, Val F1: 0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0891: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1041, Train Acc: 0.9554, Train F1: 0.9537\n",
      "Val Loss: 0.1598, Val Acc: 0.9331, Val F1: 0.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1453: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.0634, Train Acc: 0.9737, Train F1: 0.9744\n",
      "Val Loss: 0.1860, Val Acc: 0.9427, Val F1: 0.9272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0079: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0479, Train Acc: 0.9817, Train F1: 0.9810\n",
      "Val Loss: 0.2045, Val Acc: 0.9299, Val F1: 0.9256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0026: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0210, Train Acc: 0.9920, Train F1: 0.9916\n",
      "Val Loss: 0.5239, Val Acc: 0.9108, Val F1: 0.9026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0023: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0996, Train Acc: 0.9674, Train F1: 0.9665\n",
      "Val Loss: 0.2299, Val Acc: 0.9204, Val F1: 0.9104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0442: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0272, Train Acc: 0.9912, Train F1: 0.9918\n",
      "Val Loss: 0.2198, Val Acc: 0.9363, Val F1: 0.9295\n",
      "Early stopping\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0025: 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0456, Train Acc: 0.9910, Train F1: 0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0018: 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.0036, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0010: 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.0012, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.0027, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 7/7 [00:05<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.0003, Train Acc: 1.0000, Train F1: 1.0000\n",
      "\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4951: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Train Loss: 0.8277, Train Acc: 0.7532, Train F1: 0.7310\n",
      "Val Loss: 0.3096, Val Acc: 0.8949, Val F1: 0.8761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0037: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "Train Loss: 0.2414, Train Acc: 0.9053, Train F1: 0.8950\n",
      "Val Loss: 0.3961, Val Acc: 0.8854, Val F1: 0.8565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0087: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "Train Loss: 0.1802, Train Acc: 0.9275, Train F1: 0.9228\n",
      "Val Loss: 0.2693, Val Acc: 0.9013, Val F1: 0.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0332: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "Train Loss: 0.1195, Train Acc: 0.9459, Train F1: 0.9434\n",
      "Val Loss: 0.2757, Val Acc: 0.9013, Val F1: 0.8908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0636: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "Train Loss: 0.0597, Train Acc: 0.9761, Train F1: 0.9766\n",
      "Val Loss: 0.1685, Val Acc: 0.9363, Val F1: 0.9290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "Train Loss: 0.0498, Train Acc: 0.9777, Train F1: 0.9785\n",
      "Val Loss: 0.2059, Val Acc: 0.9299, Val F1: 0.9291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0080: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "Train Loss: 0.0233, Train Acc: 0.9928, Train F1: 0.9927\n",
      "Val Loss: 0.2588, Val Acc: 0.9331, Val F1: 0.9229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "Train Loss: 0.0178, Train Acc: 0.9944, Train F1: 0.9949\n",
      "Val Loss: 0.3170, Val Acc: 0.9076, Val F1: 0.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0477: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "Train Loss: 0.0291, Train Acc: 0.9920, Train F1: 0.9920\n",
      "Val Loss: 0.1705, Val Acc: 0.9459, Val F1: 0.9411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0031: 100%|██████████| 40/40 [00:29<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "Train Loss: 0.0265, Train Acc: 0.9881, Train F1: 0.9880\n",
      "Val Loss: 0.2058, Val Acc: 0.9331, Val F1: 0.9272\n",
      "Early stopping\n",
      "\n",
      "Fine-tuning with misclassified data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0030: 100%|██████████| 9/9 [00:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0170, Train Acc: 0.9962, Train F1: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 9/9 [00:06<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.0013, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 9/9 [00:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.0008, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 9/9 [00:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.0005, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 9/9 [00:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.0006, Train Acc: 1.0000, Train F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160351/481695052.py:234: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"convNext_model_fold{fold}_final.pth\"))\n",
      "Predicting Fold 1: 100%|██████████| 99/99 [00:18<00:00,  5.29it/s]\n",
      "Predicting Fold 2: 100%|██████████| 99/99 [00:18<00:00,  5.28it/s]\n",
      "Predicting Fold 3: 100%|██████████| 99/99 [00:18<00:00,  5.28it/s]\n",
      "Predicting Fold 4: 100%|██████████| 99/99 [00:18<00:00,  5.28it/s]\n",
      "Predicting Fold 5: 100%|██████████| 99/99 [00:18<00:00,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble prediction completed and saved to pred_ensemble.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    misclassified = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_np = preds.argmax(dim=1).detach().cpu().numpy()\n",
    "            targets_np = targets.detach().cpu().numpy()\n",
    "            preds_list.extend(preds_np)\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            misclassified.extend(np.where(preds_np != targets_np)[0])\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1, misclassified\n",
    "\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'convnextv2_large'\n",
    "    img_size = 224\n",
    "    LR = 1e-4\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 32\n",
    "    num_workers = 4\n",
    "    n_splits = 5\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "        ], p=0.5),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "            A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.5),\n",
    "        ], p=0.5),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(blur_limit=3, p=0.5),\n",
    "            A.MedianBlur(blur_limit=3, p=0.5),\n",
    "            A.GaussianBlur(blur_limit=3, p=0.5),\n",
    "        ], p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=5, border_mode=0, p=0.5),\n",
    "        A.CoarseDropout(max_holes=8, max_height=img_size//20, max_width=img_size//20, min_holes=5, fill_value=255, p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['target']), 1):\n",
    "        print(f\"\\nFold {fold}\")\n",
    "\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train/\"), transform=train_transform)\n",
    "        val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train/\"), transform=val_transform)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "        if fold == 1:\n",
    "            print(f\"\\nModel structure of {model_name}:\")\n",
    "            print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "        best_val_f1 = 0\n",
    "        misclassified_data = []\n",
    "        early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "            val_loss, val_acc, val_f1, misclassified = validate(val_loader, model, loss_fn, device)\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f\"convNext_model_fold{fold}.pth\")\n",
    "\n",
    "            misclassified_data.extend(val_df.iloc[misclassified].index)\n",
    "\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        misclassified_df = df.loc[misclassified_data]\n",
    "        misclassified_dataset = ImageDataset(misclassified_df, os.path.join(data_path, \"train/\"), transform=train_transform)\n",
    "        misclassified_loader = DataLoader(misclassified_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        print(\"\\nFine-tuning with misclassified data\")\n",
    "        for epoch in range(5):\n",
    "            train_loss, train_acc, train_f1 = train_one_epoch(misclassified_loader, model, optimizer, loss_fn, device)\n",
    "            print(f\"Epoch {epoch+1}/5\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"convNext_model_fold{fold}_final.pth\")\n",
    "\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test/\"), transform=val_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    ensemble_preds = []\n",
    "    for fold in range(1, n_splits + 1):\n",
    "        model.load_state_dict(torch.load(f\"convNext_model_fold{fold}_final.pth\"))\n",
    "        model.eval()\n",
    "        fold_preds = []\n",
    "\n",
    "        for image, _ in tqdm(test_loader, desc=f\"Predicting Fold {fold}\"):\n",
    "            image = image.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(image)\n",
    "            fold_preds.extend(preds.softmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        ensemble_preds.append(fold_preds)\n",
    "\n",
    "    final_preds = np.mean(ensemble_preds, axis=0).argmax(axis=1)\n",
    "\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = final_preds\n",
    "    pred_df.to_csv(\"pred_ensemble.csv\", index=False)\n",
    "    print(\"Ensemble prediction completed and saved to pred_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters Tunning With CNN Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNeXt V2 Large 모델 + Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import optuna\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# Optuna를 이용한 하이퍼파라미터 최적화 함수\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 탐색 공간 정의\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "\n",
    "    # 데이터 로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델, 손실 함수, 옵티마이저 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 학습 및 검증\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "\n",
    "    return best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '../data/'\n",
    "model_name = 'convnextv2_large'\n",
    "img_size = 224  # ConvNeXt V2에 적합한 이미지 크기\n",
    "EPOCHS = 30\n",
    "num_workers = 4\n",
    "\n",
    "# 데이터 증강 설정\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터 로드 및 분할\n",
    "df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "# Optuna를 이용한 하이퍼파라미터 최적화\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 최종 모델 학습\n",
    "best_lr = best_params['lr']\n",
    "best_batch_size = best_params['batch_size']\n",
    "best_weight_decay = best_params['weight_decay']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# 모델 구조 출력\n",
    "print(f\"\\nModel structure of {model_name}:\")\n",
    "print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "# 학습 루프\n",
    "best_val_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNext V2 Large + WanDB Sweep\n",
    "- pip install wandb\n",
    "- wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "#class ImageDataset(Dataset):\n",
    "\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "#def validate(loader, model, loss_fn, device):\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "#def print_model_summary(model, input_size):\n",
    "\n",
    "\n",
    "# wandb sweep을 위한 학습 함수\n",
    "def train():\n",
    "    # wandb 초기화\n",
    "    run = wandb.init(entity=\"cho\") #사용자에 따라 자신의 도메인 네임 설정!!!\n",
    "    config = wandb.config\n",
    "\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = config.model_name\n",
    "    img_size = config.img_size\n",
    "    LR = config.learning_rate\n",
    "    EPOCHS = config.epochs\n",
    "    BATCH_SIZE = config.batch_size\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=config.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print(f\"\\nModel structure of {model_name}:\")\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # wandb에 로그 기록\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            wandb.run.summary[\"best_val_f1\"] = best_val_f1\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# wandb sweep 설정\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'val_f1',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'model_name': {\n",
    "            'values': ['convnextv2_large', 'efficientnet_b4']\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-3\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64]\n",
    "        },\n",
    "        'img_size': {\n",
    "            'values': [224, 256, 288]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [1e-5, 1e-4, 1e-3]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 30\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# wandb sweep 실행 및 최고 성능 모델 찾기\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"cvmodel\",entity=\"cho\")\n",
    "wandb.agent(sweep_id, train, count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 최고 성능 모델의 설정 가져오기\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"dl-12/cvmodel/{sweep_id}\")\n",
    "best_run = sweep.best_run()\n",
    "best_config = best_run.config\n",
    "\n",
    "  \n",
    "# 최고 성능 모델의 설정 사용\n",
    "model_name = best_config['model_name']\n",
    "img_size = best_config['img_size']\n",
    "BATCH_SIZE = best_config['batch_size']\n",
    "num_workers = 4\n",
    "\n",
    "# 테스트 데이터 변환\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 테스트 데이터셋 및 데이터로더 생성\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 최고 성능 모델 생성\n",
    "model = timm.create_model(model_name, pretrained=False, num_classes=17).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to pred.csv\")\n",
    "\n",
    "# wandb에 결과 업로드\n",
    "wandb.init(project=\"cvmodel\", name=\"best_model_prediction\", entity=\"cho\")\n",
    "wandb.config.update(best_config)\n",
    "wandb.save(\"pred.csv\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 기반 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 192, 56, 56]           9,408\n",
      "         LayerNorm-2          [-1, 56, 56, 192]             384\n",
      "        PatchEmbed-3          [-1, 56, 56, 192]               0\n",
      "          Identity-4          [-1, 56, 56, 192]               0\n",
      "         LayerNorm-5          [-1, 56, 56, 192]             384\n",
      "            Linear-6              [-1, 49, 576]         111,168\n",
      "           Softmax-7            [-1, 6, 49, 49]               0\n",
      "           Dropout-8            [-1, 6, 49, 49]               0\n",
      "            Linear-9              [-1, 49, 192]          37,056\n",
      "          Dropout-10              [-1, 49, 192]               0\n",
      "  WindowAttention-11              [-1, 49, 192]               0\n",
      "         Identity-12          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-13            [-1, 3136, 192]             384\n",
      "           Linear-14            [-1, 3136, 768]         148,224\n",
      "             GELU-15            [-1, 3136, 768]               0\n",
      "          Dropout-16            [-1, 3136, 768]               0\n",
      "         Identity-17            [-1, 3136, 768]               0\n",
      "           Linear-18            [-1, 3136, 192]         147,648\n",
      "          Dropout-19            [-1, 3136, 192]               0\n",
      "              Mlp-20            [-1, 3136, 192]               0\n",
      "         Identity-21            [-1, 3136, 192]               0\n",
      "SwinTransformerBlock-22          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-23          [-1, 56, 56, 192]             384\n",
      "           Linear-24              [-1, 49, 576]         111,168\n",
      "          Softmax-25            [-1, 6, 49, 49]               0\n",
      "          Dropout-26            [-1, 6, 49, 49]               0\n",
      "           Linear-27              [-1, 49, 192]          37,056\n",
      "          Dropout-28              [-1, 49, 192]               0\n",
      "  WindowAttention-29              [-1, 49, 192]               0\n",
      "         DropPath-30          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-31            [-1, 3136, 192]             384\n",
      "           Linear-32            [-1, 3136, 768]         148,224\n",
      "             GELU-33            [-1, 3136, 768]               0\n",
      "          Dropout-34            [-1, 3136, 768]               0\n",
      "         Identity-35            [-1, 3136, 768]               0\n",
      "           Linear-36            [-1, 3136, 192]         147,648\n",
      "          Dropout-37            [-1, 3136, 192]               0\n",
      "              Mlp-38            [-1, 3136, 192]               0\n",
      "         DropPath-39            [-1, 3136, 192]               0\n",
      "SwinTransformerBlock-40          [-1, 56, 56, 192]               0\n",
      "SwinTransformerStage-41          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-42          [-1, 28, 28, 768]           1,536\n",
      "           Linear-43          [-1, 28, 28, 384]         294,912\n",
      "     PatchMerging-44          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-45          [-1, 28, 28, 384]             768\n",
      "           Linear-46             [-1, 49, 1152]         443,520\n",
      "          Softmax-47           [-1, 12, 49, 49]               0\n",
      "          Dropout-48           [-1, 12, 49, 49]               0\n",
      "           Linear-49              [-1, 49, 384]         147,840\n",
      "          Dropout-50              [-1, 49, 384]               0\n",
      "  WindowAttention-51              [-1, 49, 384]               0\n",
      "         DropPath-52          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-53             [-1, 784, 384]             768\n",
      "           Linear-54            [-1, 784, 1536]         591,360\n",
      "             GELU-55            [-1, 784, 1536]               0\n",
      "          Dropout-56            [-1, 784, 1536]               0\n",
      "         Identity-57            [-1, 784, 1536]               0\n",
      "           Linear-58             [-1, 784, 384]         590,208\n",
      "          Dropout-59             [-1, 784, 384]               0\n",
      "              Mlp-60             [-1, 784, 384]               0\n",
      "         DropPath-61             [-1, 784, 384]               0\n",
      "SwinTransformerBlock-62          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-63          [-1, 28, 28, 384]             768\n",
      "           Linear-64             [-1, 49, 1152]         443,520\n",
      "          Softmax-65           [-1, 12, 49, 49]               0\n",
      "          Dropout-66           [-1, 12, 49, 49]               0\n",
      "           Linear-67              [-1, 49, 384]         147,840\n",
      "          Dropout-68              [-1, 49, 384]               0\n",
      "  WindowAttention-69              [-1, 49, 384]               0\n",
      "         DropPath-70          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-71             [-1, 784, 384]             768\n",
      "           Linear-72            [-1, 784, 1536]         591,360\n",
      "             GELU-73            [-1, 784, 1536]               0\n",
      "          Dropout-74            [-1, 784, 1536]               0\n",
      "         Identity-75            [-1, 784, 1536]               0\n",
      "           Linear-76             [-1, 784, 384]         590,208\n",
      "          Dropout-77             [-1, 784, 384]               0\n",
      "              Mlp-78             [-1, 784, 384]               0\n",
      "         DropPath-79             [-1, 784, 384]               0\n",
      "SwinTransformerBlock-80          [-1, 28, 28, 384]               0\n",
      "SwinTransformerStage-81          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-82         [-1, 14, 14, 1536]           3,072\n",
      "           Linear-83          [-1, 14, 14, 768]       1,179,648\n",
      "     PatchMerging-84          [-1, 14, 14, 768]               0\n",
      "        LayerNorm-85          [-1, 14, 14, 768]           1,536\n",
      "           Linear-86             [-1, 49, 2304]       1,771,776\n",
      "          Softmax-87           [-1, 24, 49, 49]               0\n",
      "          Dropout-88           [-1, 24, 49, 49]               0\n",
      "           Linear-89              [-1, 49, 768]         590,592\n",
      "          Dropout-90              [-1, 49, 768]               0\n",
      "  WindowAttention-91              [-1, 49, 768]               0\n",
      "         DropPath-92          [-1, 14, 14, 768]               0\n",
      "        LayerNorm-93             [-1, 196, 768]           1,536\n",
      "           Linear-94            [-1, 196, 3072]       2,362,368\n",
      "             GELU-95            [-1, 196, 3072]               0\n",
      "          Dropout-96            [-1, 196, 3072]               0\n",
      "         Identity-97            [-1, 196, 3072]               0\n",
      "           Linear-98             [-1, 196, 768]       2,360,064\n",
      "          Dropout-99             [-1, 196, 768]               0\n",
      "             Mlp-100             [-1, 196, 768]               0\n",
      "        DropPath-101             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-102          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-103          [-1, 14, 14, 768]           1,536\n",
      "          Linear-104             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-105           [-1, 24, 49, 49]               0\n",
      "         Dropout-106           [-1, 24, 49, 49]               0\n",
      "          Linear-107              [-1, 49, 768]         590,592\n",
      "         Dropout-108              [-1, 49, 768]               0\n",
      " WindowAttention-109              [-1, 49, 768]               0\n",
      "        DropPath-110          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-111             [-1, 196, 768]           1,536\n",
      "          Linear-112            [-1, 196, 3072]       2,362,368\n",
      "            GELU-113            [-1, 196, 3072]               0\n",
      "         Dropout-114            [-1, 196, 3072]               0\n",
      "        Identity-115            [-1, 196, 3072]               0\n",
      "          Linear-116             [-1, 196, 768]       2,360,064\n",
      "         Dropout-117             [-1, 196, 768]               0\n",
      "             Mlp-118             [-1, 196, 768]               0\n",
      "        DropPath-119             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-120          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-121          [-1, 14, 14, 768]           1,536\n",
      "          Linear-122             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-123           [-1, 24, 49, 49]               0\n",
      "         Dropout-124           [-1, 24, 49, 49]               0\n",
      "          Linear-125              [-1, 49, 768]         590,592\n",
      "         Dropout-126              [-1, 49, 768]               0\n",
      " WindowAttention-127              [-1, 49, 768]               0\n",
      "        DropPath-128          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-129             [-1, 196, 768]           1,536\n",
      "          Linear-130            [-1, 196, 3072]       2,362,368\n",
      "            GELU-131            [-1, 196, 3072]               0\n",
      "         Dropout-132            [-1, 196, 3072]               0\n",
      "        Identity-133            [-1, 196, 3072]               0\n",
      "          Linear-134             [-1, 196, 768]       2,360,064\n",
      "         Dropout-135             [-1, 196, 768]               0\n",
      "             Mlp-136             [-1, 196, 768]               0\n",
      "        DropPath-137             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-138          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-139          [-1, 14, 14, 768]           1,536\n",
      "          Linear-140             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-141           [-1, 24, 49, 49]               0\n",
      "         Dropout-142           [-1, 24, 49, 49]               0\n",
      "          Linear-143              [-1, 49, 768]         590,592\n",
      "         Dropout-144              [-1, 49, 768]               0\n",
      " WindowAttention-145              [-1, 49, 768]               0\n",
      "        DropPath-146          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-147             [-1, 196, 768]           1,536\n",
      "          Linear-148            [-1, 196, 3072]       2,362,368\n",
      "            GELU-149            [-1, 196, 3072]               0\n",
      "         Dropout-150            [-1, 196, 3072]               0\n",
      "        Identity-151            [-1, 196, 3072]               0\n",
      "          Linear-152             [-1, 196, 768]       2,360,064\n",
      "         Dropout-153             [-1, 196, 768]               0\n",
      "             Mlp-154             [-1, 196, 768]               0\n",
      "        DropPath-155             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-156          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-157          [-1, 14, 14, 768]           1,536\n",
      "          Linear-158             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-159           [-1, 24, 49, 49]               0\n",
      "         Dropout-160           [-1, 24, 49, 49]               0\n",
      "          Linear-161              [-1, 49, 768]         590,592\n",
      "         Dropout-162              [-1, 49, 768]               0\n",
      " WindowAttention-163              [-1, 49, 768]               0\n",
      "        DropPath-164          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-165             [-1, 196, 768]           1,536\n",
      "          Linear-166            [-1, 196, 3072]       2,362,368\n",
      "            GELU-167            [-1, 196, 3072]               0\n",
      "         Dropout-168            [-1, 196, 3072]               0\n",
      "        Identity-169            [-1, 196, 3072]               0\n",
      "          Linear-170             [-1, 196, 768]       2,360,064\n",
      "         Dropout-171             [-1, 196, 768]               0\n",
      "             Mlp-172             [-1, 196, 768]               0\n",
      "        DropPath-173             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-174          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-175          [-1, 14, 14, 768]           1,536\n",
      "          Linear-176             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-177           [-1, 24, 49, 49]               0\n",
      "         Dropout-178           [-1, 24, 49, 49]               0\n",
      "          Linear-179              [-1, 49, 768]         590,592\n",
      "         Dropout-180              [-1, 49, 768]               0\n",
      " WindowAttention-181              [-1, 49, 768]               0\n",
      "        DropPath-182          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-183             [-1, 196, 768]           1,536\n",
      "          Linear-184            [-1, 196, 3072]       2,362,368\n",
      "            GELU-185            [-1, 196, 3072]               0\n",
      "         Dropout-186            [-1, 196, 3072]               0\n",
      "        Identity-187            [-1, 196, 3072]               0\n",
      "          Linear-188             [-1, 196, 768]       2,360,064\n",
      "         Dropout-189             [-1, 196, 768]               0\n",
      "             Mlp-190             [-1, 196, 768]               0\n",
      "        DropPath-191             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-192          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-193          [-1, 14, 14, 768]           1,536\n",
      "          Linear-194             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-195           [-1, 24, 49, 49]               0\n",
      "         Dropout-196           [-1, 24, 49, 49]               0\n",
      "          Linear-197              [-1, 49, 768]         590,592\n",
      "         Dropout-198              [-1, 49, 768]               0\n",
      " WindowAttention-199              [-1, 49, 768]               0\n",
      "        DropPath-200          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-201             [-1, 196, 768]           1,536\n",
      "          Linear-202            [-1, 196, 3072]       2,362,368\n",
      "            GELU-203            [-1, 196, 3072]               0\n",
      "         Dropout-204            [-1, 196, 3072]               0\n",
      "        Identity-205            [-1, 196, 3072]               0\n",
      "          Linear-206             [-1, 196, 768]       2,360,064\n",
      "         Dropout-207             [-1, 196, 768]               0\n",
      "             Mlp-208             [-1, 196, 768]               0\n",
      "        DropPath-209             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-210          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-211          [-1, 14, 14, 768]           1,536\n",
      "          Linear-212             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-213           [-1, 24, 49, 49]               0\n",
      "         Dropout-214           [-1, 24, 49, 49]               0\n",
      "          Linear-215              [-1, 49, 768]         590,592\n",
      "         Dropout-216              [-1, 49, 768]               0\n",
      " WindowAttention-217              [-1, 49, 768]               0\n",
      "        DropPath-218          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-219             [-1, 196, 768]           1,536\n",
      "          Linear-220            [-1, 196, 3072]       2,362,368\n",
      "            GELU-221            [-1, 196, 3072]               0\n",
      "         Dropout-222            [-1, 196, 3072]               0\n",
      "        Identity-223            [-1, 196, 3072]               0\n",
      "          Linear-224             [-1, 196, 768]       2,360,064\n",
      "         Dropout-225             [-1, 196, 768]               0\n",
      "             Mlp-226             [-1, 196, 768]               0\n",
      "        DropPath-227             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-228          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-229          [-1, 14, 14, 768]           1,536\n",
      "          Linear-230             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-231           [-1, 24, 49, 49]               0\n",
      "         Dropout-232           [-1, 24, 49, 49]               0\n",
      "          Linear-233              [-1, 49, 768]         590,592\n",
      "         Dropout-234              [-1, 49, 768]               0\n",
      " WindowAttention-235              [-1, 49, 768]               0\n",
      "        DropPath-236          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-237             [-1, 196, 768]           1,536\n",
      "          Linear-238            [-1, 196, 3072]       2,362,368\n",
      "            GELU-239            [-1, 196, 3072]               0\n",
      "         Dropout-240            [-1, 196, 3072]               0\n",
      "        Identity-241            [-1, 196, 3072]               0\n",
      "          Linear-242             [-1, 196, 768]       2,360,064\n",
      "         Dropout-243             [-1, 196, 768]               0\n",
      "             Mlp-244             [-1, 196, 768]               0\n",
      "        DropPath-245             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-246          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-247          [-1, 14, 14, 768]           1,536\n",
      "          Linear-248             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-249           [-1, 24, 49, 49]               0\n",
      "         Dropout-250           [-1, 24, 49, 49]               0\n",
      "          Linear-251              [-1, 49, 768]         590,592\n",
      "         Dropout-252              [-1, 49, 768]               0\n",
      " WindowAttention-253              [-1, 49, 768]               0\n",
      "        DropPath-254          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-255             [-1, 196, 768]           1,536\n",
      "          Linear-256            [-1, 196, 3072]       2,362,368\n",
      "            GELU-257            [-1, 196, 3072]               0\n",
      "         Dropout-258            [-1, 196, 3072]               0\n",
      "        Identity-259            [-1, 196, 3072]               0\n",
      "          Linear-260             [-1, 196, 768]       2,360,064\n",
      "         Dropout-261             [-1, 196, 768]               0\n",
      "             Mlp-262             [-1, 196, 768]               0\n",
      "        DropPath-263             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-264          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-265          [-1, 14, 14, 768]           1,536\n",
      "          Linear-266             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-267           [-1, 24, 49, 49]               0\n",
      "         Dropout-268           [-1, 24, 49, 49]               0\n",
      "          Linear-269              [-1, 49, 768]         590,592\n",
      "         Dropout-270              [-1, 49, 768]               0\n",
      " WindowAttention-271              [-1, 49, 768]               0\n",
      "        DropPath-272          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-273             [-1, 196, 768]           1,536\n",
      "          Linear-274            [-1, 196, 3072]       2,362,368\n",
      "            GELU-275            [-1, 196, 3072]               0\n",
      "         Dropout-276            [-1, 196, 3072]               0\n",
      "        Identity-277            [-1, 196, 3072]               0\n",
      "          Linear-278             [-1, 196, 768]       2,360,064\n",
      "         Dropout-279             [-1, 196, 768]               0\n",
      "             Mlp-280             [-1, 196, 768]               0\n",
      "        DropPath-281             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-282          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-283          [-1, 14, 14, 768]           1,536\n",
      "          Linear-284             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-285           [-1, 24, 49, 49]               0\n",
      "         Dropout-286           [-1, 24, 49, 49]               0\n",
      "          Linear-287              [-1, 49, 768]         590,592\n",
      "         Dropout-288              [-1, 49, 768]               0\n",
      " WindowAttention-289              [-1, 49, 768]               0\n",
      "        DropPath-290          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-291             [-1, 196, 768]           1,536\n",
      "          Linear-292            [-1, 196, 3072]       2,362,368\n",
      "            GELU-293            [-1, 196, 3072]               0\n",
      "         Dropout-294            [-1, 196, 3072]               0\n",
      "        Identity-295            [-1, 196, 3072]               0\n",
      "          Linear-296             [-1, 196, 768]       2,360,064\n",
      "         Dropout-297             [-1, 196, 768]               0\n",
      "             Mlp-298             [-1, 196, 768]               0\n",
      "        DropPath-299             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-300          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-301          [-1, 14, 14, 768]           1,536\n",
      "          Linear-302             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-303           [-1, 24, 49, 49]               0\n",
      "         Dropout-304           [-1, 24, 49, 49]               0\n",
      "          Linear-305              [-1, 49, 768]         590,592\n",
      "         Dropout-306              [-1, 49, 768]               0\n",
      " WindowAttention-307              [-1, 49, 768]               0\n",
      "        DropPath-308          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-309             [-1, 196, 768]           1,536\n",
      "          Linear-310            [-1, 196, 3072]       2,362,368\n",
      "            GELU-311            [-1, 196, 3072]               0\n",
      "         Dropout-312            [-1, 196, 3072]               0\n",
      "        Identity-313            [-1, 196, 3072]               0\n",
      "          Linear-314             [-1, 196, 768]       2,360,064\n",
      "         Dropout-315             [-1, 196, 768]               0\n",
      "             Mlp-316             [-1, 196, 768]               0\n",
      "        DropPath-317             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-318          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-319          [-1, 14, 14, 768]           1,536\n",
      "          Linear-320             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-321           [-1, 24, 49, 49]               0\n",
      "         Dropout-322           [-1, 24, 49, 49]               0\n",
      "          Linear-323              [-1, 49, 768]         590,592\n",
      "         Dropout-324              [-1, 49, 768]               0\n",
      " WindowAttention-325              [-1, 49, 768]               0\n",
      "        DropPath-326          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-327             [-1, 196, 768]           1,536\n",
      "          Linear-328            [-1, 196, 3072]       2,362,368\n",
      "            GELU-329            [-1, 196, 3072]               0\n",
      "         Dropout-330            [-1, 196, 3072]               0\n",
      "        Identity-331            [-1, 196, 3072]               0\n",
      "          Linear-332             [-1, 196, 768]       2,360,064\n",
      "         Dropout-333             [-1, 196, 768]               0\n",
      "             Mlp-334             [-1, 196, 768]               0\n",
      "        DropPath-335             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-336          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-337          [-1, 14, 14, 768]           1,536\n",
      "          Linear-338             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-339           [-1, 24, 49, 49]               0\n",
      "         Dropout-340           [-1, 24, 49, 49]               0\n",
      "          Linear-341              [-1, 49, 768]         590,592\n",
      "         Dropout-342              [-1, 49, 768]               0\n",
      " WindowAttention-343              [-1, 49, 768]               0\n",
      "        DropPath-344          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-345             [-1, 196, 768]           1,536\n",
      "          Linear-346            [-1, 196, 3072]       2,362,368\n",
      "            GELU-347            [-1, 196, 3072]               0\n",
      "         Dropout-348            [-1, 196, 3072]               0\n",
      "        Identity-349            [-1, 196, 3072]               0\n",
      "          Linear-350             [-1, 196, 768]       2,360,064\n",
      "         Dropout-351             [-1, 196, 768]               0\n",
      "             Mlp-352             [-1, 196, 768]               0\n",
      "        DropPath-353             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-354          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-355          [-1, 14, 14, 768]           1,536\n",
      "          Linear-356             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-357           [-1, 24, 49, 49]               0\n",
      "         Dropout-358           [-1, 24, 49, 49]               0\n",
      "          Linear-359              [-1, 49, 768]         590,592\n",
      "         Dropout-360              [-1, 49, 768]               0\n",
      " WindowAttention-361              [-1, 49, 768]               0\n",
      "        DropPath-362          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-363             [-1, 196, 768]           1,536\n",
      "          Linear-364            [-1, 196, 3072]       2,362,368\n",
      "            GELU-365            [-1, 196, 3072]               0\n",
      "         Dropout-366            [-1, 196, 3072]               0\n",
      "        Identity-367            [-1, 196, 3072]               0\n",
      "          Linear-368             [-1, 196, 768]       2,360,064\n",
      "         Dropout-369             [-1, 196, 768]               0\n",
      "             Mlp-370             [-1, 196, 768]               0\n",
      "        DropPath-371             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-372          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-373          [-1, 14, 14, 768]           1,536\n",
      "          Linear-374             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-375           [-1, 24, 49, 49]               0\n",
      "         Dropout-376           [-1, 24, 49, 49]               0\n",
      "          Linear-377              [-1, 49, 768]         590,592\n",
      "         Dropout-378              [-1, 49, 768]               0\n",
      " WindowAttention-379              [-1, 49, 768]               0\n",
      "        DropPath-380          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-381             [-1, 196, 768]           1,536\n",
      "          Linear-382            [-1, 196, 3072]       2,362,368\n",
      "            GELU-383            [-1, 196, 3072]               0\n",
      "         Dropout-384            [-1, 196, 3072]               0\n",
      "        Identity-385            [-1, 196, 3072]               0\n",
      "          Linear-386             [-1, 196, 768]       2,360,064\n",
      "         Dropout-387             [-1, 196, 768]               0\n",
      "             Mlp-388             [-1, 196, 768]               0\n",
      "        DropPath-389             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-390          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-391          [-1, 14, 14, 768]           1,536\n",
      "          Linear-392             [-1, 49, 2304]       1,771,776\n",
      "         Softmax-393           [-1, 24, 49, 49]               0\n",
      "         Dropout-394           [-1, 24, 49, 49]               0\n",
      "          Linear-395              [-1, 49, 768]         590,592\n",
      "         Dropout-396              [-1, 49, 768]               0\n",
      " WindowAttention-397              [-1, 49, 768]               0\n",
      "        DropPath-398          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-399             [-1, 196, 768]           1,536\n",
      "          Linear-400            [-1, 196, 3072]       2,362,368\n",
      "            GELU-401            [-1, 196, 3072]               0\n",
      "         Dropout-402            [-1, 196, 3072]               0\n",
      "        Identity-403            [-1, 196, 3072]               0\n",
      "          Linear-404             [-1, 196, 768]       2,360,064\n",
      "         Dropout-405             [-1, 196, 768]               0\n",
      "             Mlp-406             [-1, 196, 768]               0\n",
      "        DropPath-407             [-1, 196, 768]               0\n",
      "SwinTransformerBlock-408          [-1, 14, 14, 768]               0\n",
      "SwinTransformerStage-409          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-410           [-1, 7, 7, 3072]           6,144\n",
      "          Linear-411           [-1, 7, 7, 1536]       4,718,592\n",
      "    PatchMerging-412           [-1, 7, 7, 1536]               0\n",
      "       LayerNorm-413           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-414             [-1, 49, 4608]       7,082,496\n",
      "         Softmax-415           [-1, 48, 49, 49]               0\n",
      "         Dropout-416           [-1, 48, 49, 49]               0\n",
      "          Linear-417             [-1, 49, 1536]       2,360,832\n",
      "         Dropout-418             [-1, 49, 1536]               0\n",
      " WindowAttention-419             [-1, 49, 1536]               0\n",
      "        DropPath-420           [-1, 7, 7, 1536]               0\n",
      "       LayerNorm-421             [-1, 49, 1536]           3,072\n",
      "          Linear-422             [-1, 49, 6144]       9,443,328\n",
      "            GELU-423             [-1, 49, 6144]               0\n",
      "         Dropout-424             [-1, 49, 6144]               0\n",
      "        Identity-425             [-1, 49, 6144]               0\n",
      "          Linear-426             [-1, 49, 1536]       9,438,720\n",
      "         Dropout-427             [-1, 49, 1536]               0\n",
      "             Mlp-428             [-1, 49, 1536]               0\n",
      "        DropPath-429             [-1, 49, 1536]               0\n",
      "SwinTransformerBlock-430           [-1, 7, 7, 1536]               0\n",
      "       LayerNorm-431           [-1, 7, 7, 1536]           3,072\n",
      "          Linear-432             [-1, 49, 4608]       7,082,496\n",
      "         Softmax-433           [-1, 48, 49, 49]               0\n",
      "         Dropout-434           [-1, 48, 49, 49]               0\n",
      "          Linear-435             [-1, 49, 1536]       2,360,832\n",
      "         Dropout-436             [-1, 49, 1536]               0\n",
      " WindowAttention-437             [-1, 49, 1536]               0\n",
      "        DropPath-438           [-1, 7, 7, 1536]               0\n",
      "       LayerNorm-439             [-1, 49, 1536]           3,072\n",
      "          Linear-440             [-1, 49, 6144]       9,443,328\n",
      "            GELU-441             [-1, 49, 6144]               0\n",
      "         Dropout-442             [-1, 49, 6144]               0\n",
      "        Identity-443             [-1, 49, 6144]               0\n",
      "          Linear-444             [-1, 49, 1536]       9,438,720\n",
      "         Dropout-445             [-1, 49, 1536]               0\n",
      "             Mlp-446             [-1, 49, 1536]               0\n",
      "        DropPath-447             [-1, 49, 1536]               0\n",
      "SwinTransformerBlock-448           [-1, 7, 7, 1536]               0\n",
      "SwinTransformerStage-449           [-1, 7, 7, 1536]               0\n",
      "       LayerNorm-450           [-1, 7, 7, 1536]           3,072\n",
      "FastAdaptiveAvgPool-451                 [-1, 1536]               0\n",
      "        Identity-452                 [-1, 1536]               0\n",
      "SelectAdaptivePool2d-453                 [-1, 1536]               0\n",
      "         Dropout-454                 [-1, 1536]               0\n",
      "          Linear-455                   [-1, 17]          26,129\n",
      "        Identity-456                   [-1, 17]               0\n",
      "  ClassifierHead-457                   [-1, 17]               0\n",
      "================================================================\n",
      "Total params: 194,926,289\n",
      "Trainable params: 194,926,289\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 959.27\n",
      "Params size (MB): 743.58\n",
      "Estimated Total Size (MB): 1703.43\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1804: 100%|██████████| 259/259 [02:14<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 0.7277, Train Acc: 0.7804, Train F1: 0.7628\n",
      "Val Loss: 0.2358, Val Acc: 0.9108, Val F1: 0.8978\n",
      "Validation loss decreased (inf --> 0.235835).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0452: 100%|██████████| 259/259 [02:15<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.2097, Train Acc: 0.9219, Train F1: 0.9141\n",
      "Val Loss: 0.1192, Val Acc: 0.9531, Val F1: 0.9451\n",
      "Validation loss decreased (0.235835 --> 0.119184).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1793: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.1320, Train Acc: 0.9513, Train F1: 0.9480\n",
      "Val Loss: 0.0792, Val Acc: 0.9695, Val F1: 0.9677\n",
      "Validation loss decreased (0.119184 --> 0.079231).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0017: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.0794, Train Acc: 0.9729, Train F1: 0.9719\n",
      "Val Loss: 0.0617, Val Acc: 0.9754, Val F1: 0.9744\n",
      "Validation loss decreased (0.079231 --> 0.061746).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.0581, Train Acc: 0.9802, Train F1: 0.9799\n",
      "Val Loss: 0.0360, Val Acc: 0.9870, Val F1: 0.9861\n",
      "Validation loss decreased (0.061746 --> 0.036009).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0316: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.0386, Train Acc: 0.9880, Train F1: 0.9877\n",
      "Val Loss: 0.0275, Val Acc: 0.9912, Val F1: 0.9910\n",
      "Validation loss decreased (0.036009 --> 0.027513).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0039: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.0311, Train Acc: 0.9895, Train F1: 0.9889\n",
      "Val Loss: 0.0357, Val Acc: 0.9893, Val F1: 0.9882\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0101: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.0263, Train Acc: 0.9920, Train F1: 0.9916\n",
      "Val Loss: 0.0382, Val Acc: 0.9879, Val F1: 0.9866\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.0237, Train Acc: 0.9921, Train F1: 0.9922\n",
      "Val Loss: 0.0357, Val Acc: 0.9896, Val F1: 0.9891\n",
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.0202, Train Acc: 0.9933, Train F1: 0.9932\n",
      "Val Loss: 0.0211, Val Acc: 0.9912, Val F1: 0.9907\n",
      "Validation loss decreased (0.027513 --> 0.021077).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009: 100%|██████████| 259/259 [02:15<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0202, Train Acc: 0.9947, Train F1: 0.9944\n",
      "Val Loss: 0.0348, Val Acc: 0.9887, Val F1: 0.9868\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0015: 100%|██████████| 259/259 [02:15<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "Train Loss: 0.0139, Train Acc: 0.9953, Train F1: 0.9951\n",
      "Val Loss: 0.0268, Val Acc: 0.9912, Val F1: 0.9905\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 259/259 [02:15<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "Train Loss: 0.0144, Train Acc: 0.9954, Train F1: 0.9954\n",
      "Val Loss: 0.0311, Val Acc: 0.9898, Val F1: 0.9892\n",
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0008: 100%|██████████| 259/259 [02:15<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.0126, Train Acc: 0.9958, Train F1: 0.9954\n",
      "Val Loss: 0.0224, Val Acc: 0.9932, Val F1: 0.9928\n",
      "EarlyStopping counter: 4 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "Train Loss: 0.0144, Train Acc: 0.9956, Train F1: 0.9951\n",
      "Val Loss: 0.0197, Val Acc: 0.9932, Val F1: 0.9934\n",
      "Validation loss decreased (0.021077 --> 0.019676).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "Train Loss: 0.0072, Train Acc: 0.9979, Train F1: 0.9977\n",
      "Val Loss: 0.0482, Val Acc: 0.9884, Val F1: 0.9877\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "Train Loss: 0.0161, Train Acc: 0.9952, Train F1: 0.9950\n",
      "Val Loss: 0.0243, Val Acc: 0.9921, Val F1: 0.9919\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "Train Loss: 0.0130, Train Acc: 0.9959, Train F1: 0.9959\n",
      "Val Loss: 0.0258, Val Acc: 0.9921, Val F1: 0.9918\n",
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0068: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "Train Loss: 0.0068, Train Acc: 0.9977, Train F1: 0.9977\n",
      "Val Loss: 0.0216, Val Acc: 0.9932, Val F1: 0.9931\n",
      "EarlyStopping counter: 4 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "Train Loss: 0.0089, Train Acc: 0.9977, Train F1: 0.9975\n",
      "Val Loss: 0.0194, Val Acc: 0.9949, Val F1: 0.9948\n",
      "EarlyStopping counter: 5 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0156: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "Train Loss: 0.0095, Train Acc: 0.9966, Train F1: 0.9964\n",
      "Val Loss: 0.0202, Val Acc: 0.9944, Val F1: 0.9940\n",
      "EarlyStopping counter: 6 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "Train Loss: 0.0080, Train Acc: 0.9976, Train F1: 0.9976\n",
      "Val Loss: 0.0179, Val Acc: 0.9955, Val F1: 0.9953\n",
      "Validation loss decreased (0.019676 --> 0.017897).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "Train Loss: 0.0124, Train Acc: 0.9959, Train F1: 0.9957\n",
      "Val Loss: 0.0238, Val Acc: 0.9932, Val F1: 0.9933\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0041: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "Train Loss: 0.0102, Train Acc: 0.9969, Train F1: 0.9967\n",
      "Val Loss: 0.0161, Val Acc: 0.9960, Val F1: 0.9961\n",
      "Validation loss decreased (0.017897 --> 0.016115).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 259/259 [02:15<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "Train Loss: 0.0113, Train Acc: 0.9966, Train F1: 0.9967\n",
      "Val Loss: 0.0123, Val Acc: 0.9966, Val F1: 0.9968\n",
      "Validation loss decreased (0.016115 --> 0.012313).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0039: 100%|██████████| 259/259 [02:15<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "Train Loss: 0.0028, Train Acc: 0.9994, Train F1: 0.9993\n",
      "Val Loss: 0.0116, Val Acc: 0.9975, Val F1: 0.9975\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 259/259 [02:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "Train Loss: 0.0041, Train Acc: 0.9990, Train F1: 0.9990\n",
      "Val Loss: 0.0130, Val Acc: 0.9960, Val F1: 0.9959\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 259/259 [02:16<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "Train Loss: 0.0049, Train Acc: 0.9983, Train F1: 0.9984\n",
      "Val Loss: 0.0162, Val Acc: 0.9958, Val F1: 0.9956\n",
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 259/259 [02:15<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "Train Loss: 0.0067, Train Acc: 0.9979, Train F1: 0.9978\n",
      "Val Loss: 0.0123, Val Acc: 0.9955, Val F1: 0.9954\n",
      "EarlyStopping counter: 4 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 259/259 [02:17<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "Train Loss: 0.0047, Train Acc: 0.9984, Train F1: 0.9983\n",
      "Val Loss: 0.0124, Val Acc: 0.9946, Val F1: 0.9945\n",
      "EarlyStopping counter: 5 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 259/259 [02:19<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "Train Loss: 0.0018, Train Acc: 0.9994, Train F1: 0.9993\n",
      "Val Loss: 0.0122, Val Acc: 0.9963, Val F1: 0.9962\n",
      "EarlyStopping counter: 6 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 259/259 [02:23<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "Train Loss: 0.0028, Train Acc: 0.9993, Train F1: 0.9992\n",
      "Val Loss: 0.0116, Val Acc: 0.9977, Val F1: 0.9978\n",
      "EarlyStopping counter: 7 out of 7\n",
      "Early stopping. Best validation loss: 0.012313, Best F1 score: 0.996774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:20<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to pred_swin.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import timm\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        self.best_f1 = -np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, f1_score, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, f1_score, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        self.best_f1 = f1_score\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'swin_large_patch4_window7_224'  # Swin Transformer Large 모델\n",
    "    img_size = 224  # Swin Transformer에 적합한 이미지 크기\n",
    "    LR = 2e-5  # 학습률\n",
    "    EPOCHS = 100  # 에포크 수\n",
    "    BATCH_SIZE = 32  # 배치 크기\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"augmented_train.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"augmented_train/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"augmented_train/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 설정\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    early_stopping = EarlyStopping(patience=7, verbose=True, delta=0.001, path='aug_swin_model.pth')\n",
    "\n",
    "    # 모델 구조 출력\n",
    "    print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"aug_swin_model.pth\")\n",
    "        \n",
    "        early_stopping(val_loss, val_f1, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping. Best validation loss: {early_stopping.val_loss_min:.6f}, \"\n",
    "                  f\"Best F1 score: {early_stopping.best_f1:.6f}\")\n",
    "            break\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"aug_swin_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred_swin.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred_swin.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data Denoising Autoencoder\n",
    "- 테스트 데이터를 autoencoder 방식의 이미지를 가지고 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터셋 클래스 정의(독립실행으로 가정하고 중복 정의)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 테스트 데이터 경로 설정\n",
    "data_path = '../data/'\n",
    "\n",
    "# 테스트 데이터셋 및 데이터 로더 생성\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"denoise_test/\"), transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 모델 로드 및 평가 모드 설정\n",
    "model.load_state_dict(torch.load(\"aug_swin_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"pred_swin.csv\", index=False)\n",
    "print(\"Prediction completed and saved to pred_swin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified k-fold cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1379: 100%|██████████| 40/40 [00:20<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 1.9977, Train Acc: 0.4865, Train F1: 0.4503\n",
      "Val Loss: 0.8827, Val Acc: 0.7739, Val F1: 0.6936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4577: 100%|██████████| 40/40 [00:20<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.6732, Train Acc: 0.8049, Train F1: 0.7707\n",
      "Val Loss: 0.4043, Val Acc: 0.8631, Val F1: 0.8179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3563: 100%|██████████| 40/40 [00:20<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.3740, Train Acc: 0.8814, Train F1: 0.8610\n",
      "Val Loss: 0.2678, Val Acc: 0.8822, Val F1: 0.8490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3622: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.2812, Train Acc: 0.8989, Train F1: 0.8843\n",
      "Val Loss: 0.2281, Val Acc: 0.8949, Val F1: 0.8587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1351: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.2306, Train Acc: 0.9092, Train F1: 0.8988\n",
      "Val Loss: 0.3207, Val Acc: 0.8885, Val F1: 0.8724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4066: 100%|██████████| 40/40 [00:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.1993, Train Acc: 0.9307, Train F1: 0.9236\n",
      "Val Loss: 0.2139, Val Acc: 0.9108, Val F1: 0.8802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0564: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.1616, Train Acc: 0.9411, Train F1: 0.9367\n",
      "Val Loss: 0.2266, Val Acc: 0.9236, Val F1: 0.9109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0156: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.1497, Train Acc: 0.9427, Train F1: 0.9381\n",
      "Val Loss: 0.1785, Val Acc: 0.9076, Val F1: 0.8937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1676: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.1273, Train Acc: 0.9570, Train F1: 0.9535\n",
      "Val Loss: 0.2308, Val Acc: 0.9236, Val F1: 0.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2569: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.1152, Train Acc: 0.9562, Train F1: 0.9555\n",
      "Val Loss: 0.2009, Val Acc: 0.9299, Val F1: 0.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0047: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0833, Train Acc: 0.9737, Train F1: 0.9735\n",
      "Val Loss: 0.2476, Val Acc: 0.9236, Val F1: 0.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0017: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "Train Loss: 0.1018, Train Acc: 0.9586, Train F1: 0.9557\n",
      "Val Loss: 0.1968, Val Acc: 0.9331, Val F1: 0.9255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2389: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "Train Loss: 0.1085, Train Acc: 0.9578, Train F1: 0.9558\n",
      "Val Loss: 0.2119, Val Acc: 0.9076, Val F1: 0.8933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0059: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.0823, Train Acc: 0.9705, Train F1: 0.9685\n",
      "Val Loss: 0.1973, Val Acc: 0.9268, Val F1: 0.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "Train Loss: 0.0565, Train Acc: 0.9825, Train F1: 0.9822\n",
      "Val Loss: 0.1846, Val Acc: 0.9299, Val F1: 0.9248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0499: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "Train Loss: 0.0534, Train Acc: 0.9817, Train F1: 0.9814\n",
      "Val Loss: 0.2300, Val Acc: 0.9268, Val F1: 0.9204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0983: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "Train Loss: 0.0583, Train Acc: 0.9833, Train F1: 0.9821\n",
      "Val Loss: 0.1958, Val Acc: 0.9363, Val F1: 0.9308\n",
      "Early stopping triggered at epoch 17 with F1 score: 0.9308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:18<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7443: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 2.0241, Train Acc: 0.4682, Train F1: 0.4419\n",
      "Val Loss: 0.9525, Val Acc: 0.7516, Val F1: 0.7064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5209: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.7057, Train Acc: 0.7986, Train F1: 0.7692\n",
      "Val Loss: 0.4094, Val Acc: 0.8408, Val F1: 0.8119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3512: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.3797, Train Acc: 0.8838, Train F1: 0.8663\n",
      "Val Loss: 0.3192, Val Acc: 0.8631, Val F1: 0.8498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1091: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.2739, Train Acc: 0.9037, Train F1: 0.8893\n",
      "Val Loss: 0.3723, Val Acc: 0.8694, Val F1: 0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6342: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.2704, Train Acc: 0.9005, Train F1: 0.8886\n",
      "Val Loss: 0.2797, Val Acc: 0.8854, Val F1: 0.8749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1422: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.2116, Train Acc: 0.9172, Train F1: 0.9051\n",
      "Val Loss: 0.2317, Val Acc: 0.8949, Val F1: 0.8831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0532: 100%|██████████| 40/40 [00:20<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.1665, Train Acc: 0.9482, Train F1: 0.9401\n",
      "Val Loss: 0.2309, Val Acc: 0.8949, Val F1: 0.8818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0940: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.1499, Train Acc: 0.9530, Train F1: 0.9486\n",
      "Val Loss: 0.2127, Val Acc: 0.9108, Val F1: 0.9076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5626: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.1320, Train Acc: 0.9562, Train F1: 0.9528\n",
      "Val Loss: 0.2164, Val Acc: 0.8949, Val F1: 0.8864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0135: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.1205, Train Acc: 0.9546, Train F1: 0.9499\n",
      "Val Loss: 0.1943, Val Acc: 0.9045, Val F1: 0.8985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1353: 100%|██████████| 40/40 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0913, Train Acc: 0.9745, Train F1: 0.9730\n",
      "Val Loss: 0.2765, Val Acc: 0.9013, Val F1: 0.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0312:  78%|███████▊  | 31/40 [00:16<00:04,  1.91it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "        self.df = df.values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0, min_f1_score=0.9):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.min_f1_score = min_f1_score\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, val_f1):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience and val_f1 >= self.min_f1_score:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            \n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = 'swin_large_patch4_window7_224'  # Swin Transformer Large 모델\n",
    "    img_size = 224  # Swin Transformer에 적합한 이미지 크기\n",
    "    LR = 2e-5  # 1e-4 에서 학습률 조정 : 기존이 더 좋음\n",
    "    EPOCHS = 100 # 30에서 조정 : 기존이 더 좋음\n",
    "    BATCH_SIZE = 32  # 배치 크기 조정\n",
    "    num_workers = 4\n",
    "    n_splits = 5  # Number of K-fold splits\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "\n",
    "    # Stratified K-Fold 설정\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # 전체 결과 저장\n",
    "    final_preds_list = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['target'])):\n",
    "        print(f\"Fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        train_df = df.iloc[train_idx]\n",
    "        val_df = df.iloc[val_idx]\n",
    "\n",
    "        train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "        val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "        test_dataset = ImageDataset(pd.read_csv(os.path.join(data_path, \"sample_submission.csv\")), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        # 모델 설정\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "        # Early stopping 설정\n",
    "        early_stopping = EarlyStopping(patience=5, min_delta=0.001, min_f1_score=0.93)\n",
    "\n",
    "        # 모델 구조 출력\n",
    "        # print(f\"\\nModel structure of {model_name}:\")\n",
    "        # print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "        # 학습 루프\n",
    "        best_val_f1 = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "            val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f\"swin_t_model_fold_{fold}.pth\")\n",
    "            \n",
    "            # Early stopping 체크\n",
    "            early_stopping(val_loss, val_f1)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1} with F1 score: {val_f1:.4f}\")\n",
    "                break\n",
    "\n",
    "        # 테스트 데이터 추론\n",
    "        model.load_state_dict(torch.load(f\"swin_t_model_fold_{fold}.pth\"))\n",
    "        model.eval()\n",
    "        fold_preds_list = []\n",
    "\n",
    "        for image, _ in tqdm(test_loader):\n",
    "            image = image.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(image)\n",
    "            fold_preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        # 결과 저장\n",
    "        final_preds_list.append(fold_preds_list)\n",
    "\n",
    "    # 결과 앙상블\n",
    "    final_preds = np.mean(np.array(final_preds_list), axis=0).astype(int)\n",
    "\n",
    "    pred_df = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n",
    "    pred_df['target'] = final_preds\n",
    "    pred_df.to_csv(\"pred_swin_kfold.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred_swin.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin-T clustering & classification\n",
    "- 이미지를 유사한 이미지로 5개로 그룹핑 하고 분석하는 모델\n",
    "- early stoping 코드에서 강제적으로 f1 score boundary 를 줄수 있게 변경(분류하면 과소적합이 되는 구간이 있음.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Extracting features: 100%|██████████| 1570/1570 [08:21<00:00,  3.13it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for cluster 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.9624: 100%|██████████| 23/23 [00:07<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 2.3894, Train Acc: 0.2272, Train F1: 0.1160\n",
      "Val Loss: 1.7668, Val Acc: 0.4709, Val F1: 0.3021\n",
      "New best F1 score: 0.3021\n",
      "Validation loss decreased (inf --> 1.766814). F1 score: 0.302139. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1065: 100%|██████████| 23/23 [00:06<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 1.3509, Train Acc: 0.6027, Train F1: 0.4836\n",
      "Val Loss: 0.8510, Val Acc: 0.7302, Val F1: 0.6464\n",
      "New best F1 score: 0.6464\n",
      "Validation loss decreased (1.766814 --> 0.851001). F1 score: 0.646374. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6082: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.7066, Train Acc: 0.7891, Train F1: 0.7322\n",
      "Val Loss: 0.6014, Val Acc: 0.7778, Val F1: 0.6829\n",
      "New best F1 score: 0.6829\n",
      "Validation loss decreased (0.851001 --> 0.601394). F1 score: 0.682854. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3245: 100%|██████████| 23/23 [00:06<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.4193, Train Acc: 0.8721, Train F1: 0.8133\n",
      "Val Loss: 0.4649, Val Acc: 0.8095, Val F1: 0.7426\n",
      "New best F1 score: 0.7426\n",
      "Validation loss decreased (0.601394 --> 0.464942). F1 score: 0.742565. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2456: 100%|██████████| 23/23 [00:06<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.3084, Train Acc: 0.9034, Train F1: 0.8855\n",
      "Val Loss: 0.4378, Val Acc: 0.7989, Val F1: 0.8113\n",
      "New best F1 score: 0.8113\n",
      "Validation loss decreased (0.464942 --> 0.437803). F1 score: 0.811311. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2351: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.2114, Train Acc: 0.9374, Train F1: 0.9182\n",
      "Val Loss: 0.4343, Val Acc: 0.8148, Val F1: 0.8266\n",
      "New best F1 score: 0.8266\n",
      "Validation loss decreased (0.437803 --> 0.434306). F1 score: 0.826585. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1550: 100%|██████████| 23/23 [00:06<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.1245, Train Acc: 0.9782, Train F1: 0.9810\n",
      "Val Loss: 0.4091, Val Acc: 0.8307, Val F1: 0.8408\n",
      "New best F1 score: 0.8408\n",
      "Validation loss decreased (0.434306 --> 0.409088). F1 score: 0.840833. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0210: 100%|██████████| 23/23 [00:06<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.0843, Train Acc: 0.9823, Train F1: 0.9584\n",
      "Val Loss: 0.4157, Val Acc: 0.8254, Val F1: 0.8502\n",
      "New best F1 score: 0.8502\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1330: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.0708, Train Acc: 0.9932, Train F1: 0.9947\n",
      "Val Loss: 0.4342, Val Acc: 0.8360, Val F1: 0.8642\n",
      "New best F1 score: 0.8642\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0200: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.0580, Train Acc: 0.9891, Train F1: 0.9914\n",
      "Val Loss: 0.5063, Val Acc: 0.8148, Val F1: 0.8343\n",
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1000: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0391, Train Acc: 0.9959, Train F1: 0.9965\n",
      "Val Loss: 0.5177, Val Acc: 0.8148, Val F1: 0.8329\n",
      "EarlyStopping counter: 4 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0433: 100%|██████████| 23/23 [00:06<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "Train Loss: 0.0328, Train Acc: 0.9973, Train F1: 0.9981\n",
      "Val Loss: 0.5408, Val Acc: 0.8201, Val F1: 0.8485\n",
      "EarlyStopping counter: 5 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0346: 100%|██████████| 23/23 [00:06<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "Train Loss: 0.0216, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.5339, Val Acc: 0.8307, Val F1: 0.8589\n",
      "EarlyStopping counter: 6 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0366: 100%|██████████| 23/23 [00:06<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.0146, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.5482, Val Acc: 0.8571, Val F1: 0.8697\n",
      "New best F1 score: 0.8697\n",
      "EarlyStopping counter: 7 out of 7\n",
      "Early stopping. Best validation loss: 0.409088, Best F1 score: 0.840833\n",
      "\n",
      "Training model for cluster 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5359: 100%|██████████| 17/17 [00:04<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 1.7745, Train Acc: 0.5969, Train F1: 0.2815\n",
      "Val Loss: 0.5390, Val Acc: 0.9600, Val F1: 0.8432\n",
      "New best F1 score: 0.8432\n",
      "Validation loss decreased (inf --> 0.539037). F1 score: 0.843164. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5383: 100%|██████████| 17/17 [00:04<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "Train Loss: 0.3359, Train Acc: 0.9731, Train F1: 0.7593\n",
      "Val Loss: 0.0748, Val Acc: 0.9920, Val F1: 0.8684\n",
      "New best F1 score: 0.8684\n",
      "Validation loss decreased (0.539037 --> 0.074824). F1 score: 0.868421. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0186: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "Train Loss: 0.0658, Train Acc: 0.9942, Train F1: 0.8629\n",
      "Val Loss: 0.0176, Val Acc: 1.0000, Val F1: 1.0000\n",
      "New best F1 score: 1.0000\n",
      "Validation loss decreased (0.074824 --> 0.017606). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0084: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "Train Loss: 0.0263, Train Acc: 0.9981, Train F1: 0.8877\n",
      "Val Loss: 0.0055, Val Acc: 1.0000, Val F1: 1.0000\n",
      "Validation loss decreased (0.017606 --> 0.005532). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "Train Loss: 0.0147, Train Acc: 0.9981, Train F1: 0.8877\n",
      "Val Loss: 0.0039, Val Acc: 1.0000, Val F1: 1.0000\n",
      "Validation loss decreased (0.005532 --> 0.003931). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0066: 100%|██████████| 17/17 [00:04<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "Train Loss: 0.0122, Train Acc: 0.9981, Train F1: 0.8877\n",
      "Val Loss: 0.0028, Val Acc: 1.0000, Val F1: 1.0000\n",
      "Validation loss decreased (0.003931 --> 0.002810). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0056: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "Train Loss: 0.0107, Train Acc: 0.9981, Train F1: 0.8881\n",
      "Val Loss: 0.0017, Val Acc: 1.0000, Val F1: 1.0000\n",
      "Validation loss decreased (0.002810 --> 0.001662). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0091: 100%|██████████| 17/17 [00:04<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "Train Loss: 0.0080, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0016, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0008: 100%|██████████| 17/17 [00:04<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "Train Loss: 0.0027, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0008, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0065: 100%|██████████| 17/17 [00:04<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "Train Loss: 0.0042, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0007, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0014: 100%|██████████| 17/17 [00:04<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "Train Loss: 0.0027, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0006, Val Acc: 1.0000, Val F1: 1.0000\n",
      "Validation loss decreased (0.001662 --> 0.000624). F1 score: 1.000000. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "Train Loss: 0.0024, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0005, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 17/17 [00:04<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "Train Loss: 0.0020, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0004, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 17/17 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "Train Loss: 0.0026, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0005, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 17/17 [00:04<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "Train Loss: 0.0012, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0004, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 4 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0018: 100%|██████████| 17/17 [00:04<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "Train Loss: 0.0048, Train Acc: 0.9981, Train F1: 0.8881\n",
      "Val Loss: 0.0004, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 5 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0019: 100%|██████████| 17/17 [00:04<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "Train Loss: 0.0014, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0004, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 6 out of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 17/17 [00:04<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "Train Loss: 0.0008, Train Acc: 1.0000, Train F1: 1.0000\n",
      "Val Loss: 0.0004, Val Acc: 1.0000, Val F1: 1.0000\n",
      "EarlyStopping counter: 7 out of 7\n",
      "Early stopping. Best validation loss: 0.000624, Best F1 score: 1.000000\n",
      "\n",
      "Performing inference on test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1549283/822344420.py:299: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"swin_b_model_cluster_{cluster}.pth\"))\n",
      "Predicting cluster 0: 100%|██████████| 99/99 [00:10<00:00,  9.82it/s]\n",
      "Predicting cluster 1: 100%|██████████| 99/99 [00:10<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed and saved to pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Swin-B 모델 로드\n",
    "def load_swin_b_model(num_classes=None):\n",
    "    model = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "# 특성 추출 함수\n",
    "def extract_features(img_path, model):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = np.array(img)\n",
    "    img = transform(image=img)['image']\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = model.forward_features(img)\n",
    "    return features.squeeze().numpy()\n",
    "\n",
    "# 이미지 클러스터링 함수\n",
    "def cluster_images(data_path, n_clusters=5):\n",
    "    feature_extractor = load_swin_b_model(num_classes=None)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    image_files = [f for f in os.listdir(data_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    features = []\n",
    "    for img_file in tqdm(image_files, desc=\"Extracting features\"):\n",
    "        img_path = os.path.join(data_path, img_file)\n",
    "        feature = extract_features(img_path, feature_extractor)\n",
    "        features.append(feature.reshape(-1))  # Flatten the feature array\n",
    "    \n",
    "    features = np.array(features)\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "    return dict(zip(image_files, clusters))\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None, cluster_dict=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.cluster_dict = cluster_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        \n",
    "        cluster = self.cluster_dict.get(name, -1) if self.cluster_dict else -1\n",
    "        return img, target, cluster\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        self.best_f1 = -np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, f1_score, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, f1_score, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). '\n",
    "                            f'F1 score: {f1_score:.6f}. Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        self.best_f1 = f1_score\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets, _ in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets, _ in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    img_size = 224\n",
    "    LR = 2e-5\n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 32\n",
    "    num_workers = 4\n",
    "    n_clusters = 2\n",
    "\n",
    "    # 클러스터링 수행\n",
    "    print(\"Clustering images...\")\n",
    "    cluster_dict = cluster_images(os.path.join(data_path, \"train_preprocessed/\"), n_clusters)\n",
    "\n",
    "    # 데이터 증강 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"train_correct_labeling.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform, cluster_dict=cluster_dict)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform, cluster_dict=cluster_dict)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 각 클러스터에 대한 모델 학습\n",
    "    for cluster in range(n_clusters):\n",
    "        print(f\"\\nTraining model for cluster {cluster}\")\n",
    "        \n",
    "        # 클러스터에 해당하는 데이터만 선택\n",
    "        train_cluster = [data for data in train_dataset if data[2] == cluster]\n",
    "        val_cluster = [data for data in val_dataset if data[2] == cluster]\n",
    "        \n",
    "        if len(train_cluster) == 0 or len(val_cluster) == 0:\n",
    "            print(f\"Skipping cluster {cluster} due to insufficient data\")\n",
    "            continue\n",
    "        \n",
    "        train_cluster_loader = DataLoader(train_cluster, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        val_cluster_loader = DataLoader(val_cluster, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        # Swin-B 모델 설정\n",
    "        model = load_swin_b_model(num_classes=17).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "        # Early stopping 설정\n",
    "        early_stopping = EarlyStopping(patience=7, verbose=True, delta=0.001, path='cluster_model.pth')\n",
    "\n",
    "\n",
    "        # 모델 구조 출력\n",
    "        # print(f\"\\nModel structure of Swin-B for cluster {cluster}:\")\n",
    "        # print_model_summary(model, (3, img_size, img_size))\n",
    "\n",
    "        # 학습 루프\n",
    "        best_val_f1 = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc, train_f1 = train_one_epoch(train_cluster_loader, model, optimizer, loss_fn, device)\n",
    "            val_loss, val_acc, val_f1 = validate(val_cluster_loader, model, loss_fn, device)\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f\"swin_b_model_cluster_{cluster}.pth\")\n",
    "                print(f\"New best F1 score: {best_val_f1:.4f}\")\n",
    "\n",
    "            #조기 종료 체크 (validation 에러 기준)\n",
    "            early_stopping(val_loss, val_f1, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping. Best validation loss: {early_stopping.val_loss_min:.6f}, \"\n",
    "                    f\"Best F1 score: {early_stopping.best_f1:.6f}\")\n",
    "                break\n",
    "            \n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    print(\"\\nPerforming inference on test data\")\n",
    "    test_preds = []\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        model = load_swin_b_model(num_classes=17).to(device)\n",
    "        model.load_state_dict(torch.load(f\"swin_b_model_cluster_{cluster}.pth\"))\n",
    "        model.eval()\n",
    "        \n",
    "        cluster_preds = []\n",
    "        for image, _, _ in tqdm(test_loader, desc=f\"Predicting cluster {cluster}\"):\n",
    "            image = image.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(image)\n",
    "            cluster_preds.extend(preds.detach().cpu().numpy())\n",
    "        \n",
    "        test_preds.append(cluster_preds)\n",
    "    \n",
    "    # 모든 클러스터의 예측을 결합\n",
    "    final_preds = np.mean(test_preds, axis=0)\n",
    "    final_preds = np.argmax(final_preds, axis=1)\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = final_preds\n",
    "    pred_df.to_csv(\"swin_pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블\n",
    "CNN 모델(ConvNeXt V2 Large)과 Transformer 모델(ViT Large)을 결합한 앙상블 모델\n",
    "- CNN 모델로 'convnextv2_large'를 사용합니다.\n",
    "- Transformer 모델로 'vit_large_patch16_224'를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convnext v2 + vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torchsummary import summary\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 앙상블 모델 클래스 정의\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.model1(x)\n",
    "        out2 = self.model2(x)\n",
    "        return (out1 + out2) / 2\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = '../data/'\n",
    "img_size = 224\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "num_workers = 4\n",
    "\n",
    "# 데이터 증강 설정\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터 로드 및 분할\n",
    "df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "train_dataset = ImageDataset(train_df, os.path.join(data_path, \"train_preprocessed/\"), transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, os.path.join(data_path, \"train_preprocessed/\"), transform=val_transform)\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 모델 설정\n",
    "model1 = timm.create_model('convnextv2_large', pretrained=True, num_classes=17)\n",
    "model2 = timm.create_model('vit_large_patch16_224', pretrained=True, num_classes=17)\n",
    "\n",
    "ensemble_model = EnsembleModel(model1, model2).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(ensemble_model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "# 학습 루프\n",
    "best_val_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(train_loader, ensemble_model, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc, val_f1 = validate(val_loader, ensemble_model, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(ensemble_model.state_dict(), \"best_ensemble_model.pth\")\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "ensemble_model.load_state_dict(torch.load(\"best_ensemble_model.pth\"))\n",
    "ensemble_model.eval()\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = ensemble_model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"ensemble_pred.csv\", index=False)\n",
    "print(\"Prediction completed and saved to ensemble_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 모델 II - 리더 보드 제출용\n",
    "CNN 모델(ConvNeXt V2 Large)과 Transformer 모델(Swin Transformers)을 결합한 앙상블 모델\n",
    "- Hyper Parameter tunning이 전혀 되어 있지 않는 기본 모델 : 향후 최적화 필요\n",
    "- CNN 모델로 'convnextv2_large'를 사용합니다.\n",
    "- Transformer 모델로 'swin_large_patch4_window7_224'를 사용합니다.\n",
    "- software voting(기존 저장된 pth 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 첫 번째 모델과 두 번째 모델 로드\n",
    "model1 = timm.create_model('convnextv2_large', pretrained=False, num_classes=17).to(device)\n",
    "model2 = timm.create_model('swin_large_patch4_window7_224', pretrained=False, num_classes=17).to(device)\n",
    "\n",
    "# 모델 가중치 로드\n",
    "model1.load_state_dict(torch.load('convNext_model.pth'))\n",
    "model2.load_state_dict(torch.load('swin_t_model.pth'))\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test_preprocessed/\"), transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 소프트 보팅을 통한 예측\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for image, _ in tqdm(test_loader):\n",
    "        image = image.to(device)\n",
    "        preds1 = model1(image)\n",
    "        preds2 = model2(image)\n",
    "        \n",
    "        # 소프트 보팅: 예측 확률의 평균\n",
    "        preds_avg = (torch.softmax(preds1, dim=1) + torch.softmax(preds2, dim=1)) / 2\n",
    "        preds_list.extend(preds_avg.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "# 결과 저장\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"ensemble_pred.csv\", index=False)\n",
    "print(\"Ensemble prediction completed and saved to ensemble_pred.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert OCR 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# BERT 모델 정의\n",
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('klue/bert-base')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 데이터셋 클래스\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, csv_text_data, tokenizer=None, max_len=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.csv_text_data = csv_text_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        text = self.csv_text_data.get(img_name, \"\")\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.df.iloc[idx, 1], dtype=torch.long) if 'target' in self.df.columns else torch.tensor(0)\n",
    "        }\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    return avg_loss, f1, predictions, true_labels\n",
    "\n",
    "# CSV에서 텍스트 데이터 로드\n",
    "def load_text_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return {row['image']: ' '.join([text for text in eval(row['texts']) if text != '<extra_id_0>']) for _, row in df.iterrows()}\n",
    "\n",
    "# 메인 함수\n",
    "def main():\n",
    "    # 데이터 준비\n",
    "    data_path = '../data/'\n",
    "    train_csv_path = './corrected_train_texts.csv'\n",
    "    test_csv_path = './corrected_test_texts.csv' \n",
    "\n",
    "    train_csv_text_data = load_text_from_csv(train_csv_path)\n",
    "    test_csv_text_data = load_text_from_csv(test_csv_path)\n",
    "\n",
    "    df = pd.read_csv(f\"{data_path}/train_correct_labeling.csv\")\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "    # 토크나이저 준비\n",
    "    tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "    # 데이터셋 및 데이터로더 준비\n",
    "    train_dataset = TextDataset(train_df, train_csv_text_data, tokenizer)\n",
    "    val_dataset = TextDataset(val_df, train_csv_text_data, tokenizer)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    num_classes = len(df['target'].unique())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # BERT 모델 학습\n",
    "    bert_model = BERTModel(num_classes).to(device)\n",
    "    bert_optimizer = torch.optim.AdamW(bert_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    bert_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(bert_optimizer, T_max=30)\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(30):\n",
    "        train_loss = train(bert_model, train_loader, criterion, bert_optimizer, bert_scheduler, device)\n",
    "        val_loss, val_f1, _, _ = evaluate(bert_model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(bert_model.state_dict(), 'best_bert_model.pth')\n",
    "            print(f\"New best model saved with F1 score: {best_val_f1:.4f}\")\n",
    "\n",
    "    # 테스트 데이터 준비\n",
    "    test_df = pd.read_csv(f\"{data_path}/sample_submission.csv\")\n",
    "    test_dataset = TextDataset(test_df, test_csv_text_data, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    bert_model.load_state_dict(torch.load('best_bert_model.pth'))\n",
    "    bert_model.eval()\n",
    "    test_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting test data\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = bert_model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    submission_df = pd.DataFrame({'ID': test_df['ID'], 'target': test_predictions})\n",
    "    submission_df.to_csv(\"bert_text_pred.csv\", index=False)\n",
    "    print(\"Test predictions saved to bert_text_pred.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2가지 모델 앙상블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앙상블 파라미터 최적화(mento comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timm\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from scipy.optimize import minimize\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 컨피던스 임계값 설정\n",
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "\n",
    "# CUDA 메모리 캐시 정리 함수\n",
    "def clear_cuda_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv.values\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img_path = os.path.join(self.path, name)\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image not found: {img_path}\")\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, scaler):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # 메모리 정리\n",
    "        del image, targets, preds, loss\n",
    "        clear_cuda_memory()\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                preds = model(image)\n",
    "                loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "            # 메모리 정리\n",
    "            del image, targets, preds, loss\n",
    "            clear_cuda_memory()\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        self.best_f1 = -np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, f1_score, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, f1_score, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        self.best_f1 = f1_score\n",
    "\n",
    "# 모델 훈련 함수 (메모리 최적화)\n",
    "def train_model(model_name, train_loader, val_loader, device, epochs, lr, initial_batch_size):\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.001, path=f'aug_{model_name}_model.pth')\n",
    "    scaler = torch.amp.GradScaler() \n",
    "\n",
    "    best_val_f1 = 0\n",
    "    current_batch_size = initial_batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        try:\n",
    "            train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device, scaler)\n",
    "            val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f\"aug_{model_name}_model.pth\")\n",
    "            \n",
    "            early_stopping(val_loss, val_f1, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping. Best validation loss: {early_stopping.val_loss_min:.6f}, \"\n",
    "                      f\"Best F1 score: {early_stopping.best_f1:.6f}\")\n",
    "                break\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                clear_cuda_memory()\n",
    "                current_batch_size = max(current_batch_size // 2, 1)\n",
    "                print(f\"CUDA out of memory. Reducing batch size to {current_batch_size}\")\n",
    "                train_loader = DataLoader(train_loader.dataset, batch_size=current_batch_size, shuffle=True, num_workers=train_loader.num_workers, pin_memory=True)\n",
    "                val_loader = DataLoader(val_loader.dataset, batch_size=current_batch_size, shuffle=False, num_workers=val_loader.num_workers, pin_memory=True)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    return model\n",
    "\n",
    "# 예측 확률 계산 함수\n",
    "def get_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            confidences, _ = torch.max(probs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_confidences.append(confidences.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_probs), np.concatenate(all_confidences)\n",
    "\n",
    "# 앙상블 최적화 함수\n",
    "def optimize_ensemble(pred1_prob, pred2_prob, true_labels):\n",
    "    def objective(weights):\n",
    "        ensemble_pred_prob = weights[0] * pred1_prob + weights[1] * pred2_prob\n",
    "        return log_loss(true_labels, ensemble_pred_prob)\n",
    "\n",
    "    initial_weights = [0.5, 0.5]\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: 1 - sum(w)}\n",
    "    bounds = [(0, 1), (0, 1)]\n",
    "\n",
    "    result = minimize(objective, initial_weights, bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "# 기존 모델 로드 할때 필요한 함수\n",
    "# def load_model(model_name, model_path, device):\n",
    "#     if model_name == 'swin_large_patch4_window7_224':\n",
    "#         model = timm.create_model(model_name, pretrained=False, num_classes=17)\n",
    "#     elif model_name == 'convnextv2_large':\n",
    "#         model = timm.create_model(model_name, pretrained=False, num_classes=17)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "#     model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "#     model = model.to(device)\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# # 메인 실행 코드 내에서\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     # 훈련된 모델 불러오기\n",
    "#     swin_model = load_model('swin_large_patch4_window7_224', 'aug_swin_model.pth', device)\n",
    "#     convnext_model = load_model('convnextv2_large', 'aug_conv_model_final.pth', device)\n",
    "\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    img_size = 224\n",
    "    INITIAL_BATCH_SIZE = 32\n",
    "    num_workers = 4\n",
    "\n",
    "    # 데이터 증강 및 변환 설정\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "# 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"augmented_train.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, os.path.join(data_path, \"augmented_train/\"), transform=train_transform)\n",
    "    val_dataset = ImageDataset(val_df, os.path.join(data_path, \"augmented_train/\"), transform=val_transform)\n",
    "    test_dataset = ImageDataset(os.path.join(data_path, \"sample_submission.csv\"), os.path.join(data_path, \"test/\"), transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=INITIAL_BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=INITIAL_BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=INITIAL_BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # 모델 훈련\n",
    "    swin_model = train_model('swin_large_patch4_window7_224', train_loader, val_loader, device, epochs=100, lr=2e-5, initial_batch_size=INITIAL_BATCH_SIZE)\n",
    "    clear_cuda_memory()\n",
    "    convnext_model = train_model('convnextv2_large', train_loader, val_loader, device, epochs=100, lr=1e-4, initial_batch_size=INITIAL_BATCH_SIZE)\n",
    "\n",
    "    torch.save(swin_model.state_dict(), \"aug_swin_model.pth\")\n",
    "    torch.save(convnext_model.state_dict(), \"aug_conv_model.pth\")\n",
    "\n",
    "    # 검증 데이터에 대한 예측\n",
    "    swin_preds = get_predictions(swin_model, val_loader, device)\n",
    "    convnext_preds = get_predictions(convnext_model, val_loader, device)\n",
    "\n",
    "    # 최적의 가중치 계산\n",
    "    true_labels = val_df['target'].values\n",
    "    optimized_weights = optimize_ensemble(swin_preds, convnext_preds, true_labels)\n",
    "    print(\"Optimized Weights:\", optimized_weights)\n",
    "\n",
    "    # 앙상블 예측\n",
    "    ensemble_preds = optimized_weights[0] * swin_preds + optimized_weights[1] * convnext_preds\n",
    "    # 정규화 추가\n",
    "    epsilon = 1e-7\n",
    "    ensemble_preds = (ensemble_preds + epsilon) / np.sum(ensemble_preds + epsilon, axis=1, keepdims=True)\n",
    "    ensemble_classes = np.argmax(ensemble_preds, axis=1)\n",
    "\n",
    "    # 성능 평가\n",
    "    accuracy = accuracy_score(true_labels, ensemble_classes)\n",
    "    f1 = f1_score(true_labels, ensemble_classes, average='macro')\n",
    "    print(f\"Ensemble Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Ensemble F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    swin_test_preds = get_predictions(swin_model, test_loader, device)\n",
    "    convnext_test_preds = get_predictions(convnext_model, test_loader, device)\n",
    "    ensemble_test_preds = optimized_weights[0] * swin_test_preds + optimized_weights[1] * convnext_test_preds\n",
    "    ensemble_test_classes = np.argmax(ensemble_test_preds, axis=1)\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "    pred_df['target'] = ensemble_test_classes\n",
    "    pred_df.to_csv(\"ensemble_pred.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to ensemble_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 우선 모델 선택 (성능 향상 없음...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ConvNeXt predictions: 100%|██████████| 111/111 [00:20<00:00,  5.34it/s]\n",
      "Swin predictions: 100%|██████████| 111/111 [00:20<00:00,  5.43it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating optimal weights...\n",
      "Optimized Weights: [0.5 0.5]\n",
      "Performing ensemble prediction...\n",
      "Evaluating ensemble performance...\n",
      "Ensemble Accuracy: 0.9983\n",
      "Ensemble F1 Score: 0.9979\n",
      "Predicting on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Swin test predictions: 100%|██████████| 99/99 [00:18<00:00,  5.43it/s]\n",
      "ConvNeXt test predictions: 100%|██████████| 99/99 [00:18<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions...\n",
      "Prediction completed and saved to ensemble_pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 컨피던스 임계값 설정\n",
    "CONFIDENCE_THRESHOLD = 0.95\n",
    "\n",
    "def get_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            confidences, _ = torch.max(probs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_confidences.append(confidences.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_probs), np.concatenate(all_confidences)\n",
    "\n",
    "def optimize_ensemble(pred1_prob, pred2_prob, true_labels):\n",
    "    def objective(weights):\n",
    "        ensemble_pred_prob = weights[0] * pred1_prob + weights[1] * pred2_prob\n",
    "        return log_loss(true_labels, ensemble_pred_prob)\n",
    "\n",
    "    initial_weights = [0.5, 0.5]\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: 1 - sum(w)}\n",
    "    bounds = [(0, 1), (0, 1)]\n",
    "\n",
    "    result = minimize(objective, initial_weights, bounds=bounds, constraints=constraints)\n",
    "    return result.x\n",
    "\n",
    "# 검증 데이터에 대한 예측\n",
    "print(\"Predicting on validation data...\")\n",
    "convnext_preds, convnext_confidences = get_predictions(convnext_model, tqdm(val_loader, desc=\"ConvNeXt predictions\"), device)\n",
    "swin_preds, swin_confidences = get_predictions(swin_model, tqdm(val_loader, desc=\"Swin predictions\"), device)\n",
    "\n",
    "print(\"Calculating optimal weights...\")\n",
    "true_labels = val_df['target'].values\n",
    "optimized_weights = optimize_ensemble(convnext_preds, swin_preds, true_labels)\n",
    "print(\"Optimized Weights:\", optimized_weights)\n",
    "\n",
    "print(\"Performing ensemble prediction...\")\n",
    "ensemble_preds = np.zeros_like(swin_preds)\n",
    "for i in range(len(swin_preds)):\n",
    "    if swin_confidences[i] > CONFIDENCE_THRESHOLD:\n",
    "        ensemble_preds[i] = swin_preds[i]\n",
    "    else:\n",
    "        ensemble_preds[i] = optimized_weights[0] * swin_preds[i] + optimized_weights[1] * convnext_preds[i]\n",
    "\n",
    "# 정규화\n",
    "epsilon = 1e-7\n",
    "ensemble_preds = (ensemble_preds + epsilon) / np.sum(ensemble_preds + epsilon, axis=1, keepdims=True)\n",
    "ensemble_classes = np.argmax(ensemble_preds, axis=1)\n",
    "\n",
    "# 성능 평가\n",
    "print(\"Evaluating ensemble performance...\")\n",
    "accuracy = accuracy_score(true_labels, ensemble_classes)\n",
    "f1 = f1_score(true_labels, ensemble_classes, average='macro')\n",
    "print(f\"Ensemble Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Ensemble F1 Score: {f1:.4f}\")\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "print(\"Predicting on test data...\")\n",
    "swin_test_preds, swin_test_confidences = get_predictions(swin_model, tqdm(test_loader, desc=\"Swin test predictions\"), device)\n",
    "convnext_test_preds, convnext_test_confidences = get_predictions(convnext_model, tqdm(test_loader, desc=\"ConvNeXt test predictions\"), device)\n",
    "\n",
    "ensemble_test_preds = np.zeros_like(swin_test_preds)\n",
    "for i in range(len(swin_test_preds)):\n",
    "    if swin_test_confidences[i] > CONFIDENCE_THRESHOLD:\n",
    "        ensemble_test_preds[i] = swin_test_preds[i]\n",
    "    else:\n",
    "        ensemble_test_preds[i] = optimized_weights[0] * swin_test_preds[i] + optimized_weights[1] * convnext_test_preds[i]\n",
    "\n",
    "# 정규화\n",
    "ensemble_test_preds = (ensemble_test_preds + epsilon) / np.sum(ensemble_test_preds + epsilon, axis=1, keepdims=True)\n",
    "ensemble_test_classes = np.argmax(ensemble_test_preds, axis=1)\n",
    "\n",
    "# 결과 저장\n",
    "print(\"Saving predictions...\")\n",
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = ensemble_test_classes\n",
    "pred_df.to_csv(\"ensemble_pred_3.csv\", index=False)\n",
    "print(\"Prediction completed and saved to ensemble_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3가지 모델 앙상블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayoutLM v3 Model\n",
    "- 설치가 조금 까다로울수 있음\n",
    "- g++ 설치 : 소스 받아서, \"pip install -e .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.9540:   6%|▌         | 57/1031 [00:24<04:56,  3.29it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoProcessor\n",
    "from torchsummary import summary\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# LayoutLM 데이터셋 클래스\n",
    "class LayoutLMDataset(Dataset):\n",
    "    def __init__(self, csv, img_dir, processor, max_length=512, is_test=False):\n",
    "        if isinstance(csv, pd.DataFrame):\n",
    "            self.df = csv\n",
    "        else:\n",
    "            self.df = pd.read_csv(csv)\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.df.iloc[idx]['ID'])\n",
    "        \n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        encoding = self.processor(image, return_tensors=\"pt\", truncation=True, max_length=self.max_length)\n",
    "        \n",
    "        for key in ['input_ids', 'attention_mask', 'bbox']:\n",
    "            encoding[key] = encoding[key].squeeze(0)\n",
    "        \n",
    "        encoding['pixel_values'] = encoding['pixel_values'].squeeze(0)\n",
    "        \n",
    "        if not self.is_test:\n",
    "            label = self.df.iloc[idx]['target']\n",
    "            encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            encoding['labels'] = torch.tensor(0, dtype=torch.long)  # dummy label for test set\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(outputs.logits.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(batch['labels'].detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    bbox = [item['bbox'] for item in batch]\n",
    "    pixel_values = [item['pixel_values'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    bbox = pad_sequence(bbox, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Stack other tensors\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'bbox': bbox,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(outputs.logits.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(batch['labels'].detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        self.best_f1 = -np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, f1_score, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, f1_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, f1_score, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        self.best_f1 = f1_score\n",
    "\n",
    "# 모델 구조 출력 함수\n",
    "def print_model_summary(model, input_size):\n",
    "    summary(model, input_size)\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_path = '../data/'\n",
    "    model_name = \"microsoft/layoutlmv3-base\"\n",
    "    global processor  # processor를 전역 변수로 선언\n",
    "    LR = 1e-5\n",
    "    EPOCHS = 2\n",
    "    BATCH_SIZE = 8\n",
    "    num_workers = 4\n",
    "\n",
    "    # 모델과 프로세서 초기화\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=17)\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # 데이터 로드 및 분할\n",
    "    df = pd.read_csv(os.path.join(data_path, \"augmented_train.csv\"))\n",
    "    train_df, val_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['target'])\n",
    "\n",
    "    train_dataset = LayoutLMDataset(train_df, os.path.join(data_path, \"augmented_train/\"), processor)\n",
    "    val_dataset = LayoutLMDataset(val_df, os.path.join(data_path, \"augmented_train/\"), processor)\n",
    "    test_dataset = LayoutLMDataset(os.path.join(data_path, \"sample_submission.csv\"), \n",
    "                                os.path.join(data_path, \"test/\"), \n",
    "                                processor, \n",
    "                                is_test=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True, collate_fn=custom_collate_fn)\n",
    "    # 모델 설정\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    early_stopping = EarlyStopping(patience=7, verbose=True, delta=0.001, path='aug_layoutlmv3_model.pth')\n",
    "\n",
    "    # 학습 루프\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_fn, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"aug_layoutlmv3_model.pth\")\n",
    "        \n",
    "        early_stopping(val_loss, val_f1, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping. Best validation loss: {early_stopping.val_loss_min:.6f}, \"\n",
    "                  f\"Best F1 score: {early_stopping.best_f1:.6f}\")\n",
    "            break\n",
    "\n",
    "    # 테스트 데이터 추론\n",
    "    model.load_state_dict(torch.load(\"aug_layoutlmv3_model.pth\"))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for batch in tqdm(test_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        preds_list.extend(outputs.logits.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    # 결과 저장\n",
    "    pred_df = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n",
    "    pred_df['target'] = preds_list\n",
    "    pred_df.to_csv(\"pred_layoutlmv3.csv\", index=False)\n",
    "    print(\"Prediction completed and saved to pred_layoutlmv3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR 멀티모달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting test data: 100%|██████████| 99/99 [00:58<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to ensemble_pred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from timm import create_model\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# 멀티모달 데이터셋 클래스\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, csv_text_data, transform=None, tokenizer=None, max_len=512):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.csv_text_data = csv_text_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        img_path = f\"{self.image_dir}/{img_name}\"\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        text = self.csv_text_data.get(img_name, \"\")\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "# CSV에서 텍스트 데이터 로드\n",
    "def load_text_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return {row['image']: ' '.join([text for text in eval(row['texts']) if text != '<extra_id_0>']) for _, row in df.iterrows()}\n",
    "\n",
    "# state_dict 키 변경 함수\n",
    "def rename_keys(state_dict, key_map):\n",
    "    for old_key in list(state_dict.keys()):\n",
    "        if old_key in key_map:\n",
    "            state_dict[key_map[old_key]] = state_dict.pop(old_key)\n",
    "    return state_dict\n",
    "\n",
    "# 테스트 데이터 추론\n",
    "def predict_test_data():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 데이터 준비\n",
    "    data_path = '../data/'\n",
    "    test_csv_path = './corrected_test_texts.csv'\n",
    "    test_csv_text_data = load_text_from_csv(test_csv_path)\n",
    "    test_df = pd.read_csv(f\"{data_path}/sample_submission.csv\")\n",
    "\n",
    "    # 모델 로드\n",
    "    num_classes = 17  # 클래스 수에 맞게 조정\n",
    "    bert_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels=num_classes)\n",
    "    state_dict = torch.load('best_bert_model.pth')\n",
    "    \n",
    "    # 키 변경 맵 정의\n",
    "    key_map = {\n",
    "        \"fc.weight\": \"classifier.weight\",\n",
    "        \"fc.bias\": \"classifier.bias\"\n",
    "    }\n",
    "    \n",
    "    # state_dict 키 변경\n",
    "    state_dict = rename_keys(state_dict, key_map)\n",
    "    \n",
    "    # state_dict 로드\n",
    "    bert_model.load_state_dict(state_dict)\n",
    "    bert_model.to(device)\n",
    "    bert_model.eval()\n",
    "\n",
    "    swin_model = create_model('swin_large_patch4_window7_224', pretrained=False, num_classes=num_classes)\n",
    "    swin_model.load_state_dict(torch.load('swin_large.pt'))\n",
    "    swin_model.to(device)\n",
    "    swin_model.eval()\n",
    "\n",
    "    convnext_model = create_model('convnextv2_large', pretrained=False, num_classes=num_classes)\n",
    "    convnext_model.load_state_dict(torch.load('convnextv2_large.pt'))\n",
    "    convnext_model.to(device)\n",
    "    convnext_model.eval()\n",
    "\n",
    "    # 토크나이저 및 변환 준비\n",
    "    tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 데이터셋 및 데이터로더 준비\n",
    "    test_dataset = MultimodalDataset(test_df, f\"{data_path}/test_preprocessed\", test_csv_text_data, transform, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    test_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting test data\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            texts = batch['text']\n",
    "\n",
    "            bert_outputs = bert_model(input_ids, attention_mask).logits\n",
    "            swin_outputs = swin_model(images)\n",
    "            convnext_outputs = convnext_model(images)\n",
    "\n",
    "            for i in range(len(texts)):\n",
    "                word_count = len(texts[i].split())\n",
    "                if word_count >= 20:\n",
    "                    ensemble_outputs = 0.4 * bert_outputs[i] + 0.4 * swin_outputs[i] + 0.2 * convnext_outputs[i]\n",
    "                else:\n",
    "                    ensemble_outputs = 0.7 * swin_outputs[i] + 0.3 * convnext_outputs[i]\n",
    "\n",
    "                _, predicted = torch.max(ensemble_outputs, 0)\n",
    "                test_predictions.append(predicted.item())\n",
    "\n",
    "    # 결과 저장\n",
    "    submission_df = pd.DataFrame({'ID': test_df['ID'], 'target': test_predictions})\n",
    "    submission_df.to_csv(\"ensemble_pred.csv\", index=False)\n",
    "    print(\"Test predictions saved to ensemble_pred.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_test_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
